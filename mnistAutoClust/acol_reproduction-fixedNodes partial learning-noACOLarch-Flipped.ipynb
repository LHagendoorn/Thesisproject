{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACOL replication tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "perc = float(os.environ.get('perc', 1.0))\n",
    "balanced = bool(os.environ.get('balanced', True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage subclass labelled: 1\n",
      "Balanced: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentage subclass labelled: %g\"%(perc))\n",
    "print(\"Balanced: %r\"%(balanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from notifiers import notify\n",
    "#imports and settings:\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "#from jupyterthemes import jtplot\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import threshold\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "from tensorflow.contrib.learn.python.learn.datasets import mnist\n",
    "#jtplot.style()\n",
    "%matplotlib inline\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "TRAIN_IMAGES = 'train-images-idx3-ubyte.gz'\n",
    "TRAIN_LABELS = 'train-labels-idx1-ubyte.gz'\n",
    "TEST_IMAGES = 't10k-images-idx3-ubyte.gz'\n",
    "TEST_LABELS = 't10k-labels-idx1-ubyte.gz'\n",
    "SOURCE_URL = 'https://storage.googleapis.com/cvdf-datasets/mnist/'\n",
    "\n",
    "local_file = base.maybe_download(TRAIN_IMAGES, './MNIST_data',\n",
    "                                   SOURCE_URL + TRAIN_IMAGES)\n",
    "with open(local_file, 'rb') as f:\n",
    "    train_images = mnist.extract_images(f)\n",
    "    \n",
    "local_file = base.maybe_download(TRAIN_LABELS, './MNIST_data',\n",
    "                                   SOURCE_URL + TRAIN_LABELS)\n",
    "with open(local_file, 'rb') as f:\n",
    "    train_labels = mnist.extract_labels(f, one_hot=True)\n",
    "\n",
    "local_file = base.maybe_download(TEST_IMAGES, './MNIST_data',\n",
    "                                   SOURCE_URL + TEST_IMAGES)\n",
    "with open(local_file, 'rb') as f:\n",
    "    test_images = mnist.extract_images(f)\n",
    "\n",
    "local_file = base.maybe_download(TEST_LABELS, './MNIST_data',\n",
    "                                   SOURCE_URL + TEST_LABELS)\n",
    "with open(local_file, 'rb') as f:\n",
    "    test_labels = mnist.extract_labels(f, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def notify(body):\n",
    "#    msg = MIMEText(body)\n",
    "    # me == the sender's email address\n",
    "    # you == the recipient's email address\n",
    "#    msg['Subject'] = 'Finished training!'\n",
    "#    msg['From'] = \"scriptnotificiations@gmail.com\"\n",
    "#    msg['To'] = \"scriptnotificiations@gmail.com\"\n",
    "\n",
    "    # Send the message via our own SMTP server, but don't include the\n",
    "    # envelope header.\n",
    "#    s = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "#    s.starttls()\n",
    "#    s.login(\"scriptnotificiations@gmail.com\", \"SuperSecretMegaPassword!!!\")\n",
    "#    s.sendmail(\"scriptnotificiations@gmail.com\", [\"scriptnotificiations@gmail.com\"], msg.as_string())\n",
    "#    s.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clustCount = 10\n",
    "classCount = 2\n",
    "net = 0\n",
    "#trainsteps = 50000\n",
    "trainsteps = 50000\n",
    "#perc = 0.1\n",
    "#balanced = True\n",
    "validation_size=5000*2\n",
    "_epochs_completed_train = 0\n",
    "_index_in_epoch_train = 0\n",
    "_epochs_completed_val = 0\n",
    "_index_in_epoch_val = 0\n",
    "_epochs_completed_test = 0\n",
    "_index_in_epoch_test = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y = {0:[1,0],\n",
    "#     1:[1,0],\n",
    "#     2:[1,0],\n",
    "#     3:[1,0],\n",
    "#     4:[1,0],\n",
    "#     5:[0,1],\n",
    "#     6:[0,1],\n",
    "#     7:[0,1],\n",
    "#     8:[0,1],\n",
    "#     9:[0,1]}\n",
    "y2 = {\n",
    "    0:np.zeros((classCount,clustCount)),\n",
    "    1:np.zeros((classCount,clustCount)),\n",
    "    2:np.zeros((classCount,clustCount)),\n",
    "    3:np.zeros((classCount,clustCount)),\n",
    "    4:np.zeros((classCount,clustCount)),\n",
    "    5:np.zeros((classCount,clustCount)),\n",
    "    6:np.zeros((classCount,clustCount)),\n",
    "    7:np.zeros((classCount,clustCount)),\n",
    "    8:np.zeros((classCount,clustCount)),\n",
    "    9:np.zeros((classCount,clustCount))\n",
    "}\n",
    "y2[0][0,0] = 1\n",
    "y2[1][0,1] = 1\n",
    "y2[2][0,2] = 1\n",
    "y2[3][0,3] = 1\n",
    "y2[4][0,4] = 1\n",
    "y2[5][0,5] = 1\n",
    "y2[6][0,6] = 1\n",
    "y2[7][0,7] = 1\n",
    "y2[8][0,8] = 1\n",
    "y2[9][0,9] = 1\n",
    "\n",
    "emptyy2 = {\n",
    "    0:np.zeros((classCount,clustCount)),\n",
    "    1:np.zeros((classCount,clustCount)),\n",
    "    2:np.zeros((classCount,clustCount)),\n",
    "    3:np.zeros((classCount,clustCount)),\n",
    "    4:np.zeros((classCount,clustCount)),\n",
    "    5:np.zeros((classCount,clustCount)),\n",
    "    6:np.zeros((classCount,clustCount)),\n",
    "    7:np.zeros((classCount,clustCount)),\n",
    "    8:np.zeros((classCount,clustCount)),\n",
    "    9:np.zeros((classCount,clustCount))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_super_labels = np.array([y[np.argmax(train_labels[j])] for j in range(train_labels.shape[0])])\n",
    "#test_super_labels = np.array([y[np.argmax(test_labels[j])] for j in range(test_labels.shape[0])])\n",
    "imFlipper = [slice(None)] * 3\n",
    "imFlipper[2] = slice(None, None, -1)\n",
    "\n",
    "lblFlipper = [slice(None)] * 3\n",
    "lblFlipper[1] = slice(None, None, -1)\n",
    "\n",
    "train_labels = np.array([y2[np.argmax(train_labels[j])] for j in range(train_labels.shape[0])])\n",
    "test_labels = np.array([y2[np.argmax(test_labels[j])] for j in range(test_labels.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not 0 <= validation_size <= len(train_images):\n",
    "    raise ValueError(\n",
    "        'Validation size should be between 0 and {}. Received: {}.'\n",
    "        .format(len(train_images), validation_size))\n",
    "\n",
    "validation_images = train_images[:validation_size]\n",
    "validation_labels = train_labels[:validation_size]\n",
    "#validation_labels_clipped = train_labels_clipped[:validation_size]\n",
    "train_images = train_images[validation_size:]\n",
    "train_labels = train_labels[validation_size:]\n",
    "#train_labels_clipped = train_labels_clipped[validation_size:]\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0],train_images.shape[1] * train_images.shape[2])\n",
    "train_images = train_images.astype(np.float32)\n",
    "train_images = np.multiply(train_images, 1.0 / 255.0)\n",
    "\n",
    "validation_images = validation_images.reshape(validation_images.shape[0],validation_images.shape[1] * validation_images.shape[2])\n",
    "validation_images = validation_images.astype(np.float32)\n",
    "validation_images = np.multiply(validation_images, 1.0 / 255.0)\n",
    "\n",
    "test_images = test_images.reshape(test_images.shape[0],test_images.shape[1] * test_images.shape[2])\n",
    "test_images = test_images.astype(np.float32)\n",
    "test_images = np.multiply(test_images, 1.0 / 255.0)\n",
    "\n",
    "trainCount=len(train_images)\n",
    "if perc<1:\n",
    "    if balanced:\n",
    "        inds = []\n",
    "        classSize = int(np.ceil(trainCount*perc/10))\n",
    "        for j in range(10):\n",
    "            inds.extend([i for i, x in enumerate(np.argmax(np.sum(train_labels,1),1)) if x == j][:classSize])\n",
    "        random.shuffle(inds)\n",
    "\n",
    "        train_labels_clipped = np.array([train_labels[j,:,:] for j in inds])\n",
    "        train_labels_clipped = np.concatenate([train_labels_clipped,np.array([emptyy2[np.argmax(train_labels[j])] for j in range(trainCount) if j not in inds])])\n",
    "    else:\n",
    "        train_labels_clipped = np.array([y2[np.argmax(train_labels[j])] for j in range(int(train_labels.shape[0]*perc))])\n",
    "        train_labels_clipped = np.concatenate([train_labels_clipped,np.array([emptyy2[np.argmax(train_labels[j])] for j in range(int(train_labels.shape[0]*perc),train_labels.shape[0])])])\n",
    "else:\n",
    "    train_labels_clipped = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def next_batch(batch_size, shuffle, images, labels, ep_compl, ep_ind):\n",
    "    if batch_size/2.0 < 1:\n",
    "        if np.random.random() < 0.5:\n",
    "            flipped = np.array([1,0]).reshape((1,2))\n",
    "        else:\n",
    "            flipped = np.array([0,1]).reshape((1,2))\n",
    "    _epochs_completed = ep_compl\n",
    "    _index_in_epoch = ep_ind\n",
    "    \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "    start = _index_in_epoch\n",
    "    _num_examples = images.shape[0]\n",
    "    #Flip half\n",
    "    # Shuffle for the first epoch   \n",
    "    if _epochs_completed == 0 and start == 0 and shuffle:\n",
    "      perm0 = np.arange(_num_examples)\n",
    "      np.random.shuffle(perm0)\n",
    "      _images = images[perm0]\n",
    "      _labels = labels[perm0]\n",
    "    # Go to the next epoch\n",
    "    if start + batch_size > _num_examples:\n",
    "      # Finished epoch\n",
    "      _epochs_completed += 1\n",
    "      # Get the rest examples in this epoch\n",
    "      rest_num_examples = _num_examples - start\n",
    "      images_rest_part = _images[start:_num_examples]\n",
    "      labels_rest_part = _labels[start:_num_examples]\n",
    "      # Shuffle the data\n",
    "      if shuffle:\n",
    "        perm = np.arange(_num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        _images = images[perm]\n",
    "        print(_images)\n",
    "        _labels = labels[perm]\n",
    "      # Start next epoch\n",
    "      start = 0\n",
    "      _index_in_epoch = batch_size - rest_num_examples\n",
    "      end = _index_in_epoch\n",
    "      images_new_part = _images[start:end]\n",
    "      labels_new_part = _labels[start:end]\n",
    "      super_labels_new_part = _super_labels[start:end]\n",
    "      im = np.concatenate((images_rest_part, images_new_part), axis=0)                 \n",
    "      lbl = np.concatenate((labels_rest_part, labels_new_part), axis=0)\n",
    "      #flip half then reshuffle batch\n",
    "      if batch_size>1:\n",
    "          im = np.vstack([im[:(batch_size/2),:],im[(batch_size/2):,:].reshape((-1,28,28))[tuple(imFlipper)].reshape(-1,28*28)])\n",
    "          lbl = np.vstack([lbl[:(batch_size/2),:,:],lbl[(batch_size/2):,:,:][tuple(lblFlipper)]])\n",
    "          suplbl = np.vstack([[[1,0]]*(batch_size/2), [[0,1]]*(batch_size/2)])\n",
    "          perm1 = np.arange(batch_size)\n",
    "          np.random.shuffle(perm1)\n",
    "          im = im[perm1]\n",
    "          lbl = lbl[perm1]\n",
    "          suplbl = suplbl[perm1]\n",
    "      else:\n",
    "          if flipped[0,0]: #not flipped\n",
    "            suplbl = flipped\n",
    "            pass\n",
    "          else:\n",
    "            im = im.reshape((-1,28,28))[tuple(imFlipper)].reshape(-1,28*28)\n",
    "            lbl = lbl[tuple(lblFlipper)]\n",
    "            suplbl = flipped\n",
    "            perm1 = np.arange(batch_size)\n",
    "      return im, suplbl, lbl\n",
    "    else:\n",
    "      _index_in_epoch += batch_size\n",
    "      end = _index_in_epoch\n",
    "      im = _images[start:end]\n",
    "      lbl = _labels[start:end]\n",
    "      if batch_size>1:\n",
    "          im = np.vstack([im[:(batch_size/2),:],im[(batch_size/2):,:].reshape((-1,28,28))[tuple(imFlipper)].reshape(-1,28*28)])\n",
    "          lbl = np.vstack([lbl[:(batch_size/2),:,:],lbl[(batch_size/2):,:,:][tuple(lblFlipper)]])\n",
    "          suplbl = np.vstack([[[1,0]]*(batch_size/2), [[0,1]]*(batch_size/2)])\n",
    "          perm1 = np.arange(batch_size)\n",
    "          np.random.shuffle(perm1)\n",
    "          im = im[perm1]\n",
    "          lbl = lbl[perm1]\n",
    "          suplbl = suplbl[perm1]\n",
    "      else:\n",
    "          if flipped[0,0]: #not flipped\n",
    "            suplbl = flipped\n",
    "            pass\n",
    "          else:\n",
    "            im = im.reshape((-1,28,28))[tuple(imFlipper)].reshape(-1,28*28)\n",
    "            lbl = lbl[tuple(lblFlipper)]\n",
    "            suplbl = flipped\n",
    "      return im, suplbl, lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#helper funcs\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "def matrix_softmax(x):\n",
    "    shape = x.get_shape().as_list()\n",
    "    shape[0] = int(-1)\n",
    "    return tf.reshape(tf.nn.softmax(tf.contrib.layers.flatten(x)),shape)\n",
    "\n",
    "def avg_softmax(x):\n",
    "    shape = x.get_shape().as_list()\n",
    "    shape[0] = int(-1)\n",
    "    totalSoft = tf.reshape(tf.nn.softmax(tf.contrib.layers.flatten(x)),shape)\n",
    "    return tf.reduce_sum(totalSoft,2)\n",
    "\n",
    "def max_softmax(x):\n",
    "    shape = x.get_shape().as_list()\n",
    "    shape[0] = int(-1)\n",
    "    totalSoft = tf.reshape(tf.nn.softmax(tf.contrib.layers.flatten(x)),shape)\n",
    "    return tf.reduce_max(totalSoft,2)\n",
    "\n",
    "def initACOL(in_size,clust,clss):\n",
    "    acolLayers = []\n",
    "    for i in range(clss):\n",
    "        acolLayers.append([\n",
    "            weight_variable([in_size, clustCount]),\n",
    "            bias_variable([clustCount])\n",
    "        ])\n",
    "    return acolLayers\n",
    "        \n",
    "def connectACOL(inLayer,acol):\n",
    "    clust = []\n",
    "    for l in range(0,len(acol)):\n",
    "        clust.append(tf.matmul(inLayer, acol[l][0]) + acol[l][1])\n",
    "    return clust\n",
    "        \n",
    "def acol(input,clust_count, class_count):\n",
    "    acolLayers = []\n",
    "    for i in range(class_count):\n",
    "        if isinstance(input, tuple):\n",
    "                input = input[0]\n",
    "\n",
    "        #I don't know what this bit does, but I don't think it'll hurt anything\n",
    "        #Or maybe it does, who knows\n",
    "        input_shape = input.get_shape()\n",
    "        if input_shape.ndims == 4:\n",
    "            dim = 1\n",
    "            for d in input_shape[1:].as_list():\n",
    "                dim *= d\n",
    "        #    feed_in = tf.reshape(tf.transpose(input,[0,3,1,2]), [-1, dim])\n",
    "        else:\n",
    "            feed_in, dim = (input, int(input_shape[-1]))\n",
    "\n",
    "        init_weights = tf.truncated_normal_initializer(0.0, stddev=0.1)#(0.0, stddev=0.01)\n",
    "        init_biases = tf.constant_initializer(1.0)#(0.1)\n",
    "\n",
    "        weights = weight_variable([dim, clust_count])\n",
    "        biases = bias_variable([clust_count])\n",
    "\n",
    "        acoll = tf.nn.xw_plus_b(input,weights,biases)\n",
    "        acolLayers.append(acol)\n",
    "    return acolLayers    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create placeholders (weights&biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if net==0:\n",
    "    #Init model weights & biases\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    x_image = tf.reshape(x, [-1,28,28,1])\n",
    "    #y_ = tf.placeholder(tf.float32, shape=[None, classCount])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None,classCount])\n",
    "    \n",
    "    #conv_layer1\n",
    "    W_conv1 = weight_variable([5,5,1,32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "\n",
    "    #conv_layer2\n",
    "    W_conv2 = weight_variable([5,5,32,64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "\n",
    "    #fc layer 1\n",
    "    W_fc1 = weight_variable([7*7*64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "\n",
    "    #acol = initACOL(1024,clustCount,classCount)\n",
    "\n",
    "    #final fc layer\n",
    "    W_fc2 = weight_variable([1024, classCount])\n",
    "    b_fc2 = bias_variable([classCount])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if net==0:\n",
    "    dropout=0.3\n",
    "    #Define net\n",
    "    #conv 1\n",
    "    l_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    l_pool1 = max_pool_2x2(l_conv1)\n",
    "\n",
    "    #conv 2\n",
    "    l_conv2 = tf.nn.relu(conv2d(l_pool1, W_conv2) + b_conv2)\n",
    "    l_pool2 = max_pool_2x2(l_conv2)\n",
    "\n",
    "    #fc 1\n",
    "    l_pool2_flat = tf.reshape(l_pool2, [-1, 7*7*64])\n",
    "    l_fc1 = tf.nn.relu(tf.matmul(l_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    l_fc1_drop = tf.nn.dropout(l_fc1, tf.constant(dropout))\n",
    "\n",
    "    y_conv = tf.nn.softmax(tf.matmul(l_fc1_drop, W_fc2) + b_fc2)\n",
    "    \n",
    "    #l_acol = connectACOL(l_fc1_drop,acol)\n",
    "    #l_acol = acol(l_fc1_drop,clustCount,classCount)\n",
    "\n",
    "    #Classification layer\n",
    "    #stackedClusts = tf.stack(l_acol,1)\n",
    "    #softmaxMat = matrix_softmax(stackedClusts)\n",
    "    #smStacked = tf.reduce_max(softmaxMat,2)\n",
    "\n",
    "    #y_conv = smStacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if net==1:\n",
    "    #Init model weights & biases\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    x_image = tf.reshape(x, [-1,28,28,1])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, classCount])\n",
    "    y2_ = tf.placeholder(tf.float32, shape=[None,classCount,clustCount])\n",
    "    \n",
    "    #keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    #conv_layer1\n",
    "    W_conv1 = weight_variable([3,3,1,32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "\n",
    "    #conv_layer2\n",
    "    W_conv2 = weight_variable([3,3,32,32])\n",
    "    b_conv2 = bias_variable([32])\n",
    "\n",
    "    #conv_layer3\n",
    "    W_conv3 = weight_variable([3,3,32,64])\n",
    "    b_conv3 = bias_variable([64])\n",
    "\n",
    "    #conv_layer4\n",
    "    W_conv4 = weight_variable([3,3,64,64])\n",
    "    b_conv4 = bias_variable([64])\n",
    "\n",
    "    #fc layer 1\n",
    "    W_fc1 = weight_variable([7*7*64, 2048])\n",
    "    b_fc1 = bias_variable([2048])\n",
    "\n",
    "    #acol = initACOL(2048,clustCount,classCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if net==1:\n",
    "    #Define net\n",
    "    #conv 1\n",
    "    l_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    #conv 2\n",
    "    l_conv2 = tf.nn.relu(conv2d(l_conv1, W_conv2) + b_conv2)\n",
    "    l_pool2 = max_pool_2x2(l_conv2)\n",
    "\n",
    "    l_drop1 = tf.nn.dropout(l_pool2, tf.constant(0.25))\n",
    "\n",
    "    #conv 3\n",
    "    l_conv3 = tf.nn.relu(conv2d(l_drop1, W_conv3) + b_conv3)\n",
    "    #conv 4\n",
    "    l_conv4 = tf.nn.relu(conv2d(l_conv3, W_conv4) + b_conv4)\n",
    "    l_pool4 = max_pool_2x2(l_conv4)\n",
    "\n",
    "    l_drop2 = tf.nn.dropout(l_pool4, tf.constant(0.25))\n",
    "\n",
    "    #fc 1\n",
    "    l_pool2_flat = tf.reshape(l_drop2, [-1, 7*7*64])\n",
    "\n",
    "    l_fc1 = tf.nn.relu(tf.matmul(l_pool2_flat, W_fc1) + b_fc1)\n",
    "    \n",
    "    l_fc1_drop = tf.nn.dropout(l_fc1, tf.constant(0.5))\n",
    "    \n",
    "    #l_acol = connectACOL(l_fc1_drop,acol)\n",
    "    l_acol = acol(l_fc1_drop,clustCount, classCount)\n",
    "\n",
    "    #Classification layer\n",
    "    stackedClusts = tf.stack(l_acol,1)\n",
    "    softmaxMat = matrix_softmax(stackedClusts)\n",
    "    smStacked = tf.reduce_max(softmaxMat,2)\n",
    "\n",
    "    y_conv = smStacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helperfunctions for loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#helper loss funcs\n",
    "def zBar(x):\n",
    "    xshape = x.shape.as_list()\n",
    "    s=[-1,xshape[1]*xshape[2]]\n",
    "    return tf.maximum(tf.reshape(x,s),0)\n",
    "    \n",
    "def bigU(zb):\n",
    "    return tf.matmul(tf.transpose(zb),zb)\n",
    "\n",
    "def selectNonDiag(x):\n",
    "    selection = np.ones(x.shape.as_list()[0],dtype='float32') - np.eye(x.shape.as_list()[0],dtype='float32')\n",
    "    return tf.reduce_sum(tf.multiply(x,selection))\n",
    "\n",
    "def bigV(x):\n",
    "    smallNu=tf.reshape(tf.reduce_sum(x,axis=0),[1,-1])\n",
    "    return tf.multiply(tf.transpose(smallNu),smallNu)\n",
    "\n",
    "def specialNormalise(x):\n",
    "    top = selectNonDiag(x)\n",
    "    bottom = tf.multiply(tf.to_float(x.shape[1]-1),tf.reduce_sum(tf.multiply(x,np.eye(x.shape[1],dtype='float32'))))\n",
    "    return tf.divide(top,bottom)\n",
    "\n",
    "def frobNorm(x):\n",
    "    return tf.sqrt(tf.reduce_sum(tf.square(x)))\n",
    "\n",
    "tresh = tf.constant(0.03)\n",
    "cc0=1.0\n",
    "cc1=1.0\n",
    "cc2=1.0\n",
    "cc3=0.0003\n",
    "cc4=0.000001\n",
    "cc5=1.0\n",
    "c0 = tf.constant(cc0)\n",
    "c1 = tf.constant(cc1)\n",
    "c2 = tf.constant(cc2)\n",
    "c3val = tf.constant(cc3)\n",
    "c3 = lambda affinity: tf.cond(tf.less(affinity,tresh),lambda: c3val,lambda: tf.constant(0.0))\n",
    "c4 =tf.constant(cc4)\n",
    "c5 = tf.constant(cc5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#calculate losses\n",
    "#affinity\n",
    "#bZ = zBar(stackedClusts)#softmaxMat)\n",
    "#bU = bigU(bZ)\n",
    "#coact = selectNonDiag(bU)\n",
    "#affinity = specialNormalise(bU)\n",
    "\n",
    "#balance\n",
    "#bV=bigV(bZ)\n",
    "#balance = specialNormalise(bV)\n",
    "\n",
    "#cluster cross entropy\n",
    "#clust_cross_entropy = tf.reduce_mean(-tf.reduce_sum(y2_ * tf.log(tf.clip_by_value(softmaxMat,1e-10,1.0)), reduction_indices=[1,2]))\n",
    "\n",
    "#cross entropy\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y_conv,1e-10,1.0)), reduction_indices=[1]))\n",
    "\n",
    "#frob = frobNorm(stackedClusts)#softmaxMat)\n",
    "\n",
    "loss = c0*cross_entropy# + c5*clust_cross_entropy + c4*frob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#y = {0:[0,1], 1:[1,0]}\n",
    "y = {0:[1,0,0,0,0],\n",
    "     1:[1,0,0,0,0],\n",
    "     2:[0,1,0,0,0],\n",
    "     3:[0,1,0,0,0],\n",
    "     4:[0,0,1,0,0],\n",
    "     5:[0,0,1,0,0],\n",
    "     6:[0,0,0,1,0],\n",
    "     7:[0,0,0,1,0],\n",
    "     8:[0,0,0,0,1],\n",
    "     9:[0,0,0,0,1]}\n",
    "\n",
    "y = {0:[1,0],\n",
    "     1:[1,0],\n",
    "     2:[1,0],\n",
    "     3:[1,0],\n",
    "     4:[1,0],\n",
    "     5:[0,1],\n",
    "     6:[0,1],\n",
    "     7:[0,1],\n",
    "     8:[0,1],\n",
    "     9:[0,1]}\n",
    "\n",
    "y2 = {\n",
    "    0:np.zeros((classCount,clustCount)),\n",
    "    1:np.zeros((classCount,clustCount)),\n",
    "    2:np.zeros((classCount,clustCount)),\n",
    "    3:np.zeros((classCount,clustCount)),\n",
    "    4:np.zeros((classCount,clustCount)),\n",
    "    5:np.zeros((classCount,clustCount)),\n",
    "    6:np.zeros((classCount,clustCount)),\n",
    "    7:np.zeros((classCount,clustCount)),\n",
    "    8:np.zeros((classCount,clustCount)),\n",
    "    9:np.zeros((classCount,clustCount))\n",
    "}\n",
    "\n",
    "y2[0][0,0] = 1\n",
    "y2[1][0,1] = 1\n",
    "y2[2][0,2] = 1\n",
    "y2[3][0,3] = 1\n",
    "y2[4][0,4] = 1\n",
    "y2[5][1,0] = 1\n",
    "y2[6][1,1] = 1\n",
    "y2[7][1,2] = 1\n",
    "y2[8][1,3] = 1\n",
    "y2[9][1,4] = 1\n",
    "\n",
    "emptyy2 = {\n",
    "    0:np.zeros((classCount,clustCount)),\n",
    "    1:np.zeros((classCount,clustCount)),\n",
    "    2:np.zeros((classCount,clustCount)),\n",
    "    3:np.zeros((classCount,clustCount)),\n",
    "    4:np.zeros((classCount,clustCount)),\n",
    "    5:np.zeros((classCount,clustCount)),\n",
    "    6:np.zeros((classCount,clustCount)),\n",
    "    7:np.zeros((classCount,clustCount)),\n",
    "    8:np.zeros((classCount,clustCount)),\n",
    "    9:np.zeros((classCount,clustCount))\n",
    "}\n",
    "\n",
    "totalSteps = trainsteps\n",
    "stepCount=0\n",
    "batchSize = 128\n",
    "hist = {\n",
    "    'train_acc':[],\n",
    "    'val_acc':[],\n",
    "    'train_loss':[],\n",
    "    'val_loss':[],\n",
    "    'affinity':[],\n",
    "    'balance':[],\n",
    "    'coactivity':[],\n",
    "    'clustEntr':[]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "step 0/50000 \n",
      " Train: accuracy: 0.460938, loss: 4.59776 \n",
      " Validation: accuracy: 0.492188 loss: 4.84387\n",
      " cross_entropy: 4.75707\n",
      "step 100/50000 \n",
      " Train: accuracy: 0.695312, loss: 1.71639 \n",
      " Validation: accuracy: 0.53125 loss: 2.81\n",
      " cross_entropy: 1.9357\n",
      "step 200/50000 \n",
      " Train: accuracy: 0.703125, loss: 1.76605 \n",
      " Validation: accuracy: 0.664062 loss: 1.6067\n",
      " cross_entropy: 1.3811\n",
      "step 300/50000 \n",
      " Train: accuracy: 0.679688, loss: 1.51855 \n",
      " Validation: accuracy: 0.710938 loss: 1.25923\n",
      " cross_entropy: 1.54999\n",
      "step 400/50000 \n",
      " Train: accuracy: 0.734375, loss: 0.961076 \n",
      " Validation: accuracy: 0.765625 loss: 0.933929\n",
      " cross_entropy: 1.30269\n",
      "step 500/50000 \n",
      " Train: accuracy: 0.78125, loss: 0.795763 \n",
      " Validation: accuracy: 0.867188 loss: 0.536283\n",
      " cross_entropy: 0.797085\n",
      "step 600/50000 \n",
      " Train: accuracy: 0.789062, loss: 0.798129 \n",
      " Validation: accuracy: 0.804688 loss: 0.624738\n",
      " cross_entropy: 0.564454\n",
      "step 700/50000 \n",
      " Train: accuracy: 0.875, loss: 0.357113 \n",
      " Validation: accuracy: 0.828125 loss: 0.535932\n",
      " cross_entropy: 0.44003\n",
      "step 800/50000 \n",
      " Train: accuracy: 0.820312, loss: 0.461662 \n",
      " Validation: accuracy: 0.828125 loss: 0.511839\n",
      " cross_entropy: 0.481292\n",
      "step 900/50000 \n",
      " Train: accuracy: 0.875, loss: 0.41196 \n",
      " Validation: accuracy: 0.890625 loss: 0.24815\n",
      " cross_entropy: 0.573857\n",
      "step 1000/50000 \n",
      " Train: accuracy: 0.828125, loss: 0.516765 \n",
      " Validation: accuracy: 0.867188 loss: 0.356207\n",
      " cross_entropy: 0.299314\n",
      "step 1100/50000 \n",
      " Train: accuracy: 0.929688, loss: 0.306992 \n",
      " Validation: accuracy: 0.890625 loss: 0.309042\n",
      " cross_entropy: 0.289781\n",
      "step 1200/50000 \n",
      " Train: accuracy: 0.867188, loss: 0.285482 \n",
      " Validation: accuracy: 0.90625 loss: 0.293076\n",
      " cross_entropy: 0.24481\n",
      "step 1300/50000 \n",
      " Train: accuracy: 0.859375, loss: 0.346243 \n",
      " Validation: accuracy: 0.851562 loss: 0.325865\n",
      " cross_entropy: 0.303509\n",
      "step 1400/50000 \n",
      " Train: accuracy: 0.867188, loss: 0.486065 \n",
      " Validation: accuracy: 0.898438 loss: 0.303272\n",
      " cross_entropy: 0.329818\n",
      "step 1500/50000 \n",
      " Train: accuracy: 0.921875, loss: 0.216211 \n",
      " Validation: accuracy: 0.898438 loss: 0.239739\n",
      " cross_entropy: 0.224786\n",
      "step 1600/50000 \n",
      " Train: accuracy: 0.914062, loss: 0.216844 \n",
      " Validation: accuracy: 0.890625 loss: 0.235652\n",
      " cross_entropy: 0.153592\n",
      "step 1700/50000 \n",
      " Train: accuracy: 0.90625, loss: 0.210704 \n",
      " Validation: accuracy: 0.921875 loss: 0.157151\n",
      " cross_entropy: 0.153559\n",
      "step 1800/50000 \n",
      " Train: accuracy: 0.960938, loss: 0.107071 \n",
      " Validation: accuracy: 0.914062 loss: 0.216394\n",
      " cross_entropy: 0.129188\n",
      "step 1900/50000 \n",
      " Train: accuracy: 0.898438, loss: 0.207133 \n",
      " Validation: accuracy: 0.90625 loss: 0.210399\n",
      " cross_entropy: 0.141645\n",
      "step 2000/50000 \n",
      " Train: accuracy: 0.914062, loss: 0.147366 \n",
      " Validation: accuracy: 0.945312 loss: 0.113162\n",
      " cross_entropy: 0.234484\n",
      "step 2100/50000 \n",
      " Train: accuracy: 0.90625, loss: 0.242294 \n",
      " Validation: accuracy: 0.914062 loss: 0.207433\n",
      " cross_entropy: 0.164914\n",
      "step 2200/50000 \n",
      " Train: accuracy: 0.960938, loss: 0.121976 \n",
      " Validation: accuracy: 0.945312 loss: 0.138342\n",
      " cross_entropy: 0.148153\n",
      "step 2300/50000 \n",
      " Train: accuracy: 0.929688, loss: 0.227596 \n",
      " Validation: accuracy: 0.960938 loss: 0.196527\n",
      " cross_entropy: 0.239735\n",
      "step 2400/50000 \n",
      " Train: accuracy: 0.9375, loss: 0.151353 \n",
      " Validation: accuracy: 0.914062 loss: 0.157406\n",
      " cross_entropy: 0.162948\n",
      "step 2500/50000 \n",
      " Train: accuracy: 0.953125, loss: 0.11247 \n",
      " Validation: accuracy: 0.9375 loss: 0.131083\n",
      " cross_entropy: 0.180694\n",
      "step 2600/50000 \n",
      " Train: accuracy: 0.960938, loss: 0.118077 \n",
      " Validation: accuracy: 0.976562 loss: 0.0948789\n",
      " cross_entropy: 0.106505\n",
      "step 2700/50000 \n",
      " Train: accuracy: 0.9375, loss: 0.135049 \n",
      " Validation: accuracy: 0.9375 loss: 0.148374\n",
      " cross_entropy: 0.158239\n",
      "step 2800/50000 \n",
      " Train: accuracy: 0.96875, loss: 0.106496 \n",
      " Validation: accuracy: 0.960938 loss: 0.148167\n",
      " cross_entropy: 0.0919074\n",
      "step 2900/50000 \n",
      " Train: accuracy: 0.953125, loss: 0.101095 \n",
      " Validation: accuracy: 0.914062 loss: 0.141395\n",
      " cross_entropy: 0.0518086\n",
      "step 3000/50000 \n",
      " Train: accuracy: 0.953125, loss: 0.13063 \n",
      " Validation: accuracy: 0.945312 loss: 0.122939\n",
      " cross_entropy: 0.147747\n",
      "step 3100/50000 \n",
      " Train: accuracy: 0.953125, loss: 0.0840111 \n",
      " Validation: accuracy: 0.960938 loss: 0.0906981\n",
      " cross_entropy: 0.149182\n",
      "step 3200/50000 \n",
      " Train: accuracy: 0.898438, loss: 0.165807 \n",
      " Validation: accuracy: 0.976562 loss: 0.0860956\n",
      " cross_entropy: 0.0949913\n",
      "step 3300/50000 \n",
      " Train: accuracy: 0.953125, loss: 0.113617 \n",
      " Validation: accuracy: 0.960938 loss: 0.0885805\n",
      " cross_entropy: 0.115311\n",
      "step 3400/50000 \n",
      " Train: accuracy: 0.960938, loss: 0.110323 \n",
      " Validation: accuracy: 0.984375 loss: 0.061424\n",
      " cross_entropy: 0.124802\n",
      "step 3500/50000 \n",
      " Train: accuracy: 0.9375, loss: 0.111412 \n",
      " Validation: accuracy: 0.960938 loss: 0.122325\n",
      " cross_entropy: 0.0787591\n",
      "step 3600/50000 \n",
      " Train: accuracy: 0.953125, loss: 0.103701 \n",
      " Validation: accuracy: 0.945312 loss: 0.120578\n",
      " cross_entropy: 0.0740869\n",
      "step 3700/50000 \n",
      " Train: accuracy: 0.9375, loss: 0.172125 \n",
      " Validation: accuracy: 0.984375 loss: 0.0552547\n",
      " cross_entropy: 0.16511\n",
      "step 3800/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0783814 \n",
      " Validation: accuracy: 0.929688 loss: 0.223121\n",
      " cross_entropy: 0.0654934\n",
      "step 3900/50000 \n",
      " Train: accuracy: 0.953125, loss: 0.11494 \n",
      " Validation: accuracy: 0.960938 loss: 0.148664\n",
      " cross_entropy: 0.105244\n",
      "step 4000/50000 \n",
      " Train: accuracy: 0.945312, loss: 0.124553 \n",
      " Validation: accuracy: 0.984375 loss: 0.0518272\n",
      " cross_entropy: 0.0918926\n",
      "step 4100/50000 \n",
      " Train: accuracy: 0.953125, loss: 0.0944916 \n",
      " Validation: accuracy: 0.953125 loss: 0.181311\n",
      " cross_entropy: 0.078441\n",
      "step 4200/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0461003 \n",
      " Validation: accuracy: 0.976562 loss: 0.0687386\n",
      " cross_entropy: 0.0475147\n",
      "step 4300/50000 \n",
      " Train: accuracy: 0.9375, loss: 0.159915 \n",
      " Validation: accuracy: 0.953125 loss: 0.116673\n",
      " cross_entropy: 0.0902708\n",
      "step 4400/50000 \n",
      " Train: accuracy: 0.96875, loss: 0.0856824 \n",
      " Validation: accuracy: 0.96875 loss: 0.0645232\n",
      " cross_entropy: 0.0753112\n",
      "step 4500/50000 \n",
      " Train: accuracy: 0.9375, loss: 0.116927 \n",
      " Validation: accuracy: 0.992188 loss: 0.106424\n",
      " cross_entropy: 0.1082\n",
      "step 4600/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0716534 \n",
      " Validation: accuracy: 0.984375 loss: 0.0437164\n",
      " cross_entropy: 0.0547478\n",
      "step 4700/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0561262 \n",
      " Validation: accuracy: 0.992188 loss: 0.0384052\n",
      " cross_entropy: 0.0472922\n",
      "step 4800/50000 \n",
      " Train: accuracy: 0.96875, loss: 0.12062 \n",
      " Validation: accuracy: 0.976562 loss: 0.0469578\n",
      " cross_entropy: 0.0839012\n",
      "step 4900/50000 \n",
      " Train: accuracy: 0.960938, loss: 0.153669 \n",
      " Validation: accuracy: 0.953125 loss: 0.111173\n",
      " cross_entropy: 0.129479\n",
      "step 5000/50000 \n",
      " Train: accuracy: 0.960938, loss: 0.0733671 \n",
      " Validation: accuracy: 0.945312 loss: 0.0753407\n",
      " cross_entropy: 0.0747733\n",
      "step 5100/50000 \n",
      " Train: accuracy: 0.960938, loss: 0.0847644 \n",
      " Validation: accuracy: 0.984375 loss: 0.0396511\n",
      " cross_entropy: 0.0750866\n",
      "step 5200/50000 \n",
      " Train: accuracy: 0.96875, loss: 0.0866581 \n",
      " Validation: accuracy: 0.976562 loss: 0.0660357\n",
      " cross_entropy: 0.0722344\n",
      "step 5300/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0657658 \n",
      " Validation: accuracy: 0.976562 loss: 0.0848764\n",
      " cross_entropy: 0.0565417\n",
      "step 5400/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.048332 \n",
      " Validation: accuracy: 0.984375 loss: 0.0591588\n",
      " cross_entropy: 0.0302368\n",
      "step 5500/50000 \n",
      " Train: accuracy: 0.945312, loss: 0.0861175 \n",
      " Validation: accuracy: 0.976562 loss: 0.0443603\n",
      " cross_entropy: 0.0722231\n",
      "step 5600/50000 \n",
      " Train: accuracy: 0.953125, loss: 0.0976477 \n",
      " Validation: accuracy: 0.992188 loss: 0.0361622\n",
      " cross_entropy: 0.0577851\n",
      "step 5700/50000 \n",
      " Train: accuracy: 0.960938, loss: 0.075977 \n",
      " Validation: accuracy: 0.976562 loss: 0.0524296\n",
      " cross_entropy: 0.077029\n",
      "step 5800/50000 \n",
      " Train: accuracy: 0.96875, loss: 0.07975 \n",
      " Validation: accuracy: 0.976562 loss: 0.0608454\n",
      " cross_entropy: 0.0864753\n",
      "step 5900/50000 \n",
      " Train: accuracy: 1, loss: 0.0154708 \n",
      " Validation: accuracy: 0.960938 loss: 0.124961\n",
      " cross_entropy: 0.0468048\n",
      "step 6000/50000 \n",
      " Train: accuracy: 0.96875, loss: 0.104222 \n",
      " Validation: accuracy: 0.992188 loss: 0.0387874\n",
      " cross_entropy: 0.0630161\n",
      "step 6100/50000 \n",
      " Train: accuracy: 0.953125, loss: 0.0957441 \n",
      " Validation: accuracy: 0.984375 loss: 0.0333736\n",
      " cross_entropy: 0.0641932\n",
      "step 6200/50000 \n",
      " Train: accuracy: 0.96875, loss: 0.0637138 \n",
      " Validation: accuracy: 0.976562 loss: 0.0411915\n",
      " cross_entropy: 0.0524888\n",
      "step 6300/50000 \n",
      " Train: accuracy: 0.953125, loss: 0.122697 \n",
      " Validation: accuracy: 0.984375 loss: 0.0878295\n",
      " cross_entropy: 0.106521\n",
      "step 6400/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0461416 \n",
      " Validation: accuracy: 0.976562 loss: 0.0507764\n",
      " cross_entropy: 0.0504096\n",
      "step 6500/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0394135 \n",
      " Validation: accuracy: 0.976562 loss: 0.0655955\n",
      " cross_entropy: 0.0799955\n",
      "step 6600/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0308017 \n",
      " Validation: accuracy: 0.96875 loss: 0.0990018\n",
      " cross_entropy: 0.0284173\n",
      "step 6700/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0265617 \n",
      " Validation: accuracy: 0.96875 loss: 0.05597\n",
      " cross_entropy: 0.0267287\n",
      "step 6800/50000 \n",
      " Train: accuracy: 0.96875, loss: 0.0576866 \n",
      " Validation: accuracy: 0.984375 loss: 0.0339632\n",
      " cross_entropy: 0.0396467\n",
      "step 6900/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0912376 \n",
      " Validation: accuracy: 0.992188 loss: 0.0230288\n",
      " cross_entropy: 0.0841707\n",
      "step 7000/50000 \n",
      " Train: accuracy: 0.96875, loss: 0.0840799 \n",
      " Validation: accuracy: 0.992188 loss: 0.027644\n",
      " cross_entropy: 0.0511299\n",
      "step 7100/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0541236 \n",
      " Validation: accuracy: 0.976562 loss: 0.0691972\n",
      " cross_entropy: 0.0282521\n",
      "step 7200/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0964656 \n",
      " Validation: accuracy: 0.96875 loss: 0.0762878\n",
      " cross_entropy: 0.0582567\n",
      "step 7300/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0406462 \n",
      " Validation: accuracy: 1 loss: 0.0184549\n",
      " cross_entropy: 0.0288111\n",
      "step 7400/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0453005 \n",
      " Validation: accuracy: 0.976562 loss: 0.0540615\n",
      " cross_entropy: 0.0374177\n",
      "step 7500/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0313686 \n",
      " Validation: accuracy: 0.976562 loss: 0.0604352\n",
      " cross_entropy: 0.0612856\n",
      "step 7600/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0433539 \n",
      " Validation: accuracy: 0.984375 loss: 0.0289227\n",
      " cross_entropy: 0.0443117\n",
      "step 7700/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0658523 \n",
      " Validation: accuracy: 0.960938 loss: 0.0864427\n",
      " cross_entropy: 0.0961267\n",
      "step 7800/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0488084 \n",
      " Validation: accuracy: 0.984375 loss: 0.0517709\n",
      " cross_entropy: 0.0412972\n",
      "step 7900/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0647189 \n",
      " Validation: accuracy: 0.976562 loss: 0.0458421\n",
      " cross_entropy: 0.0577204\n",
      "step 8000/50000 \n",
      " Train: accuracy: 0.960938, loss: 0.0944012 \n",
      " Validation: accuracy: 1 loss: 0.0283309\n",
      " cross_entropy: 0.0785337\n",
      "step 8100/50000 \n",
      " Train: accuracy: 0.96875, loss: 0.0661357 \n",
      " Validation: accuracy: 0.984375 loss: 0.0350894\n",
      " cross_entropy: 0.0375857\n",
      "step 8200/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0315071 \n",
      " Validation: accuracy: 0.992188 loss: 0.0415609\n",
      " cross_entropy: 0.0296954\n",
      "step 8300/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0374384 \n",
      " Validation: accuracy: 0.992188 loss: 0.0424279\n",
      " cross_entropy: 0.0578084\n",
      "step 8400/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.051527 \n",
      " Validation: accuracy: 0.984375 loss: 0.0534686\n",
      " cross_entropy: 0.0694079\n",
      "step 8500/50000 \n",
      " Train: accuracy: 1, loss: 0.0289936 \n",
      " Validation: accuracy: 1 loss: 0.0156243\n",
      " cross_entropy: 0.0168169\n",
      "step 8600/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0410194 \n",
      " Validation: accuracy: 0.960938 loss: 0.0654966\n",
      " cross_entropy: 0.0826609\n",
      "step 8700/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0280202 \n",
      " Validation: accuracy: 1 loss: 0.0283234\n",
      " cross_entropy: 0.0302894\n",
      "step 8800/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0456178 \n",
      " Validation: accuracy: 0.992188 loss: 0.0261969\n",
      " cross_entropy: 0.0474127\n",
      "step 8900/50000 \n",
      " Train: accuracy: 0.960938, loss: 0.0631038 \n",
      " Validation: accuracy: 0.953125 loss: 0.0707619\n",
      " cross_entropy: 0.085709\n",
      "step 9000/50000 \n",
      " Train: accuracy: 0.96875, loss: 0.0782814 \n",
      " Validation: accuracy: 1 loss: 0.023103\n",
      " cross_entropy: 0.0315689\n",
      "step 9100/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0874561 \n",
      " Validation: accuracy: 0.96875 loss: 0.0631018\n",
      " cross_entropy: 0.0469046\n",
      "step 9200/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.039794 \n",
      " Validation: accuracy: 0.992188 loss: 0.034953\n",
      " cross_entropy: 0.0271967\n",
      "step 9300/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0300207 \n",
      " Validation: accuracy: 0.96875 loss: 0.0568496\n",
      " cross_entropy: 0.0340969\n",
      "step 9400/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0556938 \n",
      " Validation: accuracy: 0.992188 loss: 0.0326741\n",
      " cross_entropy: 0.0631395\n",
      "step 9500/50000 \n",
      " Train: accuracy: 0.960938, loss: 0.0693412 \n",
      " Validation: accuracy: 0.984375 loss: 0.0355508\n",
      " cross_entropy: 0.024096\n",
      "step 9600/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0353135 \n",
      " Validation: accuracy: 0.984375 loss: 0.0483884\n",
      " cross_entropy: 0.023784\n",
      "step 9700/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0325597 \n",
      " Validation: accuracy: 0.992188 loss: 0.0169706\n",
      " cross_entropy: 0.0109057\n",
      "step 9800/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0333523 \n",
      " Validation: accuracy: 0.992188 loss: 0.0242038\n",
      " cross_entropy: 0.0290438\n",
      "step 9900/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0451684 \n",
      " Validation: accuracy: 0.96875 loss: 0.0457349\n",
      " cross_entropy: 0.0469495\n",
      "step 10000/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0448618 \n",
      " Validation: accuracy: 0.984375 loss: 0.0270246\n",
      " cross_entropy: 0.0451071\n",
      "step 10100/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0357488 \n",
      " Validation: accuracy: 0.992188 loss: 0.0219619\n",
      " cross_entropy: 0.0463556\n",
      "step 10200/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0369668 \n",
      " Validation: accuracy: 0.976562 loss: 0.0465063\n",
      " cross_entropy: 0.108967\n",
      "step 10300/50000 \n",
      " Train: accuracy: 1, loss: 0.0163121 \n",
      " Validation: accuracy: 1 loss: 0.0115872\n",
      " cross_entropy: 0.0413413\n",
      "step 10400/50000 \n",
      " Train: accuracy: 0.96875, loss: 0.0553024 \n",
      " Validation: accuracy: 0.984375 loss: 0.0685969\n",
      " cross_entropy: 0.057284\n",
      "step 10500/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0376408 \n",
      " Validation: accuracy: 0.96875 loss: 0.0589589\n",
      " cross_entropy: 0.0524607\n",
      "step 10600/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.011573 \n",
      " Validation: accuracy: 0.960938 loss: 0.0704776\n",
      " cross_entropy: 0.0271376\n",
      "step 10700/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0503797 \n",
      " Validation: accuracy: 0.992188 loss: 0.0288832\n",
      " cross_entropy: 0.0607322\n",
      "step 10800/50000 \n",
      " Train: accuracy: 1, loss: 0.0147247 \n",
      " Validation: accuracy: 0.96875 loss: 0.0552942\n",
      " cross_entropy: 0.023452\n",
      "step 10900/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0440215 \n",
      " Validation: accuracy: 0.976562 loss: 0.0576088\n",
      " cross_entropy: 0.0248604\n",
      "step 11000/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0442487 \n",
      " Validation: accuracy: 0.992188 loss: 0.0312172\n",
      " cross_entropy: 0.0171323\n",
      "step 11100/50000 \n",
      " Train: accuracy: 1, loss: 0.0175558 \n",
      " Validation: accuracy: 0.976562 loss: 0.0358089\n",
      " cross_entropy: 0.041736\n",
      "step 11200/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0173375 \n",
      " Validation: accuracy: 0.976562 loss: 0.0371074\n",
      " cross_entropy: 0.0283911\n",
      "step 11300/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0263809 \n",
      " Validation: accuracy: 0.984375 loss: 0.0599204\n",
      " cross_entropy: 0.0684562\n",
      "step 11400/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0651706 \n",
      " Validation: accuracy: 0.976562 loss: 0.0543158\n",
      " cross_entropy: 0.036698\n",
      "step 11500/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0289628 \n",
      " Validation: accuracy: 0.984375 loss: 0.043495\n",
      " cross_entropy: 0.0277467\n",
      "step 11600/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0330698 \n",
      " Validation: accuracy: 1 loss: 0.0237166\n",
      " cross_entropy: 0.0224429\n",
      "step 11700/50000 \n",
      " Train: accuracy: 0.960938, loss: 0.0849602 \n",
      " Validation: accuracy: 0.984375 loss: 0.0366673\n",
      " cross_entropy: 0.0513153\n",
      "step 11800/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0285721 \n",
      " Validation: accuracy: 1 loss: 0.00610225\n",
      " cross_entropy: 0.0184799\n",
      "step 11900/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0328502 \n",
      " Validation: accuracy: 0.984375 loss: 0.0418709\n",
      " cross_entropy: 0.0497434\n",
      "step 12000/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0454644 \n",
      " Validation: accuracy: 0.992188 loss: 0.0196878\n",
      " cross_entropy: 0.0343454\n",
      "step 12100/50000 \n",
      " Train: accuracy: 0.96875, loss: 0.0984305 \n",
      " Validation: accuracy: 0.992188 loss: 0.0231062\n",
      " cross_entropy: 0.0677964\n",
      "step 12200/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0342597 \n",
      " Validation: accuracy: 1 loss: 0.00907589\n",
      " cross_entropy: 0.0324905\n",
      "step 12300/50000 \n",
      " Train: accuracy: 1, loss: 0.0149753 \n",
      " Validation: accuracy: 0.992188 loss: 0.0248671\n",
      " cross_entropy: 0.0504956\n",
      "step 12400/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0608495 \n",
      " Validation: accuracy: 0.984375 loss: 0.0280372\n",
      " cross_entropy: 0.0681218\n",
      "step 12500/50000 \n",
      " Train: accuracy: 0.96875, loss: 0.0568676 \n",
      " Validation: accuracy: 0.984375 loss: 0.0510283\n",
      " cross_entropy: 0.0277593\n",
      "step 12600/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0492079 \n",
      " Validation: accuracy: 1 loss: 0.0149205\n",
      " cross_entropy: 0.0177597\n",
      "step 12700/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0326833 \n",
      " Validation: accuracy: 0.992188 loss: 0.022588\n",
      " cross_entropy: 0.034799\n",
      "step 12800/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0616388 \n",
      " Validation: accuracy: 1 loss: 0.0195367\n",
      " cross_entropy: 0.0357376\n",
      "step 12900/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0420524 \n",
      " Validation: accuracy: 0.992188 loss: 0.0388202\n",
      " cross_entropy: 0.0167579\n",
      "step 13000/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0226156 \n",
      " Validation: accuracy: 1 loss: 0.0159533\n",
      " cross_entropy: 0.0224579\n",
      "step 13100/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0296126 \n",
      " Validation: accuracy: 1 loss: 0.0159123\n",
      " cross_entropy: 0.0184936\n",
      "step 13200/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0267537 \n",
      " Validation: accuracy: 0.976562 loss: 0.0347888\n",
      " cross_entropy: 0.0262083\n",
      "step 13300/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0221907 \n",
      " Validation: accuracy: 1 loss: 0.0156996\n",
      " cross_entropy: 0.0161872\n",
      "step 13400/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0274948 \n",
      " Validation: accuracy: 0.984375 loss: 0.094008\n",
      " cross_entropy: 0.0225363\n",
      "step 13500/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0444661 \n",
      " Validation: accuracy: 0.984375 loss: 0.0298612\n",
      " cross_entropy: 0.0174251\n",
      "step 13600/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0374826 \n",
      " Validation: accuracy: 0.992188 loss: 0.0359875\n",
      " cross_entropy: 0.05874\n",
      "step 13700/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0340349 \n",
      " Validation: accuracy: 0.984375 loss: 0.0574996\n",
      " cross_entropy: 0.0672666\n",
      "step 13800/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0204267 \n",
      " Validation: accuracy: 0.960938 loss: 0.0791235\n",
      " cross_entropy: 0.0171699\n",
      "step 13900/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0479832 \n",
      " Validation: accuracy: 0.953125 loss: 0.0973565\n",
      " cross_entropy: 0.0277886\n",
      "step 14000/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0323043 \n",
      " Validation: accuracy: 1 loss: 0.0102354\n",
      " cross_entropy: 0.0067196\n",
      "step 14100/50000 \n",
      " Train: accuracy: 0.96875, loss: 0.0800055 \n",
      " Validation: accuracy: 0.992188 loss: 0.0134217\n",
      " cross_entropy: 0.050202\n",
      "step 14200/50000 \n",
      " Train: accuracy: 0.960938, loss: 0.0593083 \n",
      " Validation: accuracy: 0.984375 loss: 0.0327522\n",
      " cross_entropy: 0.0431817\n",
      "step 14300/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0341312 \n",
      " Validation: accuracy: 0.984375 loss: 0.041054\n",
      " cross_entropy: 0.0300656\n",
      "step 14400/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0521666 \n",
      " Validation: accuracy: 0.984375 loss: 0.0351008\n",
      " cross_entropy: 0.0455814\n",
      "step 14500/50000 \n",
      " Train: accuracy: 1, loss: 0.00214208 \n",
      " Validation: accuracy: 0.992188 loss: 0.0244159\n",
      " cross_entropy: 0.0071027\n",
      "step 14600/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0190411 \n",
      " Validation: accuracy: 0.992188 loss: 0.0165294\n",
      " cross_entropy: 0.0151989\n",
      "step 14700/50000 \n",
      " Train: accuracy: 0.953125, loss: 0.078352 \n",
      " Validation: accuracy: 1 loss: 0.00986932\n",
      " cross_entropy: 0.0670433\n",
      "step 14800/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.00970521 \n",
      " Validation: accuracy: 0.984375 loss: 0.0369373\n",
      " cross_entropy: 0.0134334\n",
      "step 14900/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0551805 \n",
      " Validation: accuracy: 1 loss: 0.0131919\n",
      " cross_entropy: 0.0735061\n",
      "step 15000/50000 \n",
      " Train: accuracy: 1, loss: 0.0106536 \n",
      " Validation: accuracy: 1 loss: 0.00843696\n",
      " cross_entropy: 0.0111272\n",
      "step 15100/50000 \n",
      " Train: accuracy: 1, loss: 0.009674 \n",
      " Validation: accuracy: 1 loss: 0.00451394\n",
      " cross_entropy: 0.0162243\n",
      "step 15200/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0789302 \n",
      " Validation: accuracy: 0.992188 loss: 0.0300367\n",
      " cross_entropy: 0.0797621\n",
      "step 15300/50000 \n",
      " Train: accuracy: 1, loss: 0.0109636 \n",
      " Validation: accuracy: 0.976562 loss: 0.0363654\n",
      " cross_entropy: 0.0113729\n",
      "step 15400/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0144452 \n",
      " Validation: accuracy: 1 loss: 0.0110452\n",
      " cross_entropy: 0.00838243\n",
      "step 15500/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0416699 \n",
      " Validation: accuracy: 1 loss: 0.00579711\n",
      " cross_entropy: 0.0437695\n",
      "step 15600/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0217357 \n",
      " Validation: accuracy: 0.96875 loss: 0.092187\n",
      " cross_entropy: 0.0171314\n",
      "step 15700/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0411931 \n",
      " Validation: accuracy: 0.992188 loss: 0.0460928\n",
      " cross_entropy: 0.0279919\n",
      "step 15800/50000 \n",
      " Train: accuracy: 1, loss: 0.0216712 \n",
      " Validation: accuracy: 1 loss: 0.00423004\n",
      " cross_entropy: 0.0364508\n",
      "step 15900/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0376792 \n",
      " Validation: accuracy: 0.984375 loss: 0.0297059\n",
      " cross_entropy: 0.0547112\n",
      "step 16000/50000 \n",
      " Train: accuracy: 1, loss: 0.0146968 \n",
      " Validation: accuracy: 0.984375 loss: 0.0386995\n",
      " cross_entropy: 0.0423741\n",
      "step 16100/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0529126 \n",
      " Validation: accuracy: 0.96875 loss: 0.082843\n",
      " cross_entropy: 0.0522558\n",
      "step 16200/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0463272 \n",
      " Validation: accuracy: 0.992188 loss: 0.0357226\n",
      " cross_entropy: 0.0359903\n",
      "step 16300/50000 \n",
      " Train: accuracy: 1, loss: 0.0163107 \n",
      " Validation: accuracy: 0.960938 loss: 0.0775096\n",
      " cross_entropy: 0.0340594\n",
      "step 16400/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0331466 \n",
      " Validation: accuracy: 0.992188 loss: 0.0211159\n",
      " cross_entropy: 0.0224174\n",
      "step 16500/50000 \n",
      " Train: accuracy: 1, loss: 0.0114663 \n",
      " Validation: accuracy: 0.992188 loss: 0.0262937\n",
      " cross_entropy: 0.0174701\n",
      "step 16600/50000 \n",
      " Train: accuracy: 0.953125, loss: 0.0746828 \n",
      " Validation: accuracy: 0.992188 loss: 0.0205351\n",
      " cross_entropy: 0.0356362\n",
      "step 16700/50000 \n",
      " Train: accuracy: 1, loss: 0.0151003 \n",
      " Validation: accuracy: 0.976562 loss: 0.0778499\n",
      " cross_entropy: 0.0248058\n",
      "step 16800/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0349653 \n",
      " Validation: accuracy: 0.984375 loss: 0.0196991\n",
      " cross_entropy: 0.0208728\n",
      "step 16900/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0598529 \n",
      " Validation: accuracy: 0.976562 loss: 0.0452431\n",
      " cross_entropy: 0.0843093\n",
      "step 17000/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0508949 \n",
      " Validation: accuracy: 0.992188 loss: 0.0357682\n",
      " cross_entropy: 0.0298043\n",
      "step 17100/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0234088 \n",
      " Validation: accuracy: 0.992188 loss: 0.0342998\n",
      " cross_entropy: 0.0259702\n",
      "step 17200/50000 \n",
      " Train: accuracy: 1, loss: 0.00471114 \n",
      " Validation: accuracy: 0.992188 loss: 0.0200956\n",
      " cross_entropy: 0.0201087\n",
      "step 17300/50000 \n",
      " Train: accuracy: 1, loss: 0.00701346 \n",
      " Validation: accuracy: 0.984375 loss: 0.0354592\n",
      " cross_entropy: 0.0234724\n",
      "step 17400/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0177251 \n",
      " Validation: accuracy: 0.976562 loss: 0.0529179\n",
      " cross_entropy: 0.0163696\n",
      "step 17500/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0306437 \n",
      " Validation: accuracy: 1 loss: 0.00927839\n",
      " cross_entropy: 0.00997684\n",
      "step 17600/50000 \n",
      " Train: accuracy: 1, loss: 0.0037496 \n",
      " Validation: accuracy: 0.992188 loss: 0.0226135\n",
      " cross_entropy: 0.00639931\n",
      "step 17700/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.018981 \n",
      " Validation: accuracy: 0.984375 loss: 0.059995\n",
      " cross_entropy: 0.00887308\n",
      "step 17800/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0411083 \n",
      " Validation: accuracy: 0.976562 loss: 0.0428029\n",
      " cross_entropy: 0.0402946\n",
      "step 17900/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0184798 \n",
      " Validation: accuracy: 0.976562 loss: 0.0628484\n",
      " cross_entropy: 0.00753692\n",
      "step 18000/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0374128 \n",
      " Validation: accuracy: 1 loss: 0.015486\n",
      " cross_entropy: 0.00942137\n",
      "step 18100/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0349327 \n",
      " Validation: accuracy: 0.984375 loss: 0.0388406\n",
      " cross_entropy: 0.0218727\n",
      "step 18200/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0280987 \n",
      " Validation: accuracy: 1 loss: 0.00514298\n",
      " cross_entropy: 0.0174411\n",
      "step 18300/50000 \n",
      " Train: accuracy: 1, loss: 0.00971493 \n",
      " Validation: accuracy: 0.992188 loss: 0.0164533\n",
      " cross_entropy: 0.00909708\n",
      "step 18400/50000 \n",
      " Train: accuracy: 1, loss: 0.00859947 \n",
      " Validation: accuracy: 0.992188 loss: 0.0328401\n",
      " cross_entropy: 0.019629\n",
      "step 18500/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0207734 \n",
      " Validation: accuracy: 0.992188 loss: 0.0333656\n",
      " cross_entropy: 0.0301337\n",
      "step 18600/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0235199 \n",
      " Validation: accuracy: 0.992188 loss: 0.0134532\n",
      " cross_entropy: 0.0384065\n",
      "step 18700/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0250668 \n",
      " Validation: accuracy: 0.992188 loss: 0.0325503\n",
      " cross_entropy: 0.0595053\n",
      "step 18800/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0450886 \n",
      " Validation: accuracy: 1 loss: 0.00530157\n",
      " cross_entropy: 0.0273676\n",
      "step 18900/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0286175 \n",
      " Validation: accuracy: 1 loss: 0.00813626\n",
      " cross_entropy: 0.0203421\n",
      "step 19000/50000 \n",
      " Train: accuracy: 1, loss: 0.011934 \n",
      " Validation: accuracy: 1 loss: 0.0126737\n",
      " cross_entropy: 0.022027\n",
      "step 19100/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0257033 \n",
      " Validation: accuracy: 1 loss: 0.00406892\n",
      " cross_entropy: 0.0406903\n",
      "step 19200/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0660722 \n",
      " Validation: accuracy: 0.984375 loss: 0.0361063\n",
      " cross_entropy: 0.0310146\n",
      "step 19300/50000 \n",
      " Train: accuracy: 1, loss: 0.0128304 \n",
      " Validation: accuracy: 0.992188 loss: 0.0153645\n",
      " cross_entropy: 0.0103597\n",
      "step 19400/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0201957 \n",
      " Validation: accuracy: 0.984375 loss: 0.0286245\n",
      " cross_entropy: 0.0201148\n",
      "step 19500/50000 \n",
      " Train: accuracy: 1, loss: 0.0161376 \n",
      " Validation: accuracy: 0.992188 loss: 0.0281903\n",
      " cross_entropy: 0.00879591\n",
      "step 19600/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.012975 \n",
      " Validation: accuracy: 0.992188 loss: 0.0127029\n",
      " cross_entropy: 0.0219556\n",
      "step 19700/50000 \n",
      " Train: accuracy: 1, loss: 0.0023747 \n",
      " Validation: accuracy: 0.992188 loss: 0.0181252\n",
      " cross_entropy: 0.00592771\n",
      "step 19800/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.018495 \n",
      " Validation: accuracy: 1 loss: 0.00870798\n",
      " cross_entropy: 0.0253827\n",
      "step 19900/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0305111 \n",
      " Validation: accuracy: 1 loss: 0.0102414\n",
      " cross_entropy: 0.00732304\n",
      "step 20000/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0171643 \n",
      " Validation: accuracy: 0.96875 loss: 0.0527487\n",
      " cross_entropy: 0.00792002\n",
      "step 20100/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0243533 \n",
      " Validation: accuracy: 0.992188 loss: 0.0307396\n",
      " cross_entropy: 0.0371972\n",
      "step 20200/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0231274 \n",
      " Validation: accuracy: 0.984375 loss: 0.0267088\n",
      " cross_entropy: 0.042649\n",
      "step 20300/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0317532 \n",
      " Validation: accuracy: 0.992188 loss: 0.0467144\n",
      " cross_entropy: 0.0556531\n",
      "step 20400/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0190896 \n",
      " Validation: accuracy: 1 loss: 0.0117769\n",
      " cross_entropy: 0.0171789\n",
      "step 20500/50000 \n",
      " Train: accuracy: 1, loss: 0.0092029 \n",
      " Validation: accuracy: 1 loss: 0.00496479\n",
      " cross_entropy: 0.0376927\n",
      "step 20600/50000 \n",
      " Train: accuracy: 1, loss: 0.00311842 \n",
      " Validation: accuracy: 0.984375 loss: 0.0286244\n",
      " cross_entropy: 0.0196886\n",
      "step 20700/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0180966 \n",
      " Validation: accuracy: 0.992188 loss: 0.0180612\n",
      " cross_entropy: 0.0503091\n",
      "step 20800/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0225535 \n",
      " Validation: accuracy: 1 loss: 0.0124954\n",
      " cross_entropy: 0.00843747\n",
      "step 20900/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0424906 \n",
      " Validation: accuracy: 0.992188 loss: 0.0187711\n",
      " cross_entropy: 0.0443788\n",
      "step 21000/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0173364 \n",
      " Validation: accuracy: 1 loss: 0.00891626\n",
      " cross_entropy: 0.019314\n",
      "step 21100/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0136012 \n",
      " Validation: accuracy: 0.984375 loss: 0.0353052\n",
      " cross_entropy: 0.0215613\n",
      "step 21200/50000 \n",
      " Train: accuracy: 1, loss: 0.00843021 \n",
      " Validation: accuracy: 0.984375 loss: 0.0356902\n",
      " cross_entropy: 0.00914648\n",
      "step 21300/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0283302 \n",
      " Validation: accuracy: 1 loss: 0.0090876\n",
      " cross_entropy: 0.0379046\n",
      "step 21400/50000 \n",
      " Train: accuracy: 1, loss: 0.0116489 \n",
      " Validation: accuracy: 0.992188 loss: 0.0164184\n",
      " cross_entropy: 0.0208098\n",
      "step 21500/50000 \n",
      " Train: accuracy: 1, loss: 0.00655586 \n",
      " Validation: accuracy: 0.984375 loss: 0.0245901\n",
      " cross_entropy: 0.00875764\n",
      "step 21600/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0490748 \n",
      " Validation: accuracy: 0.992188 loss: 0.0158439\n",
      " cross_entropy: 0.0291995\n",
      "step 21700/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.022786 \n",
      " Validation: accuracy: 0.992188 loss: 0.0159854\n",
      " cross_entropy: 0.0117997\n",
      "step 21800/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0570966 \n",
      " Validation: accuracy: 0.992188 loss: 0.00930821\n",
      " cross_entropy: 0.050434\n",
      "step 21900/50000 \n",
      " Train: accuracy: 1, loss: 0.0157145 \n",
      " Validation: accuracy: 0.992188 loss: 0.0124281\n",
      " cross_entropy: 0.0194076\n",
      "step 22000/50000 \n",
      " Train: accuracy: 1, loss: 0.00209993 \n",
      " Validation: accuracy: 0.992188 loss: 0.0140784\n",
      " cross_entropy: 0.00996915\n",
      "step 22100/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0123893 \n",
      " Validation: accuracy: 0.984375 loss: 0.0358434\n",
      " cross_entropy: 0.0414089\n",
      "step 22200/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0389578 \n",
      " Validation: accuracy: 1 loss: 0.0217853\n",
      " cross_entropy: 0.0529111\n",
      "step 22300/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0551958 \n",
      " Validation: accuracy: 0.984375 loss: 0.0213747\n",
      " cross_entropy: 0.0113323\n",
      "step 22400/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.021687 \n",
      " Validation: accuracy: 0.984375 loss: 0.0485708\n",
      " cross_entropy: 0.0197716\n",
      "step 22500/50000 \n",
      " Train: accuracy: 0.953125, loss: 0.0799203 \n",
      " Validation: accuracy: 0.992188 loss: 0.0234713\n",
      " cross_entropy: 0.0197539\n",
      "step 22600/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0467069 \n",
      " Validation: accuracy: 0.976562 loss: 0.0514785\n",
      " cross_entropy: 0.0721105\n",
      "step 22700/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0256271 \n",
      " Validation: accuracy: 1 loss: 0.00632219\n",
      " cross_entropy: 0.0356284\n",
      "step 22800/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0336152 \n",
      " Validation: accuracy: 1 loss: 0.0184321\n",
      " cross_entropy: 0.00520284\n",
      "step 22900/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0171222 \n",
      " Validation: accuracy: 1 loss: 0.0129956\n",
      " cross_entropy: 0.00713449\n",
      "step 23000/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0273926 \n",
      " Validation: accuracy: 1 loss: 0.00773347\n",
      " cross_entropy: 0.0328796\n",
      "step 23100/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0270172 \n",
      " Validation: accuracy: 0.992188 loss: 0.0140875\n",
      " cross_entropy: 0.0104926\n",
      "step 23200/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0229391 \n",
      " Validation: accuracy: 0.984375 loss: 0.0486558\n",
      " cross_entropy: 0.042969\n",
      "step 23300/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0194654 \n",
      " Validation: accuracy: 0.976562 loss: 0.046686\n",
      " cross_entropy: 0.00966243\n",
      "step 23400/50000 \n",
      " Train: accuracy: 1, loss: 0.00849934 \n",
      " Validation: accuracy: 1 loss: 0.00708167\n",
      " cross_entropy: 0.0314082\n",
      "step 23500/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0432623 \n",
      " Validation: accuracy: 0.992188 loss: 0.0110482\n",
      " cross_entropy: 0.068162\n",
      "step 23600/50000 \n",
      " Train: accuracy: 1, loss: 0.0166844 \n",
      " Validation: accuracy: 0.992188 loss: 0.0307696\n",
      " cross_entropy: 0.016543\n",
      "step 23700/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0165456 \n",
      " Validation: accuracy: 0.984375 loss: 0.0358354\n",
      " cross_entropy: 0.0213905\n",
      "step 23800/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0253929 \n",
      " Validation: accuracy: 0.992188 loss: 0.0278432\n",
      " cross_entropy: 0.0266688\n",
      "step 23900/50000 \n",
      " Train: accuracy: 1, loss: 0.00744449 \n",
      " Validation: accuracy: 0.992188 loss: 0.0522488\n",
      " cross_entropy: 0.00605101\n",
      "step 24000/50000 \n",
      " Train: accuracy: 1, loss: 0.00491428 \n",
      " Validation: accuracy: 1 loss: 0.0117031\n",
      " cross_entropy: 0.00948086\n",
      "step 24100/50000 \n",
      " Train: accuracy: 1, loss: 0.00279583 \n",
      " Validation: accuracy: 0.992188 loss: 0.0119102\n",
      " cross_entropy: 0.0154799\n",
      "step 24200/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0288299 \n",
      " Validation: accuracy: 1 loss: 0.00240785\n",
      " cross_entropy: 0.00647436\n",
      "step 24300/50000 \n",
      " Train: accuracy: 1, loss: 0.00710127 \n",
      " Validation: accuracy: 0.984375 loss: 0.0190247\n",
      " cross_entropy: 0.00512395\n",
      "step 24400/50000 \n",
      " Train: accuracy: 1, loss: 0.0168099 \n",
      " Validation: accuracy: 0.984375 loss: 0.0676747\n",
      " cross_entropy: 0.019583\n",
      "step 24500/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0330046 \n",
      " Validation: accuracy: 0.992188 loss: 0.0125202\n",
      " cross_entropy: 0.0428174\n",
      "step 24600/50000 \n",
      " Train: accuracy: 1, loss: 0.0129034 \n",
      " Validation: accuracy: 1 loss: 0.00639156\n",
      " cross_entropy: 0.0237887\n",
      "step 24700/50000 \n",
      " Train: accuracy: 1, loss: 0.00577347 \n",
      " Validation: accuracy: 0.992188 loss: 0.0114166\n",
      " cross_entropy: 0.00997663\n",
      "step 24800/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0108271 \n",
      " Validation: accuracy: 0.984375 loss: 0.0575277\n",
      " cross_entropy: 0.0118066\n",
      "step 24900/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0203909 \n",
      " Validation: accuracy: 1 loss: 0.00582181\n",
      " cross_entropy: 0.0192311\n",
      "step 25000/50000 \n",
      " Train: accuracy: 1, loss: 0.00938942 \n",
      " Validation: accuracy: 0.992188 loss: 0.0512303\n",
      " cross_entropy: 0.0194807\n",
      "step 25100/50000 \n",
      " Train: accuracy: 1, loss: 0.0101212 \n",
      " Validation: accuracy: 0.992188 loss: 0.0225697\n",
      " cross_entropy: 0.026143\n",
      "step 25200/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0115502 \n",
      " Validation: accuracy: 1 loss: 0.00509298\n",
      " cross_entropy: 0.00736799\n",
      "step 25300/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0328319 \n",
      " Validation: accuracy: 0.960938 loss: 0.101772\n",
      " cross_entropy: 0.0586335\n",
      "step 25400/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0532474 \n",
      " Validation: accuracy: 0.992188 loss: 0.00806834\n",
      " cross_entropy: 0.0649959\n",
      "step 25500/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0328825 \n",
      " Validation: accuracy: 1 loss: 0.00854484\n",
      " cross_entropy: 0.00741963\n",
      "step 25600/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0337739 \n",
      " Validation: accuracy: 1 loss: 0.010142\n",
      " cross_entropy: 0.0388869\n",
      "step 25700/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0133444 \n",
      " Validation: accuracy: 0.992188 loss: 0.019701\n",
      " cross_entropy: 0.0289357\n",
      "step 25800/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0439403 \n",
      " Validation: accuracy: 1 loss: 0.00872299\n",
      " cross_entropy: 0.00618949\n",
      "step 25900/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0179332 \n",
      " Validation: accuracy: 1 loss: 0.0133034\n",
      " cross_entropy: 0.00729251\n",
      "step 26000/50000 \n",
      " Train: accuracy: 1, loss: 0.00878524 \n",
      " Validation: accuracy: 0.992188 loss: 0.0491074\n",
      " cross_entropy: 0.0433737\n",
      "step 26100/50000 \n",
      " Train: accuracy: 1, loss: 0.0119703 \n",
      " Validation: accuracy: 0.992188 loss: 0.0266898\n",
      " cross_entropy: 0.0251654\n",
      "step 26200/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.013865 \n",
      " Validation: accuracy: 0.992188 loss: 0.0197753\n",
      " cross_entropy: 0.00758751\n",
      "step 26300/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0345365 \n",
      " Validation: accuracy: 1 loss: 0.0131172\n",
      " cross_entropy: 0.022116\n",
      "step 26400/50000 \n",
      " Train: accuracy: 1, loss: 0.00548973 \n",
      " Validation: accuracy: 1 loss: 0.00877885\n",
      " cross_entropy: 0.0177126\n",
      "step 26500/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0255196 \n",
      " Validation: accuracy: 1 loss: 0.0104201\n",
      " cross_entropy: 0.0267005\n",
      "step 26600/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0369612 \n",
      " Validation: accuracy: 0.984375 loss: 0.0441787\n",
      " cross_entropy: 0.0311456\n",
      "step 26700/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0318234 \n",
      " Validation: accuracy: 0.992188 loss: 0.0376713\n",
      " cross_entropy: 0.052505\n",
      "step 26800/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0327996 \n",
      " Validation: accuracy: 0.984375 loss: 0.0200815\n",
      " cross_entropy: 0.04873\n",
      "step 26900/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.025318 \n",
      " Validation: accuracy: 0.992188 loss: 0.0276966\n",
      " cross_entropy: 0.0163396\n",
      "step 27000/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0135372 \n",
      " Validation: accuracy: 1 loss: 0.00641285\n",
      " cross_entropy: 0.00602895\n",
      "step 27100/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0312409 \n",
      " Validation: accuracy: 0.992188 loss: 0.0174163\n",
      " cross_entropy: 0.0083613\n",
      "step 27200/50000 \n",
      " Train: accuracy: 1, loss: 0.00605109 \n",
      " Validation: accuracy: 0.984375 loss: 0.0408909\n",
      " cross_entropy: 0.00452791\n",
      "step 27300/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0277073 \n",
      " Validation: accuracy: 0.992188 loss: 0.0271104\n",
      " cross_entropy: 0.0197058\n",
      "step 27400/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0139902 \n",
      " Validation: accuracy: 0.984375 loss: 0.0252807\n",
      " cross_entropy: 0.00302419\n",
      "step 27500/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0180181 \n",
      " Validation: accuracy: 1 loss: 0.00309149\n",
      " cross_entropy: 0.0202623\n",
      "step 27600/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0117276 \n",
      " Validation: accuracy: 1 loss: 0.0166582\n",
      " cross_entropy: 0.0120122\n",
      "step 27700/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0243612 \n",
      " Validation: accuracy: 0.992188 loss: 0.0355124\n",
      " cross_entropy: 0.0301065\n",
      "step 27800/50000 \n",
      " Train: accuracy: 1, loss: 0.00963449 \n",
      " Validation: accuracy: 0.984375 loss: 0.0281883\n",
      " cross_entropy: 0.004362\n",
      "step 27900/50000 \n",
      " Train: accuracy: 1, loss: 0.00701424 \n",
      " Validation: accuracy: 0.992188 loss: 0.0307675\n",
      " cross_entropy: 0.0137118\n",
      "step 28000/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0189229 \n",
      " Validation: accuracy: 1 loss: 0.00462008\n",
      " cross_entropy: 0.0210155\n",
      "step 28100/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0172565 \n",
      " Validation: accuracy: 1 loss: 0.0148825\n",
      " cross_entropy: 0.0160937\n",
      "step 28200/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.00961145 \n",
      " Validation: accuracy: 1 loss: 0.0125946\n",
      " cross_entropy: 0.0209911\n",
      "step 28300/50000 \n",
      " Train: accuracy: 1, loss: 0.00604745 \n",
      " Validation: accuracy: 1 loss: 0.00769679\n",
      " cross_entropy: 0.00702239\n",
      "step 28400/50000 \n",
      " Train: accuracy: 1, loss: 0.00893305 \n",
      " Validation: accuracy: 1 loss: 0.00430045\n",
      " cross_entropy: 0.0149358\n",
      "step 28500/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0268388 \n",
      " Validation: accuracy: 1 loss: 0.00983581\n",
      " cross_entropy: 0.0119152\n",
      "step 28600/50000 \n",
      " Train: accuracy: 1, loss: 0.00595123 \n",
      " Validation: accuracy: 1 loss: 0.00435065\n",
      " cross_entropy: 0.00536533\n",
      "step 28700/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0271087 \n",
      " Validation: accuracy: 0.96875 loss: 0.0758687\n",
      " cross_entropy: 0.00677536\n",
      "step 28800/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.031055 \n",
      " Validation: accuracy: 1 loss: 0.00926604\n",
      " cross_entropy: 0.0236476\n",
      "step 28900/50000 \n",
      " Train: accuracy: 1, loss: 0.00369168 \n",
      " Validation: accuracy: 1 loss: 0.0031922\n",
      " cross_entropy: 0.00237799\n",
      "step 29000/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0231643 \n",
      " Validation: accuracy: 0.992188 loss: 0.0192334\n",
      " cross_entropy: 0.0158683\n",
      "step 29100/50000 \n",
      " Train: accuracy: 1, loss: 0.00378989 \n",
      " Validation: accuracy: 0.976562 loss: 0.0364764\n",
      " cross_entropy: 0.0035741\n",
      "step 29200/50000 \n",
      " Train: accuracy: 1, loss: 0.0112471 \n",
      " Validation: accuracy: 1 loss: 0.00337988\n",
      " cross_entropy: 0.0142889\n",
      "step 29300/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0340232 \n",
      " Validation: accuracy: 0.992188 loss: 0.00887945\n",
      " cross_entropy: 0.0348539\n",
      "step 29400/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0195542 \n",
      " Validation: accuracy: 1 loss: 0.0130595\n",
      " cross_entropy: 0.0114513\n",
      "step 29500/50000 \n",
      " Train: accuracy: 1, loss: 0.00307586 \n",
      " Validation: accuracy: 0.984375 loss: 0.0386417\n",
      " cross_entropy: 0.0267208\n",
      "step 29600/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0116055 \n",
      " Validation: accuracy: 0.992188 loss: 0.0275924\n",
      " cross_entropy: 0.0110203\n",
      "step 29700/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0151803 \n",
      " Validation: accuracy: 1 loss: 0.00353511\n",
      " cross_entropy: 0.0529164\n",
      "step 29800/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0322178 \n",
      " Validation: accuracy: 0.984375 loss: 0.0244405\n",
      " cross_entropy: 0.00564677\n",
      "step 29900/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.035398 \n",
      " Validation: accuracy: 1 loss: 0.0131657\n",
      " cross_entropy: 0.0263508\n",
      "step 30000/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0201767 \n",
      " Validation: accuracy: 0.992188 loss: 0.0231883\n",
      " cross_entropy: 0.0191305\n",
      "step 30100/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.013269 \n",
      " Validation: accuracy: 1 loss: 0.00662817\n",
      " cross_entropy: 0.0178244\n",
      "step 30200/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0243579 \n",
      " Validation: accuracy: 0.992188 loss: 0.0128806\n",
      " cross_entropy: 0.0352423\n",
      "step 30300/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0193873 \n",
      " Validation: accuracy: 1 loss: 0.00673844\n",
      " cross_entropy: 0.00458152\n",
      "step 30400/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0406614 \n",
      " Validation: accuracy: 1 loss: 0.00570125\n",
      " cross_entropy: 0.0207639\n",
      "step 30500/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0130859 \n",
      " Validation: accuracy: 1 loss: 0.0049949\n",
      " cross_entropy: 0.0305232\n",
      "step 30600/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.029468 \n",
      " Validation: accuracy: 1 loss: 0.00766073\n",
      " cross_entropy: 0.0238375\n",
      "step 30700/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.00998223 \n",
      " Validation: accuracy: 1 loss: 0.009177\n",
      " cross_entropy: 0.0263652\n",
      "step 30800/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.037613 \n",
      " Validation: accuracy: 0.992188 loss: 0.0117451\n",
      " cross_entropy: 0.0101892\n",
      "step 30900/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0138771 \n",
      " Validation: accuracy: 1 loss: 0.00736558\n",
      " cross_entropy: 0.0277526\n",
      "step 31000/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0382364 \n",
      " Validation: accuracy: 1 loss: 0.00653155\n",
      " cross_entropy: 0.0166747\n",
      "step 31100/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0479358 \n",
      " Validation: accuracy: 1 loss: 0.00354797\n",
      " cross_entropy: 0.00722449\n",
      "step 31200/50000 \n",
      " Train: accuracy: 1, loss: 0.00808067 \n",
      " Validation: accuracy: 0.976562 loss: 0.0677852\n",
      " cross_entropy: 0.0139283\n",
      "step 31300/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0333383 \n",
      " Validation: accuracy: 1 loss: 0.00399222\n",
      " cross_entropy: 0.00998403\n",
      "step 31400/50000 \n",
      " Train: accuracy: 1, loss: 0.00997125 \n",
      " Validation: accuracy: 0.992188 loss: 0.0254362\n",
      " cross_entropy: 0.00634646\n",
      "step 31500/50000 \n",
      " Train: accuracy: 1, loss: 0.00819102 \n",
      " Validation: accuracy: 1 loss: 0.00296891\n",
      " cross_entropy: 0.00968782\n",
      "step 31600/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0173931 \n",
      " Validation: accuracy: 0.992188 loss: 0.0530146\n",
      " cross_entropy: 0.0245209\n",
      "step 31700/50000 \n",
      " Train: accuracy: 1, loss: 0.0119137 \n",
      " Validation: accuracy: 0.992188 loss: 0.015227\n",
      " cross_entropy: 0.0234971\n",
      "step 31800/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0395498 \n",
      " Validation: accuracy: 0.984375 loss: 0.0243765\n",
      " cross_entropy: 0.0163397\n",
      "step 31900/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0127362 \n",
      " Validation: accuracy: 1 loss: 0.00687844\n",
      " cross_entropy: 0.00896641\n",
      "step 32000/50000 \n",
      " Train: accuracy: 1, loss: 0.00408386 \n",
      " Validation: accuracy: 1 loss: 0.00443472\n",
      " cross_entropy: 0.00367666\n",
      "step 32100/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0149311 \n",
      " Validation: accuracy: 0.992188 loss: 0.0209864\n",
      " cross_entropy: 0.0017842\n",
      "step 32200/50000 \n",
      " Train: accuracy: 1, loss: 0.00235376 \n",
      " Validation: accuracy: 0.992188 loss: 0.00772999\n",
      " cross_entropy: 0.0010567\n",
      "step 32300/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0406498 \n",
      " Validation: accuracy: 1 loss: 0.00250521\n",
      " cross_entropy: 0.0330683\n",
      "step 32400/50000 \n",
      " Train: accuracy: 1, loss: 0.0080247 \n",
      " Validation: accuracy: 1 loss: 0.00661393\n",
      " cross_entropy: 0.00821689\n",
      "step 32500/50000 \n",
      " Train: accuracy: 1, loss: 0.00296917 \n",
      " Validation: accuracy: 0.992188 loss: 0.0212135\n",
      " cross_entropy: 0.00373127\n",
      "step 32600/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0247497 \n",
      " Validation: accuracy: 0.992188 loss: 0.00985519\n",
      " cross_entropy: 0.0107688\n",
      "step 32700/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0172407 \n",
      " Validation: accuracy: 0.992188 loss: 0.0393772\n",
      " cross_entropy: 0.0402161\n",
      "step 32800/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0464292 \n",
      " Validation: accuracy: 0.976562 loss: 0.0944243\n",
      " cross_entropy: 0.0584268\n",
      "step 32900/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0161303 \n",
      " Validation: accuracy: 0.984375 loss: 0.0246284\n",
      " cross_entropy: 0.0255425\n",
      "step 33000/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0153097 \n",
      " Validation: accuracy: 0.976562 loss: 0.0400185\n",
      " cross_entropy: 0.00312905\n",
      "step 33100/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0185532 \n",
      " Validation: accuracy: 0.984375 loss: 0.0323919\n",
      " cross_entropy: 0.0115524\n",
      "step 33200/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0200064 \n",
      " Validation: accuracy: 1 loss: 0.00265242\n",
      " cross_entropy: 0.0391243\n",
      "step 33300/50000 \n",
      " Train: accuracy: 1, loss: 0.0110888 \n",
      " Validation: accuracy: 1 loss: 0.00146585\n",
      " cross_entropy: 0.015765\n",
      "step 33400/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0211297 \n",
      " Validation: accuracy: 1 loss: 0.000749888\n",
      " cross_entropy: 0.0247623\n",
      "step 33500/50000 \n",
      " Train: accuracy: 1, loss: 0.0018466 \n",
      " Validation: accuracy: 1 loss: 0.0167699\n",
      " cross_entropy: 0.00166704\n",
      "step 33600/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.034573 \n",
      " Validation: accuracy: 1 loss: 0.00299805\n",
      " cross_entropy: 0.0323415\n",
      "step 33700/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0320262 \n",
      " Validation: accuracy: 1 loss: 0.00461198\n",
      " cross_entropy: 0.027798\n",
      "step 33800/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0402478 \n",
      " Validation: accuracy: 0.984375 loss: 0.0257174\n",
      " cross_entropy: 0.0345035\n",
      "step 33900/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0188827 \n",
      " Validation: accuracy: 1 loss: 0.011065\n",
      " cross_entropy: 0.00876195\n",
      "step 34000/50000 \n",
      " Train: accuracy: 1, loss: 0.00752034 \n",
      " Validation: accuracy: 0.992188 loss: 0.0128536\n",
      " cross_entropy: 0.00368458\n",
      "step 34100/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0189117 \n",
      " Validation: accuracy: 1 loss: 0.003948\n",
      " cross_entropy: 0.0270666\n",
      "step 34200/50000 \n",
      " Train: accuracy: 1, loss: 0.00239953 \n",
      " Validation: accuracy: 0.992188 loss: 0.0181429\n",
      " cross_entropy: 0.00901731\n",
      "step 34300/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0434676 \n",
      " Validation: accuracy: 0.992188 loss: 0.0162449\n",
      " cross_entropy: 0.0612132\n",
      "step 34400/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0424227 \n",
      " Validation: accuracy: 1 loss: 0.00774418\n",
      " cross_entropy: 0.0660247\n",
      "step 34500/50000 \n",
      " Train: accuracy: 1, loss: 0.00374078 \n",
      " Validation: accuracy: 0.984375 loss: 0.0535795\n",
      " cross_entropy: 0.0245676\n",
      "step 34600/50000 \n",
      " Train: accuracy: 1, loss: 0.0113546 \n",
      " Validation: accuracy: 1 loss: 0.00444038\n",
      " cross_entropy: 0.0072614\n",
      "step 34700/50000 \n",
      " Train: accuracy: 1, loss: 0.0129028 \n",
      " Validation: accuracy: 0.992188 loss: 0.0108937\n",
      " cross_entropy: 0.0204587\n",
      "step 34800/50000 \n",
      " Train: accuracy: 1, loss: 0.0115473 \n",
      " Validation: accuracy: 1 loss: 0.0029286\n",
      " cross_entropy: 0.0231223\n",
      "step 34900/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.00972911 \n",
      " Validation: accuracy: 0.992188 loss: 0.0239816\n",
      " cross_entropy: 0.00395333\n",
      "step 35000/50000 \n",
      " Train: accuracy: 1, loss: 0.00492189 \n",
      " Validation: accuracy: 0.992188 loss: 0.0299069\n",
      " cross_entropy: 0.00530104\n",
      "step 35100/50000 \n",
      " Train: accuracy: 1, loss: 0.00358645 \n",
      " Validation: accuracy: 0.992188 loss: 0.0261838\n",
      " cross_entropy: 0.0216111\n",
      "step 35200/50000 \n",
      " Train: accuracy: 1, loss: 0.00992104 \n",
      " Validation: accuracy: 0.992188 loss: 0.0158851\n",
      " cross_entropy: 0.00606128\n",
      "step 35300/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0204741 \n",
      " Validation: accuracy: 0.992188 loss: 0.0276096\n",
      " cross_entropy: 0.0223254\n",
      "step 35400/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.00935266 \n",
      " Validation: accuracy: 0.984375 loss: 0.0206913\n",
      " cross_entropy: 0.00942268\n",
      "step 35500/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0357625 \n",
      " Validation: accuracy: 0.992188 loss: 0.0565923\n",
      " cross_entropy: 0.00401126\n",
      "step 35600/50000 \n",
      " Train: accuracy: 1, loss: 0.00112547 \n",
      " Validation: accuracy: 1 loss: 0.00970386\n",
      " cross_entropy: 0.00904629\n",
      "step 35700/50000 \n",
      " Train: accuracy: 1, loss: 0.00673935 \n",
      " Validation: accuracy: 0.984375 loss: 0.0715422\n",
      " cross_entropy: 0.00786373\n",
      "step 35800/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0324161 \n",
      " Validation: accuracy: 0.992188 loss: 0.0658651\n",
      " cross_entropy: 0.0188642\n",
      "step 35900/50000 \n",
      " Train: accuracy: 1, loss: 0.0100476 \n",
      " Validation: accuracy: 1 loss: 0.0164516\n",
      " cross_entropy: 0.0048891\n",
      "step 36000/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0132981 \n",
      " Validation: accuracy: 0.984375 loss: 0.0336499\n",
      " cross_entropy: 0.0119137\n",
      "step 36100/50000 \n",
      " Train: accuracy: 1, loss: 0.00622478 \n",
      " Validation: accuracy: 0.992188 loss: 0.0116704\n",
      " cross_entropy: 0.043097\n",
      "step 36200/50000 \n",
      " Train: accuracy: 1, loss: 0.00350078 \n",
      " Validation: accuracy: 0.984375 loss: 0.0206195\n",
      " cross_entropy: 0.00113404\n",
      "step 36300/50000 \n",
      " Train: accuracy: 1, loss: 0.00621075 \n",
      " Validation: accuracy: 0.976562 loss: 0.062406\n",
      " cross_entropy: 0.0142224\n",
      "step 36400/50000 \n",
      " Train: accuracy: 1, loss: 0.00272055 \n",
      " Validation: accuracy: 0.976562 loss: 0.0746416\n",
      " cross_entropy: 0.00182799\n",
      "step 36500/50000 \n",
      " Train: accuracy: 1, loss: 0.00669908 \n",
      " Validation: accuracy: 1 loss: 0.00219602\n",
      " cross_entropy: 0.00584946\n",
      "step 36600/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0631853 \n",
      " Validation: accuracy: 0.992188 loss: 0.0146358\n",
      " cross_entropy: 0.0155872\n",
      "step 36700/50000 \n",
      " Train: accuracy: 1, loss: 0.0135998 \n",
      " Validation: accuracy: 0.984375 loss: 0.0235913\n",
      " cross_entropy: 0.0158202\n",
      "step 36800/50000 \n",
      " Train: accuracy: 1, loss: 0.00855933 \n",
      " Validation: accuracy: 1 loss: 0.00591261\n",
      " cross_entropy: 0.0102252\n",
      "step 36900/50000 \n",
      " Train: accuracy: 1, loss: 0.00928147 \n",
      " Validation: accuracy: 0.992188 loss: 0.0288803\n",
      " cross_entropy: 0.0228623\n",
      "step 37000/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0217057 \n",
      " Validation: accuracy: 1 loss: 0.00311262\n",
      " cross_entropy: 0.00538886\n",
      "step 37100/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.00773141 \n",
      " Validation: accuracy: 1 loss: 0.0020823\n",
      " cross_entropy: 0.0222444\n",
      "step 37200/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0208815 \n",
      " Validation: accuracy: 1 loss: 0.00215406\n",
      " cross_entropy: 0.00338126\n",
      "step 37300/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0114961 \n",
      " Validation: accuracy: 1 loss: 0.0107347\n",
      " cross_entropy: 0.0271122\n",
      "step 37400/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0132583 \n",
      " Validation: accuracy: 1 loss: 0.00894545\n",
      " cross_entropy: 0.0151404\n",
      "step 37500/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0149454 \n",
      " Validation: accuracy: 1 loss: 0.00576513\n",
      " cross_entropy: 0.0261102\n",
      "step 37600/50000 \n",
      " Train: accuracy: 1, loss: 0.00842826 \n",
      " Validation: accuracy: 1 loss: 0.00535687\n",
      " cross_entropy: 0.00273354\n",
      "step 37700/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0223098 \n",
      " Validation: accuracy: 1 loss: 0.00281443\n",
      " cross_entropy: 0.0236805\n",
      "step 37800/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0202601 \n",
      " Validation: accuracy: 0.992188 loss: 0.0199174\n",
      " cross_entropy: 0.00850771\n",
      "step 37900/50000 \n",
      " Train: accuracy: 1, loss: 0.00814128 \n",
      " Validation: accuracy: 1 loss: 0.00182361\n",
      " cross_entropy: 0.043733\n",
      "step 38000/50000 \n",
      " Train: accuracy: 1, loss: 0.00169646 \n",
      " Validation: accuracy: 1 loss: 0.00290624\n",
      " cross_entropy: 0.000739079\n",
      "step 38100/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0125247 \n",
      " Validation: accuracy: 1 loss: 0.00419874\n",
      " cross_entropy: 0.0119308\n",
      "step 38200/50000 \n",
      " Train: accuracy: 1, loss: 0.0092386 \n",
      " Validation: accuracy: 1 loss: 0.00337959\n",
      " cross_entropy: 0.0053414\n",
      "step 38300/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0320007 \n",
      " Validation: accuracy: 0.992188 loss: 0.0358812\n",
      " cross_entropy: 0.00494572\n",
      "step 38400/50000 \n",
      " Train: accuracy: 1, loss: 0.00885651 \n",
      " Validation: accuracy: 1 loss: 0.00925414\n",
      " cross_entropy: 0.0089292\n",
      "step 38500/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0193404 \n",
      " Validation: accuracy: 1 loss: 0.00306291\n",
      " cross_entropy: 0.0112869\n",
      "step 38600/50000 \n",
      " Train: accuracy: 1, loss: 0.000720592 \n",
      " Validation: accuracy: 0.992188 loss: 0.039074\n",
      " cross_entropy: 0.0026279\n",
      "step 38700/50000 \n",
      " Train: accuracy: 1, loss: 0.00273494 \n",
      " Validation: accuracy: 0.992188 loss: 0.02306\n",
      " cross_entropy: 0.00269392\n",
      "step 38800/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0153112 \n",
      " Validation: accuracy: 0.992188 loss: 0.0138668\n",
      " cross_entropy: 0.015089\n",
      "step 38900/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0211704 \n",
      " Validation: accuracy: 0.992188 loss: 0.0155216\n",
      " cross_entropy: 0.0345681\n",
      "step 39000/50000 \n",
      " Train: accuracy: 1, loss: 0.00482519 \n",
      " Validation: accuracy: 1 loss: 0.00339889\n",
      " cross_entropy: 0.00812378\n",
      "step 39100/50000 \n",
      " Train: accuracy: 1, loss: 0.00910047 \n",
      " Validation: accuracy: 0.976562 loss: 0.0650811\n",
      " cross_entropy: 0.0209031\n",
      "step 39200/50000 \n",
      " Train: accuracy: 1, loss: 0.0063963 \n",
      " Validation: accuracy: 1 loss: 0.00979703\n",
      " cross_entropy: 0.0151298\n",
      "step 39300/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0309708 \n",
      " Validation: accuracy: 1 loss: 0.0100078\n",
      " cross_entropy: 0.0108331\n",
      "step 39400/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0261092 \n",
      " Validation: accuracy: 0.984375 loss: 0.0279937\n",
      " cross_entropy: 0.00426522\n",
      "step 39500/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0157732 \n",
      " Validation: accuracy: 0.984375 loss: 0.0158724\n",
      " cross_entropy: 0.00916645\n",
      "step 39600/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0390057 \n",
      " Validation: accuracy: 0.992188 loss: 0.0188113\n",
      " cross_entropy: 0.039099\n",
      "step 39700/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0109837 \n",
      " Validation: accuracy: 1 loss: 0.00365094\n",
      " cross_entropy: 0.00516002\n",
      "step 39800/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0205105 \n",
      " Validation: accuracy: 1 loss: 0.00224973\n",
      " cross_entropy: 0.0314746\n",
      "step 39900/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0306946 \n",
      " Validation: accuracy: 1 loss: 0.0021682\n",
      " cross_entropy: 0.0129328\n",
      "step 40000/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0142963 \n",
      " Validation: accuracy: 1 loss: 0.00167393\n",
      " cross_entropy: 0.00923693\n",
      "step 40100/50000 \n",
      " Train: accuracy: 1, loss: 0.00388673 \n",
      " Validation: accuracy: 0.992188 loss: 0.0145933\n",
      " cross_entropy: 0.00646082\n",
      "step 40200/50000 \n",
      " Train: accuracy: 1, loss: 0.00492396 \n",
      " Validation: accuracy: 0.992188 loss: 0.0123092\n",
      " cross_entropy: 0.00225599\n",
      "step 40300/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0389911 \n",
      " Validation: accuracy: 1 loss: 0.00860029\n",
      " cross_entropy: 0.0624232\n",
      "step 40400/50000 \n",
      " Train: accuracy: 1, loss: 0.00951506 \n",
      " Validation: accuracy: 0.992188 loss: 0.00782438\n",
      " cross_entropy: 0.0391944\n",
      "step 40500/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0351193 \n",
      " Validation: accuracy: 1 loss: 0.0069218\n",
      " cross_entropy: 0.0136276\n",
      "step 40600/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0207441 \n",
      " Validation: accuracy: 1 loss: 0.00163226\n",
      " cross_entropy: 0.00527255\n",
      "step 40700/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0220282 \n",
      " Validation: accuracy: 1 loss: 0.00547905\n",
      " cross_entropy: 0.00823631\n",
      "step 40800/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0133847 \n",
      " Validation: accuracy: 1 loss: 0.00863066\n",
      " cross_entropy: 0.00442318\n",
      "step 40900/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0136477 \n",
      " Validation: accuracy: 0.976562 loss: 0.0310365\n",
      " cross_entropy: 0.0132604\n",
      "step 41000/50000 \n",
      " Train: accuracy: 1, loss: 0.00378048 \n",
      " Validation: accuracy: 0.992188 loss: 0.0160915\n",
      " cross_entropy: 0.014325\n",
      "step 41100/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.00896915 \n",
      " Validation: accuracy: 0.992188 loss: 0.00950985\n",
      " cross_entropy: 0.00334084\n",
      "step 41200/50000 \n",
      " Train: accuracy: 1, loss: 0.0011819 \n",
      " Validation: accuracy: 0.992188 loss: 0.0350036\n",
      " cross_entropy: 0.00185613\n",
      "step 41300/50000 \n",
      " Train: accuracy: 1, loss: 0.0128561 \n",
      " Validation: accuracy: 1 loss: 0.00418812\n",
      " cross_entropy: 0.00160389\n",
      "step 41400/50000 \n",
      " Train: accuracy: 1, loss: 0.00189369 \n",
      " Validation: accuracy: 1 loss: 0.00364511\n",
      " cross_entropy: 0.00604112\n",
      "step 41500/50000 \n",
      " Train: accuracy: 1, loss: 0.0077309 \n",
      " Validation: accuracy: 1 loss: 0.00457701\n",
      " cross_entropy: 0.0354951\n",
      "step 41600/50000 \n",
      " Train: accuracy: 1, loss: 0.00440561 \n",
      " Validation: accuracy: 1 loss: 0.0034583\n",
      " cross_entropy: 0.00225573\n",
      "step 41700/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0233227 \n",
      " Validation: accuracy: 1 loss: 0.00612046\n",
      " cross_entropy: 0.0135218\n",
      "step 41800/50000 \n",
      " Train: accuracy: 1, loss: 0.00715654 \n",
      " Validation: accuracy: 1 loss: 0.00247274\n",
      " cross_entropy: 0.00196022\n",
      "step 41900/50000 \n",
      " Train: accuracy: 1, loss: 0.00877368 \n",
      " Validation: accuracy: 1 loss: 0.00223385\n",
      " cross_entropy: 0.0392324\n",
      "step 42000/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0210182 \n",
      " Validation: accuracy: 0.992188 loss: 0.0302349\n",
      " cross_entropy: 0.00954196\n",
      "step 42100/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0412805 \n",
      " Validation: accuracy: 1 loss: 0.00325301\n",
      " cross_entropy: 0.0235332\n",
      "step 42200/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.00772957 \n",
      " Validation: accuracy: 0.976562 loss: 0.0458875\n",
      " cross_entropy: 0.00337259\n",
      "step 42300/50000 \n",
      " Train: accuracy: 1, loss: 0.00469144 \n",
      " Validation: accuracy: 0.992188 loss: 0.0238554\n",
      " cross_entropy: 0.00975116\n",
      "step 42400/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0164849 \n",
      " Validation: accuracy: 0.992188 loss: 0.0172835\n",
      " cross_entropy: 0.0155428\n",
      "step 42500/50000 \n",
      " Train: accuracy: 0.976562, loss: 0.0517826 \n",
      " Validation: accuracy: 0.984375 loss: 0.0297715\n",
      " cross_entropy: 0.0576415\n",
      "step 42600/50000 \n",
      " Train: accuracy: 1, loss: 0.00169097 \n",
      " Validation: accuracy: 0.976562 loss: 0.0590561\n",
      " cross_entropy: 0.0132936\n",
      "step 42700/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0205639 \n",
      " Validation: accuracy: 0.984375 loss: 0.0396677\n",
      " cross_entropy: 0.00692528\n",
      "step 42800/50000 \n",
      " Train: accuracy: 1, loss: 0.00420271 \n",
      " Validation: accuracy: 0.992188 loss: 0.0151979\n",
      " cross_entropy: 0.0231146\n",
      "step 42900/50000 \n",
      " Train: accuracy: 1, loss: 0.00122826 \n",
      " Validation: accuracy: 1 loss: 0.00738126\n",
      " cross_entropy: 0.0100335\n",
      "step 43000/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0107736 \n",
      " Validation: accuracy: 1 loss: 0.00442889\n",
      " cross_entropy: 0.0168751\n",
      "step 43100/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0202094 \n",
      " Validation: accuracy: 1 loss: 0.000718581\n",
      " cross_entropy: 0.010087\n",
      "step 43200/50000 \n",
      " Train: accuracy: 1, loss: 0.0170937 \n",
      " Validation: accuracy: 0.976562 loss: 0.0793911\n",
      " cross_entropy: 0.0103584\n",
      "step 43300/50000 \n",
      " Train: accuracy: 1, loss: 0.00420556 \n",
      " Validation: accuracy: 0.992188 loss: 0.0077277\n",
      " cross_entropy: 0.013808\n",
      "step 43400/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0315031 \n",
      " Validation: accuracy: 1 loss: 0.000694603\n",
      " cross_entropy: 0.0134196\n",
      "step 43500/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0258355 \n",
      " Validation: accuracy: 1 loss: 0.00754614\n",
      " cross_entropy: 0.0248617\n",
      "step 43600/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0100209 \n",
      " Validation: accuracy: 1 loss: 0.00740871\n",
      " cross_entropy: 0.00383928\n",
      "step 43700/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0164767 \n",
      " Validation: accuracy: 1 loss: 0.00491029\n",
      " cross_entropy: 0.00911552\n",
      "step 43800/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0153901 \n",
      " Validation: accuracy: 0.984375 loss: 0.0191116\n",
      " cross_entropy: 0.0110884\n",
      "step 43900/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0147735 \n",
      " Validation: accuracy: 0.992188 loss: 0.0195933\n",
      " cross_entropy: 0.00728878\n",
      "step 44000/50000 \n",
      " Train: accuracy: 1, loss: 0.00251679 \n",
      " Validation: accuracy: 1 loss: 0.00485393\n",
      " cross_entropy: 0.000945982\n",
      "step 44100/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.028638 \n",
      " Validation: accuracy: 1 loss: 0.0027465\n",
      " cross_entropy: 0.0192219\n",
      "step 44200/50000 \n",
      " Train: accuracy: 1, loss: 0.00305369 \n",
      " Validation: accuracy: 1 loss: 0.000453149\n",
      " cross_entropy: 0.0013091\n",
      "step 44300/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0150525 \n",
      " Validation: accuracy: 1 loss: 0.00180171\n",
      " cross_entropy: 0.0215787\n",
      "step 44400/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0186456 \n",
      " Validation: accuracy: 1 loss: 0.00690103\n",
      " cross_entropy: 0.0119081\n",
      "step 44500/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0121515 \n",
      " Validation: accuracy: 1 loss: 0.00289951\n",
      " cross_entropy: 0.0152139\n",
      "step 44600/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0130519 \n",
      " Validation: accuracy: 0.984375 loss: 0.0191126\n",
      " cross_entropy: 0.00801956\n",
      "step 44700/50000 \n",
      " Train: accuracy: 1, loss: 0.00441731 \n",
      " Validation: accuracy: 0.992188 loss: 0.0446344\n",
      " cross_entropy: 0.0112293\n",
      "step 44800/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0315214 \n",
      " Validation: accuracy: 0.992188 loss: 0.0107321\n",
      " cross_entropy: 0.0243444\n",
      "step 44900/50000 \n",
      " Train: accuracy: 1, loss: 0.00316112 \n",
      " Validation: accuracy: 0.992188 loss: 0.0131006\n",
      " cross_entropy: 0.016892\n",
      "step 45000/50000 \n",
      " Train: accuracy: 1, loss: 0.00565887 \n",
      " Validation: accuracy: 0.992188 loss: 0.0237482\n",
      " cross_entropy: 0.0190467\n",
      "step 45100/50000 \n",
      " Train: accuracy: 1, loss: 0.00320028 \n",
      " Validation: accuracy: 0.992188 loss: 0.0147081\n",
      " cross_entropy: 0.00601659\n",
      "step 45200/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0180663 \n",
      " Validation: accuracy: 1 loss: 0.00199332\n",
      " cross_entropy: 0.0028045\n",
      "step 45300/50000 \n",
      " Train: accuracy: 1, loss: 0.0075895 \n",
      " Validation: accuracy: 0.992188 loss: 0.0108609\n",
      " cross_entropy: 0.00477852\n",
      "step 45400/50000 \n",
      " Train: accuracy: 1, loss: 0.00522799 \n",
      " Validation: accuracy: 0.992188 loss: 0.01482\n",
      " cross_entropy: 0.00292461\n",
      "step 45500/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0260239 \n",
      " Validation: accuracy: 1 loss: 0.00972665\n",
      " cross_entropy: 0.00931419\n",
      "step 45600/50000 \n",
      " Train: accuracy: 1, loss: 0.0153878 \n",
      " Validation: accuracy: 1 loss: 0.00144682\n",
      " cross_entropy: 0.0107501\n",
      "step 45700/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0385445 \n",
      " Validation: accuracy: 0.984375 loss: 0.032051\n",
      " cross_entropy: 0.0199057\n",
      "step 45800/50000 \n",
      " Train: accuracy: 1, loss: 0.00317653 \n",
      " Validation: accuracy: 0.984375 loss: 0.0584017\n",
      " cross_entropy: 0.00134592\n",
      "step 45900/50000 \n",
      " Train: accuracy: 1, loss: 0.00940659 \n",
      " Validation: accuracy: 1 loss: 0.00117878\n",
      " cross_entropy: 0.013294\n",
      "step 46000/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0181565 \n",
      " Validation: accuracy: 0.984375 loss: 0.0164108\n",
      " cross_entropy: 0.0200727\n",
      "step 46100/50000 \n",
      " Train: accuracy: 1, loss: 0.00547165 \n",
      " Validation: accuracy: 0.976562 loss: 0.0636781\n",
      " cross_entropy: 0.00110227\n",
      "step 46200/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0250711 \n",
      " Validation: accuracy: 0.992188 loss: 0.0121399\n",
      " cross_entropy: 0.010028\n",
      "step 46300/50000 \n",
      " Train: accuracy: 1, loss: 0.0101769 \n",
      " Validation: accuracy: 0.976562 loss: 0.0420127\n",
      " cross_entropy: 0.0278718\n",
      "step 46400/50000 \n",
      " Train: accuracy: 1, loss: 0.00128576 \n",
      " Validation: accuracy: 0.984375 loss: 0.0277272\n",
      " cross_entropy: 0.0110545\n",
      "step 46500/50000 \n",
      " Train: accuracy: 1, loss: 0.00897829 \n",
      " Validation: accuracy: 1 loss: 0.00151938\n",
      " cross_entropy: 0.00596921\n",
      "step 46600/50000 \n",
      " Train: accuracy: 1, loss: 0.00429043 \n",
      " Validation: accuracy: 0.992188 loss: 0.030826\n",
      " cross_entropy: 0.0350281\n",
      "step 46700/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0144233 \n",
      " Validation: accuracy: 0.992188 loss: 0.0203545\n",
      " cross_entropy: 0.0114496\n",
      "step 46800/50000 \n",
      " Train: accuracy: 1, loss: 0.00809398 \n",
      " Validation: accuracy: 0.992188 loss: 0.0103138\n",
      " cross_entropy: 0.0124419\n",
      "step 46900/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0333139 \n",
      " Validation: accuracy: 0.992188 loss: 0.014658\n",
      " cross_entropy: 0.0462492\n",
      "step 47000/50000 \n",
      " Train: accuracy: 1, loss: 0.00286376 \n",
      " Validation: accuracy: 1 loss: 0.00754347\n",
      " cross_entropy: 0.0286615\n",
      "step 47100/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.020573 \n",
      " Validation: accuracy: 1 loss: 0.00153869\n",
      " cross_entropy: 0.00929433\n",
      "step 47200/50000 \n",
      " Train: accuracy: 1, loss: 0.00247553 \n",
      " Validation: accuracy: 0.992188 loss: 0.0253681\n",
      " cross_entropy: 0.00522811\n",
      "step 47300/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.00761168 \n",
      " Validation: accuracy: 1 loss: 0.000505395\n",
      " cross_entropy: 0.011697\n",
      "step 47400/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0257386 \n",
      " Validation: accuracy: 0.984375 loss: 0.0257622\n",
      " cross_entropy: 0.00347156\n",
      "step 47500/50000 \n",
      " Train: accuracy: 1, loss: 0.00726812 \n",
      " Validation: accuracy: 1 loss: 0.00655033\n",
      " cross_entropy: 0.00898298\n",
      "step 47600/50000 \n",
      " Train: accuracy: 1, loss: 0.00598557 \n",
      " Validation: accuracy: 1 loss: 0.00201777\n",
      " cross_entropy: 0.00321477\n",
      "step 47700/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0175933 \n",
      " Validation: accuracy: 0.992188 loss: 0.0166435\n",
      " cross_entropy: 0.0359448\n",
      "step 47800/50000 \n",
      " Train: accuracy: 1, loss: 0.00311377 \n",
      " Validation: accuracy: 0.992188 loss: 0.00959895\n",
      " cross_entropy: 0.0265435\n",
      "step 47900/50000 \n",
      " Train: accuracy: 1, loss: 0.00195075 \n",
      " Validation: accuracy: 0.984375 loss: 0.0149072\n",
      " cross_entropy: 0.0124956\n",
      "step 48000/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.00922466 \n",
      " Validation: accuracy: 0.992188 loss: 0.0373037\n",
      " cross_entropy: 0.02117\n",
      "step 48100/50000 \n",
      " Train: accuracy: 1, loss: 0.0105447 \n",
      " Validation: accuracy: 0.984375 loss: 0.0569995\n",
      " cross_entropy: 0.0161332\n",
      "step 48200/50000 \n",
      " Train: accuracy: 1, loss: 0.000855461 \n",
      " Validation: accuracy: 1 loss: 0.0141329\n",
      " cross_entropy: 0.00261803\n",
      "step 48300/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0119402 \n",
      " Validation: accuracy: 0.992188 loss: 0.0413519\n",
      " cross_entropy: 0.0130734\n",
      "step 48400/50000 \n",
      " Train: accuracy: 1, loss: 0.0134549 \n",
      " Validation: accuracy: 0.992188 loss: 0.0137225\n",
      " cross_entropy: 0.0138781\n",
      "step 48500/50000 \n",
      " Train: accuracy: 1, loss: 0.00794491 \n",
      " Validation: accuracy: 1 loss: 0.00256812\n",
      " cross_entropy: 0.0189071\n",
      "step 48600/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.00695945 \n",
      " Validation: accuracy: 1 loss: 0.00548936\n",
      " cross_entropy: 0.0150751\n",
      "step 48700/50000 \n",
      " Train: accuracy: 1, loss: 0.00406715 \n",
      " Validation: accuracy: 1 loss: 0.00175522\n",
      " cross_entropy: 0.00245631\n",
      "step 48800/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0149946 \n",
      " Validation: accuracy: 0.992188 loss: 0.00833875\n",
      " cross_entropy: 0.015199\n",
      "step 48900/50000 \n",
      " Train: accuracy: 1, loss: 0.00796144 \n",
      " Validation: accuracy: 1 loss: 0.00952201\n",
      " cross_entropy: 0.00971647\n",
      "step 49000/50000 \n",
      " Train: accuracy: 1, loss: 0.00267609 \n",
      " Validation: accuracy: 0.992188 loss: 0.0185896\n",
      " cross_entropy: 0.013662\n",
      "step 49100/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0210423 \n",
      " Validation: accuracy: 1 loss: 0.00661881\n",
      " cross_entropy: 0.0186297\n",
      "step 49200/50000 \n",
      " Train: accuracy: 0.960938, loss: 0.0800521 \n",
      " Validation: accuracy: 0.984375 loss: 0.0471823\n",
      " cross_entropy: 0.0175613\n",
      "step 49300/50000 \n",
      " Train: accuracy: 1, loss: 0.00643859 \n",
      " Validation: accuracy: 1 loss: 0.00127748\n",
      " cross_entropy: 0.010273\n",
      "step 49400/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0185209 \n",
      " Validation: accuracy: 0.992188 loss: 0.0127569\n",
      " cross_entropy: 0.00780305\n",
      "step 49500/50000 \n",
      " Train: accuracy: 1, loss: 0.0093336 \n",
      " Validation: accuracy: 0.992188 loss: 0.0221167\n",
      " cross_entropy: 0.0217527\n",
      "step 49600/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0173002 \n",
      " Validation: accuracy: 0.992188 loss: 0.0287542\n",
      " cross_entropy: 0.0263181\n",
      "step 49700/50000 \n",
      " Train: accuracy: 0.984375, loss: 0.0231191 \n",
      " Validation: accuracy: 1 loss: 0.00392532\n",
      " cross_entropy: 0.00907547\n",
      "step 49800/50000 \n",
      " Train: accuracy: 0.992188, loss: 0.0154062 \n",
      " Validation: accuracy: 0.992188 loss: 0.0133024\n",
      " cross_entropy: 0.00808924\n",
      "step 49900/50000 \n",
      " Train: accuracy: 1, loss: 0.0063677 \n",
      " Validation: accuracy: 1 loss: 0.00168255\n",
      " cross_entropy: 0.00178376\n"
     ]
    }
   ],
   "source": [
    "convy2 = y2\n",
    "#totalSteps = int(totalSteps*perc)\n",
    "print totalSteps\n",
    "for i in range(totalSteps):\n",
    "    #if i > totalSteps*perc:\n",
    "    #   convy2 = emptyy2\n",
    "        \n",
    "    trainbatch = next_batch(batchSize,True,train_images, train_labels_clipped,_epochs_completed_train,_index_in_epoch_train)\n",
    "    valbatch = next_batch(batchSize,True,validation_images, validation_labels,_epochs_completed_val,_index_in_epoch_val)\n",
    "\n",
    "    if i%100 == 0:\n",
    "        train_loss,train_acc = sess.run([loss, accuracy],feed_dict={x: trainbatch[0], y_: trainbatch[1]})\n",
    "        val_loss, val_acc = sess.run([loss, accuracy],feed_dict={x: valbatch[0], y_: valbatch[1]})\n",
    "        hist['train_acc'].append(train_acc)\n",
    "        hist['val_acc'].append(val_acc)\n",
    "        hist['train_loss'].append(train_loss)\n",
    "        hist['val_loss'].append(val_loss)\n",
    "        print(\"step %d/%d \\n Train: accuracy: %g, loss: %g \\n Validation: accuracy: %g loss: %g\"%(i,totalSteps, train_acc, train_loss, val_acc, val_loss))\n",
    "        #hist['affinity'].append(affinity.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1], y2_: trainbatch[2]}))\n",
    "        #hist['balance'].append(balance.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1], y2_: trainbatch[2]}))\n",
    "        #hist['coactivity'].append(coact.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1], y2_: trainbatch[2]}))\n",
    "        entr = cross_entropy.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1]})\n",
    "        #hist['clustEntr'].append(clust_cross_entropy.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1], y2_: trainbatch[2]}))\n",
    "        #frb = frob.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1], y2_: trainbatch[2]})\n",
    "        \n",
    "        #print bV.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1]})\n",
    "        \n",
    "        print(\" cross_entropy: %g\"%(cc0*entr))\n",
    "    feed_dict = {x: trainbatch[0], y_: trainbatch[1]}\n",
    "    _ = sess.run([train_step],feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing & visualising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: accuracy: 1, loss: 0.00440716\n",
      "Test: accuracy: 0.992188, loss: 0.0120216\n",
      "Test: accuracy: 1, loss: 0.0110925\n",
      "Test: accuracy: 0.992188, loss: 0.0273951\n",
      "Test: accuracy: 0.984375, loss: 0.0256044\n",
      "Test: accuracy: 0.984375, loss: 0.024005\n",
      "Test: accuracy: 1, loss: 0.00711957\n",
      "Test: accuracy: 0.992188, loss: 0.0184605\n",
      "Test: accuracy: 1, loss: 0.00240914\n",
      "Test: accuracy: 0.992188, loss: 0.0158024\n",
      "Test: accuracy: 0.992188, loss: 0.0516368\n",
      "Test: accuracy: 1, loss: 0.00322441\n",
      "Test: accuracy: 1, loss: 0.00517339\n",
      "Test: accuracy: 0.992188, loss: 0.0121308\n",
      "Test: accuracy: 0.992188, loss: 0.0175524\n",
      "Test: accuracy: 0.984375, loss: 0.0224174\n",
      "Test: accuracy: 0.984375, loss: 0.0732102\n",
      "Test: accuracy: 1, loss: 0.00405968\n",
      "Test: accuracy: 1, loss: 0.00272107\n",
      "Test: accuracy: 1, loss: 0.00273038\n",
      "Test: accuracy: 0.992188, loss: 0.0141099\n",
      "Test: accuracy: 1, loss: 0.00231287\n",
      "Test: accuracy: 1, loss: 0.00583636\n",
      "Test: accuracy: 0.984375, loss: 0.0215354\n",
      "Test: accuracy: 0.976562, loss: 0.0405668\n",
      "Test: accuracy: 1, loss: 0.00926865\n",
      "Test: accuracy: 1, loss: 0.0133317\n",
      "Test: accuracy: 1, loss: 0.00506307\n",
      "Test: accuracy: 0.992188, loss: 0.0211015\n",
      "Test: accuracy: 0.984375, loss: 0.0438495\n",
      "Test: accuracy: 0.96875, loss: 0.0456904\n",
      "Test: accuracy: 1, loss: 0.00384441\n",
      "Test: accuracy: 0.984375, loss: 0.0354358\n",
      "Test: accuracy: 0.992188, loss: 0.0657807\n",
      "Test: accuracy: 0.992188, loss: 0.014597\n",
      "Test: accuracy: 0.992188, loss: 0.0231931\n",
      "Test: accuracy: 0.992188, loss: 0.00997694\n",
      "Test: accuracy: 0.992188, loss: 0.0147282\n",
      "Test: accuracy: 1, loss: 0.00583613\n",
      "Test: accuracy: 1, loss: 0.00572778\n",
      "Test: accuracy: 0.992188, loss: 0.0493064\n",
      "Test: accuracy: 1, loss: 0.00645078\n",
      "Test: accuracy: 0.992188, loss: 0.025475\n",
      "Test: accuracy: 0.992188, loss: 0.00916044\n",
      "Test: accuracy: 1, loss: 0.00851791\n",
      "Test: accuracy: 1, loss: 0.00634068\n",
      "Test: accuracy: 1, loss: 0.00358477\n",
      "Test: accuracy: 1, loss: 0.0114934\n",
      "Test: accuracy: 0.992188, loss: 0.0360946\n",
      "Test: accuracy: 0.992188, loss: 0.0122623\n",
      "Test: accuracy: 0.992188, loss: 0.0111809\n",
      "Test: accuracy: 1, loss: 0.00597094\n",
      "Test: accuracy: 0.992188, loss: 0.010598\n",
      "Test: accuracy: 0.992188, loss: 0.0124006\n",
      "Test: accuracy: 0.984375, loss: 0.0192949\n",
      "Test: accuracy: 1, loss: 0.00453474\n",
      "Test: accuracy: 0.992188, loss: 0.033129\n",
      "Test: accuracy: 1, loss: 0.00753815\n",
      "Test: accuracy: 1, loss: 0.00402645\n",
      "Test: accuracy: 0.992188, loss: 0.0108886\n",
      "Test: accuracy: 1, loss: 0.00959564\n",
      "Test: accuracy: 0.984375, loss: 0.0238508\n",
      "Test: accuracy: 1, loss: 0.00414957\n",
      "Test: accuracy: 1, loss: 0.00358641\n",
      "Test: accuracy: 1, loss: 0.00614101\n",
      "Test: accuracy: 1, loss: 0.0114182\n",
      "Test: accuracy: 0.984375, loss: 0.0346163\n",
      "Test: accuracy: 0.992188, loss: 0.0261382\n",
      "Test: accuracy: 0.992188, loss: 0.0196529\n",
      "Test: accuracy: 0.992188, loss: 0.0338197\n",
      "Test: accuracy: 1, loss: 0.00459829\n",
      "Test: accuracy: 1, loss: 0.00679875\n",
      "Test: accuracy: 1, loss: 0.00272179\n",
      "Test: accuracy: 1, loss: 0.00634452\n",
      "Test: accuracy: 0.992188, loss: 0.0270933\n",
      "Test: accuracy: 0.992188, loss: 0.00844817\n",
      "Test: accuracy: 0.992188, loss: 0.0338225\n",
      "Test: accuracy: 0.984375, loss: 0.0307132\n",
      "Test: accuracy: 1, loss: 0.00216327\n",
      "Test: accuracy: 1, loss: 0.00308477\n",
      "Test: accuracy: 1, loss: 0.000744694\n",
      "Test: accuracy: 0.992188, loss: 0.023078\n",
      "Test: accuracy: 1, loss: 0.00514681\n",
      "Test: accuracy: 1, loss: 0.0026909\n",
      "Test: accuracy: 0.984375, loss: 0.0205273\n",
      "Test: accuracy: 0.992188, loss: 0.0175379\n",
      "Test: accuracy: 0.992188, loss: 0.0307283\n",
      "Test: accuracy: 0.992188, loss: 0.0137432\n",
      "Test: accuracy: 1, loss: 0.000773174\n",
      "Test: accuracy: 0.992188, loss: 0.0239067\n",
      "Test: accuracy: 1, loss: 0.00675223\n",
      "Test: accuracy: 1, loss: 0.000530448\n",
      "Test: accuracy: 0.984375, loss: 0.0268467\n",
      "Test: accuracy: 0.992188, loss: 0.111417\n",
      "Test: accuracy: 1, loss: 0.0108035\n",
      "Test: accuracy: 0.976562, loss: 0.0488493\n",
      "Test: accuracy: 0.992188, loss: 0.0225282\n",
      "Test: accuracy: 1, loss: 0.00726642\n",
      "Test: accuracy: 0.992188, loss: 0.0288079\n",
      "Test: accuracy: 1, loss: 0.00745312\n",
      "0.994219\n"
     ]
    }
   ],
   "source": [
    "tAcc = []\n",
    "testSize = 1000\n",
    "for i in range(100):\n",
    "    testbatch = next_batch(batchSize,True,test_images, test_labels,_epochs_completed_test,_index_in_epoch_test)\n",
    "\n",
    "    test_loss,test_acc = sess.run([loss,accuracy],{x: testbatch[0], y_: testbatch[1]})\n",
    "    tAcc.append(test_acc)\n",
    "    print('Test: accuracy: %g, loss: %g'%(test_acc,test_loss))\n",
    "print np.average(tAcc)\n",
    "testAcc = np.average(tAcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f2aaa189410>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA20AAANSCAYAAAD23iayAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2aaa770bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot accuracy and loss\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(211)\n",
    "plt.plot(hist['train_acc'],'-b',label='train_acc')\n",
    "plt.plot(hist['val_acc'],'-r',label='val_acc')\n",
    "plt.legend()\n",
    "plt.subplot(212)\n",
    "plt.plot(hist['train_loss'],'-b',label='train_loss')\n",
    "plt.plot(hist['val_loss'],'-r',label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f2aa0be2a10>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3wAAAGfCAYAAAAEW9AnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XuwXnVh7//Pt0kwWqhcwlFK8JfoL3VyIdmQDaaAJUXu\nxkARDgHkZpFBuchQsGlxLN46jNCjglxEIAWPHS7xCGmJh0O4ODiAZeMvVUJUEiaVUEBMKGKRQuj3\n90ceckLYuZC9d3byzes1syfPWuu71vN99qzZw5u1nucptdYAAADQnt8b7AkAAAAwMAQfAABAowQf\nAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAo4YO9gQ2xogRI+qoUaMGexoA\nAACD4pFHHvl1rXXn9Y3bIoNv1KhR6enpGexpAAAADIpSyr9uyDi3dAIAADRK8AEAADRK8AEAADRq\ni3wPHwAAsPl69dVXs3Tp0rz88suDPZUt3vDhwzNy5MgMGzZso/YXfAAAQL9aunRptttuu4waNSql\nlMGezhar1pply5Zl6dKlGT169EYdwy2dAABAv3r55Zez0047ib0+KqVkp5126tOVUsEHAAD0O7HX\nP/r6exR8AAAAjRJ8AADAVuHWW2/N2LFj86d/+qdJkuOOOy4TJ07MV7/61Xzuc5/LvHnz1rn/nDlz\ncvHFFydJbrvttjz22GMDPue+8qEtAADAVuG6667Lt771rey333555pln8vDDD2fRokUbvP/06dMz\nffr0JCuDb9q0aRk3btxATbdfuMIHAAA058gjj8zkyZMzfvz4XHPNNfnCF76QH/7wh/nzP//zXHDB\nBTn44IPz1FNPpaurK/fff39OOeWUzJ49O0kyatSo/M3f/E323HPP7L777vnZz36WJPn7v//7nHXW\nWXnggQcyZ86cXHDBBenq6srixYuz5557rnruxx9//A3Lg8kVPgAAYOCce24yf37/HrOrK/na19Y5\n5Prrr8+OO+6Y3/3ud9lrr73ygx/8IPfcc08uvfTSdHd358wzz8y0adMyvzO366677g37jxgxIj/+\n8Y9z5ZVX5tJLL8211167ats+++yT6dOnZ9q0aTn66KOTJO985zszf/78dHV1ZdasWTn11FP79zVv\nJFf4AACA5lx22WWZNGlSpkyZkieffDKPP/74W9r/qKOOSpJMnjw5S5YsWe/40047LbNmzcprr72W\nm2++Occff/zGTLvfucIHAAAMnPVciRsI9913X+bNm5cHH3ww73jHOzJ16tS3/F12b3vb25IkQ4YM\nyYoVK9Y7/qMf/Wg+//nP54ADDsjkyZOz0047bdTc+5srfAAAQFNeeOGF7LDDDnnHO96Rn/3sZ3no\noYf6/Tm22267vPjii6uWhw8fnkMOOSSf/OQnN5vbORPBBwAANObQQw/NihUrMnbs2MycOTNTpkzp\n9+eYMWNGLrnkkuyxxx5ZvHhxkuSEE07I7/3e7+Xggw/u9+fbWKXWOthzeMu6u7trT0/PYE8DAADo\nxcKFCzN27NjBnsYmd+mll+aFF17IF7/4xX49bm+/z1LKI7XW7vXt6z18AAAAffRnf/ZnWbx4ce65\n557BnsobCD4AAIA++t73vjfYU+iV9/ABAAA0SvABAAA0SvABAAA0SvABAAA0SvABAABNWbJkSSZM\nmLDB40855ZTMnj17AGc0eAQfAABAowQfAADQnBUrVuSEE07I2LFjc/TRR+ell17KF77whey1116Z\nMGFCTj/99NRa37Tf2sZMnTo1f/mXf5m99947f/RHf5T7778/SfLaa6/l/PPPz4QJEzJx4sRcfvnl\nSZJHHnkk+++/fyZPnpxDDjkkTz/99KZ78avxPXwAAMCAefzxc/Pb387v12Nuu21Xxoz52jrH/Pzn\nP891112XfffdNx//+Mdz5ZVX5qyzzsrnPve5JMmJJ56Yf/qnf8pHPvKRN+y3rjErVqzIP//zP2fu\n3Ln5/Oc/n3nz5uWaa67JkiVLMn/+/AwdOjTLly/Pq6++mrPPPju33357dt5559x888258MILc/31\n1/fr72FDCD4AAKA5u+22W/bdd98kycc+9rFcdtllGT16dL7yla/kpZdeyvLlyzN+/Pg3Bd+99967\n1jFHHXVUkmTy5MlZsmRJkmTevHk544wzMnToyrTacccd8+ijj+bRRx/NQQcdlGTlVcBddtllU7zs\nNxF8AADAgFnflbiBUkp50/KnPvWp9PT0ZLfddstFF12Ul19++Q1jXn755XWOedvb3pYkGTJkSFas\nWLHW5661Zvz48XnwwQf78RVtHO/hAwAAmvPLX/5yVXD9wz/8Q/bbb78kyYgRI/Lb3/6210/lfD3u\n1jVmTQcddFC++c1vrgrA5cuX5/3vf3+ee+65Vc//6quvZsGCBf3yut4qV/gAAIDmvP/9788VV1yR\nj3/84xk3blw++clP5vnnn8+ECRPy7ne/O3vttdeb9tl+++3ziU98Yp1j1nTaaaflF7/4RSZOnJhh\nw4blE5/4RM4666zMnj0755xzTl544YWsWLEi5557bsaPHz8QL3WdSm+fTLO56+7urj09PYM9DQAA\noBcLFy7M2LFjB3sazejt91lKeaTW2r2+fd3SCQAA0CjBBwAA0CjBBwAA0CjBBwAA0CjBBwAA0CjB\nBwAA0CjBBwAAsIH+9m//9g3L++yzzzrH9/T05JxzzkmS3HfffXnggQcGbG69EXwAAAAbaM3gW1/A\ndXd357LLLksi+AAAAPrFjTfemIkTJ2bSpEk58cQTs2TJkhxwwAGZOHFiPvShD+WXv/xlkuQf//Ef\n84EPfCB77LFHDjzwwDz77LNJkt/+9rc59dRTs/vuu2fixIn57ne/m5kzZ+Z3v/tdurq6csIJJyRJ\ntt122yTJjBkzcscdd6x6/lNOOSWzZ8/Offfdl2nTpmXJkiW5+uqr89WvfjVdXV25//77M3r06Lz6\n6qtJkt/85jdvWO4vQ/v1aAAAAKs593+fm/nPzO/XY3a9uytfO/Rra92+YMGCfOlLX8oDDzyQESNG\nZPny5Tn55JNX/Vx//fU555xzctttt2W//fbLQw89lFJKrr322nzlK1/J3/3d3+WLX/xi3vnOd+an\nP/1pkuT555/PRz/60XzjG9/I/Plvfj3HHntsbrnllnz4wx/OK6+8krvvvjtXXXVVfvSjHyVJRo0a\nlTPOOCPbbrttzj///CTJ1KlTc8cdd+TII4/MTTfdlKOOOirDhg3r19+VK3wAAEBT7rnnnhxzzDEZ\nMWJEkmTHHXfMgw8+mOOPPz5JcuKJJ+aHP/xhkmTp0qU55JBDsvvuu+eSSy7JggULkiTz5s3LmWee\nueqYO+ywwzqf87DDDsu9996b//zP/8z3v//9/Mmf/Ene/va3r3Of0047LbNmzUqSzJo1K6eeeurG\nveB1cIUPAAAYMOu6Erc5OPvss3Peeedl+vTpue+++3LRRRdt1HGGDx+eqVOn5s4778zNN9+cGTNm\nrHeffffdN0uWLMl9992X1157LRMmTNio514XV/gAAICmHHDAAbn11luzbNmyJMny5cuzzz775Kab\nbkqSfOc738kHP/jBJMkLL7yQXXfdNUlyww03rDrGQQcdlCuuuGLV8vPPP58kGTZs2FrfZ3fsscdm\n1qxZuf/++3PooYe+aft2222XF1988Q3rTjrppBx//PEDcnUvEXwAAEBjxo8fnwsvvDD7779/Jk2a\nlPPOOy+XX355Zs2alYkTJ+bb3/52vv71rydJLrroohxzzDGZPHnyqltAk+Szn/1snn/++UyYMCGT\nJk3KvffemyQ5/fTTM3HixFUf2rK6gw8+OD/4wQ9y4IEHZptttnnT9o985CP53ve+t+pDW5LkhBNO\nyPPPP5/jjjtuIH4VKbXWATnwQOru7q49PT2DPQ0AAKAXCxcuzNixYwd7GluE2bNn5/bbb8+3v/3t\ntY7p7fdZSnmk1tq9vuP3y3v4SimHJvl6kiFJrq21XrzG9tLZfniSl5KcUmv98WrbhyTpSfJUrXVa\nf8wJAABgc3b22Wfn+9//fubOnTtgz9Hn4OvE2hVJDkqyNMnDpZQ5tdbHVht2WJIxnZ8PJLmq8+/r\nPp1kYZI/6Ot8AAAAtgSXX375gD9Hf7yHb+8ki2qtT9RaX0lyU5Ij1hhzRJIb60oPJdm+lLJLkpRS\nRib5cJJr+2EuAAAAdPRH8O2a5MnVlpd21m3omK8l+UyS/1rXk5RSTi+l9JRSep577rm+zRgAAGAr\nMKif0llKmZbkV7XWR9Y3ttZ6Ta21u9bavfPOO2+C2QEAAGzZ+iP4nkqy22rLIzvrNmTMvkmml1KW\nZOWtoAeUUv5nP8wJAABgq9cfwfdwkjGllNGllG2SzEgyZ40xc5KcVFaakuSFWuvTtda/qrWOrLWO\n6ux3T631Y/0wJwAAgFUuuuiiXHrppW95v3//93/PlVdeuWp5yZIlefvb356urq5VPzfeeOM6j3Hb\nbbflscceW+eYgdLnT+msta4opZyV5M6s/FqG62utC0opZ3S2X51kblZ+JcOirPxahoH5GnkAAIB+\n9HrwfepTn1q17n3ve1/mz5+/wce47bbbMm3atIwbN+5N21asWJGhQ/vl2/J61S/v4au1zq21/lGt\n9X211i931l3dib10Pp3zzM723Wutb/rW9Frrfb6DDwAA6A833nhjJk6cmEmTJuXEE098w7apU6em\np2dlkvz617/OqFGjkiQLFizI3nvvna6urkycODGPP/54Zs6cmcWLF6erqysXXHDBOp9z2223zYUX\nXphJkyZlypQpefbZZ/PAAw9kzpw5ueCCC9LV1ZXFixdn6tSpOffcc9Pd3Z2vf/3rA/L6XzdwKQkA\nAGz1Pv+PC/LYv/2mX4857g//IH/zkfFr3b5gwYJ86UtfygMPPJARI0Zk+fLlueyyy9Z73Kuvvjqf\n/vSnc8IJJ+SVV17Ja6+9losvvjiPPvroqit6S5YsWRWAr7v88svzwQ9+MP/xH/+RKVOm5Mtf/nI+\n85nP5Fvf+lY++9nPZvr06Zk2bVqOPvroVfu88sorq6JzIAk+AACgKffcc0+OOeaYjBgxIkmy4447\nbtB+f/zHf5wvf/nLWbp0aY466qiMGTOm13Fru6Vzm222ybRpK29anDx5cu666661Ptexxx67QXPq\nK8EHAAAMmHVdiRssQ4cOzX/918qvAX/55ZdXrT/++OPzgQ98IHfccUcOP/zwfPOb38x73/veDT7u\nsGHDUkpJkgwZMiQrVqxY69jf//3f38jZvzWD+j18AAAA/e2AAw7IrbfemmXLliVJli9f/obto0aN\nyiOPrPwq8NmzZ69a/8QTT+S9731vzjnnnBxxxBH5yU9+ku222y4vvvhin+bTH8fYWIIPAABoyvjx\n43PhhRdm//33z6RJk3Leeee9Yfv555+fq666KnvssUd+/etfr1p/yy23ZMKECenq6sqjjz6ak046\nKTvttFP23XffTJgwYdWHtrz+Hr7Xf9b3/sAZM2bkkksuyR577JHFixf3/wteh1Jr3aRP2B+6u7vr\npniDIwAA8NYtXLgwY8eOHexpNKO332cp5ZFaa/f69nWFDwAAoFGCDwAAoFGCDwAA6Hdb4lvHNkd9\n/T0KPgAAoF8NHz48y5YtE319VGvNsmXLMnz48I0+hu/hAwAA+tXIkSOzdOnSPPfcc4M9lS3e8OHD\nM3LkyI3eX/ABAAD9atiwYRk9evRgT4O4pRMAAKBZgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8A\nAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBR\ngg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8A\nAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBR\ngg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8A\nAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBR/RJ8pZRDSyk/L6UsKqXM7GV7KaVc1tn+k1LKnp31\nu5VS7i2lPFZKWVBK+XR/zAcAAIB+CL5SypAkVyQ5LMm4JMeVUsatMeywJGM6P6cnuaqzfkWSv6i1\njksyJcmZvewLAADARuiPK3x7J1lUa32i1vpKkpuSHLHGmCOS3FhXeijJ9qWUXWqtT9daf5wktdYX\nkyxMsms/zAkAAGCr1x/Bt2uSJ1dbXpo3R9t6x5RSRiXZI8mPenuSUsrppZSeUkrPc88918cpAwAA\ntG+z+NCWUsq2Sb6b5Nxa6296G1NrvabW2l1r7d5555037QQBAAC2QP0RfE8l2W215ZGddRs0ppQy\nLCtj7zu11v/VD/MBAAAg/RN8DycZU0oZXUrZJsmMJHPWGDMnyUmdT+uckuSFWuvTpZSS5LokC2ut\n/6Mf5gIAAEDH0L4eoNa6opRyVpI7kwxJcn2tdUEp5YzO9quTzE1yeJJFSV5Kcmpn932TnJjkp6WU\n+Z11f11rndvXeQEAAGztSq11sOfwlnV3d9eenp7BngYAAMCgKKU8UmvtXt+4zeJDWwAAAOh/gg8A\nAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBR\ngg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8A\nAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBR\ngg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8A\nAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBR\ngg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8A\nAKBRgg8AAKBR/RJ8pZRDSyk/L6UsKqXM7GV7KaVc1tn+k1LKnhu6LwAAABunz8FXShmS5IokhyUZ\nl+S4Usq4NYYdlmRM5+f0JFe9hX0BAADYCP1xhW/vJItqrU/UWl9JclOSI9YYc0SSG+tKDyXZvpSy\nywbuCwAAwEboj+DbNcmTqy0v7azbkDEbsi8AAAAbYYv50JZSyumllJ5SSs9zzz032NMBAADY7PVH\n8D2VZLfVlkd21m3ImA3ZN0lSa72m1tpda+3eeeed+zxpAACA1vVH8D2cZEwpZXQpZZskM5LMWWPM\nnCQndT6tc0qSF2qtT2/gvgAAAGyEoX09QK11RSnlrCR3JhmS5Ppa64JSyhmd7VcnmZvk8CSLkryU\n5NR17dvXOQEAAJCUWutgz+Et6+7urj09PYM9DQAAgEFRSnmk1tq9vnFbzIe2AAAA8NYIPgAAgEYJ\nPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAA\ngEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJ\nPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAA\ngEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJ\nPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAA\ngEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJPgAAgEYJ\nPgAAgEb1KfhKKTuWUu4qpTze+XeHtYw7tJTy81LKolLKzNXWX1JK+Vkp5SellO+VUrbvy3wAAAD4\nv/p6hW9mkrtrrWOS3N1ZfoNSypAkVyQ5LMm4JMeVUsZ1Nt+VZEKtdWKSXyT5qz7OBwAAgI6+Bt8R\nSW7oPL4hyZG9jNk7yaJa6xO11leS3NTZL7XW/1NrXdEZ91CSkX2cDwAAAB19Db531Vqf7jx+Jsm7\nehmza5InV1te2lm3po8n+X4f5wMAAEDH0PUNKKXMS/LuXjZduPpCrbWWUurGTKKUcmGSFUm+s44x\npyc5PUne8573bMzTAAAAbFXWG3y11gPXtq2U8mwpZZda69OllF2S/KqXYU8l2W215ZGdda8f45Qk\n05J8qNa61mCstV6T5Jok6e7u3qiwBAAA2Jr09ZbOOUlO7jw+OcntvYx5OMmYUsroUso2SWZ09ksp\n5dAkn0kyvdb6Uh/nAgAAwGr6GnwXJzmolPJ4kgM7yyml/GEpZW6SdD6U5awkdyZZmOSWWuuCzv7f\nSLJdkrtKKfNLKVf3cT4AAAB0rPeWznWptS5L8qFe1v9bksNXW56bZG4v4/7fvjw/AAAAa9fXK3wA\nAABspgQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABA\nowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQf\nAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABA\nowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQf\nAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABA\nowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQfAABAowQf\nAABAowQfAABAowQfAABAo/oUfKWUHUspd5VSHu/8u8Naxh1aSvl5KWVRKWVmL9v/opRSSykj+jIf\nAAAA/q++XuGbmeTuWuuYJHd3lt+glDIkyRVJDksyLslxpZRxq23fLcnBSX7Zx7kAAACwmr4G3xFJ\nbug8viHJkb2M2TvJolrrE7XWV5Lc1NnvdV9N8pkktY9zAQAAYDV9Db531Vqf7jx+Jsm7ehmza5In\nV1te2lmXUsoRSZ6qtf5LH+cBAADAGoaub0ApZV6Sd/ey6cLVF2qttZSywVfpSinvSPLXWXk754aM\nPz3J6Unynve8Z0OfBgAAYKu13uCrtR64tm2llGdLKbvUWp8upeyS5Fe9DHsqyW6rLY/srHtfktFJ\n/qWU8vr6H5dS9q61PtPLPK5Jck2SdHd3u/0TAABgPfp6S+ecJCd3Hp+c5PZexjycZEwpZXQpZZsk\nM5LMqbX+tNb632qto2qto7LyVs89e4s9AAAA3rq+Bt/FSQ4qpTye5MDOckopf1hKmZsktdYVSc5K\ncmeShUluqbUu6OPzAgAAsB7rvaVzXWqty5J8qJf1/5bk8NWW5yaZu55jjerLXAAAAHijvl7hAwAA\nYDMl+AAAABol+AAAABol+AAAABol+AAAABol+AAAABol+AAAABol+AAAABol+AAAABol+AAAABol\n+AAAABol+AAAABol+AAAABol+AAAABol+AAAABol+AAAABol+AAAABol+AAAABol+AAAABol+AAA\nABol+AAAABol+AAAABol+AAAABol+AAAABol+AAAABol+AAAABol+AAAABol+AAAABol+AAAABol\n+AAAABol+AAAABol+AAAABol+AAAABol+AAAABol+AAAABol+AAAABol+AAAABol+AAAABol+AAA\nABol+AAAABol+AAAABol+AAAABol+AAAABol+AAAABol+AAAABol+AAAABol+AAAABpVaq2DPYe3\nrJTyXJJ/Hex50Gcjkvx6sCdBs5xfDCTnFwPNOcZAcn614f+pte68vkFbZPDRhlJKT621e7DnQZuc\nXwwk5xcDzTnGQHJ+bV3c0gkAANAowQcAANAowcdgumawJ0DTnF8MJOcXA805xkByfm1FvIcPAACg\nUa7wAQAANErwMaBKKTuWUu4qpTze+XeHtYw7tJTy81LKolLKzF62/0UppZZSRgz8rNlS9PX8KqVc\nUkr5WSnlJ6WU75VStt90s2dztQF/j0op5bLO9p+UUvbc0H1hY8+vUspupZR7SymPlVIWlFI+veln\nz+auL3+/OtuHlFL+v1LKP226WTPQBB8DbWaSu2utY5Lc3Vl+g1LKkCRXJDksybgkx5VSxq22fbck\nByf55SaZMVuSvp5fdyWZUGudmOQXSf5qk8yazdb6/h51HJZkTOfn9CRXvYV92Yr15fxKsiLJX9Ra\nxyWZkuRM5xer6+P59bpPJ1k4wFNlExN8DLQjktzQeXxDkiN7GbN3kkW11idqra8kuamz3+u+muQz\nSbzhlDX16fyqtf6fWuuKzriHkowc4Pmy+Vvf36N0lm+sKz2UZPtSyi4buC9bt40+v2qtT9daf5wk\ntdYXs/I/ynfdlJNns9eXv18ppYxM8uEk127KSTPwBB8D7V211qc7j59J8q5exuya5MnVlpd21qWU\nckSSp2qt/zKgs2RL1afzaw0fT/L9/p0eW6ANOV/WNmZDzzW2Xn05v1YppYxKskeSH/X7DNmS9fX8\n+lpW/g/2/xqoCTI4hg72BNjylVLmJXl3L5suXH2h1lpLKRt8la6U8o4kf52Vt3OylRqo82uN57gw\nK2+X+s7G7A+wqZRStk3y3STn1lp/M9jzoQ2llGlJflVrfaSUMnWw50P/Enz0Wa31wLVtK6U8+/qt\nKJ1bBn5k47H9AAABsUlEQVTVy7Cnkuy22vLIzrr3JRmd5F9KKa+v/3EpZe9a6zP99gLYrA3g+fX6\nMU5JMi3Jh6rvqWE958t6xgzbgH3ZuvXl/EopZVhWxt53aq3/awDnyZapL+fXR5NML6UcnmR4kj8o\npfzPWuvHBnC+bCJu6WSgzUlycufxyUlu72XMw0nGlFJGl1K2STIjyZxa609rrf+t1jqq1joqK287\n2FPssZqNPr+SlZ9mlpW3r0yvtb60CebL5m+t58tq5iQ5qfNpd1OSvNC5tXhD9mXrttHnV1n5fz6v\nS7Kw1vo/Nu202UJs9PlVa/2rWuvIzn9vzUhyj9hrhyt8DLSLk9xSSvnzJP+a5L8nSSnlD5NcW2s9\nvNa6opRyVpI7kwxJcn2tdcGgzZgtSV/Pr28keVuSuzpXkR+qtZ6xqV8Em4+1nS+llDM6269OMjfJ\n4UkWJXkpyanr2ncQXgabqb6cX0n2TXJikp+WUuZ31v11rXXupnwNbL76eH7RsOIOJgAAgDa5pRMA\nAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBRgg8AAKBR/z/KxQQFzk7PeQAA\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2aaa578810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "#plt.plot(hist['train_loss'],'-b',label='train_loss')\n",
    "plt.plot(hist['affinity'],'-r',label='affinity')\n",
    "plt.plot(np.subtract(1,hist['balance']),'-y',label='balance')\n",
    "plt.plot(hist['coactivity'],'-g',label='coactivity')\n",
    "plt.plot(hist['clustEntr'], label='clustEntr')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "digitTrace = np.zeros((classCount*clustCount,784))\n",
    "digitTraceCount = np.zeros((classCount*clustCount))\n",
    "digitCount = np.zeros(clustCount*classCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sess.run(tf.global_variables_initializer())\n",
    "#for i in range(1000):\n",
    "#    testbatch = next_batch(1,True,test_images, test_labels,_epochs_completed_test,_index_in_epoch_test)\n",
    "#    lbl = testbatch[2].ravel()\n",
    "#    digitCount[np.argmax(lbl)]+=1\n",
    "#    acc = sess.run([accuracy],feed_dict={x: testbatch[0], y_: testbatch[1]})\n",
    "    #ypred = softmaxMat.eval({x: testbatch[0], y_: testbatch[1]})\n",
    "    #ypred.reshape(clustCount*classCount)\n",
    "    #digitTrace[np.argmax(ypred),:] += testbatch[0].ravel()\n",
    "    #digitTraceCount[np.argmax(ypred)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(digitCount)\n",
    "print(digitTraceCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python2.7/site-packages/ipykernel/__main__.py:1: RuntimeWarning: invalid value encountered in divide\n",
      "  if __name__ == '__main__':\n",
      "/opt/conda/lib/python2.7/site-packages/numpy/ma/core.py:4185: UserWarning: Warning: converting a masked element to nan.\n",
      "  warnings.warn(\"Warning: converting a masked element to nan.\")\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1kAAAC4CAYAAAAR8SaJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMrIAAACCjCwAAICg/wCHIYyhKdbFbQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2aa104a750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digitTrace = digitTrace/np.max(digitTrace)\n",
    "f,ax=plt.subplots(nrows=classCount, ncols=clustCount, figsize=(1.5*clustCount,1.5*classCount))\n",
    "gs = gridspec.GridSpec(classCount,clustCount)\n",
    "gs.update(wspace=0.025, hspace=0.025)\n",
    "f.subplots_adjust(wspace=0,hspace=0)\n",
    "for i in range(digitTrace.shape[0]):\n",
    "    sp = plt.subplot(gs[i])\n",
    "    sp.set_xticklabels([])\n",
    "    sp.set_yticklabels([])\n",
    "    sp.set_aspect('equal')\n",
    "    sp.grid = False\n",
    "    plt.imshow(np.reshape(digitTrace[i,:],(28,28)),cmap='bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stepCount = len(hist['train_acc'])*100\n",
    "with open('./trainlog.txt','ab') as f:\n",
    "    f.write('lr: %g, batchsize: %i, steps: %i, thresh: %g, c1: %g, c2: %g, c3: %g, c4: %g, test_acc: %g, test_loss: %g\\n'%\n",
    "            (lr,batchSize,stepCount,tresh.eval(), cc1, cc2, cc3, cc4, test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 2, 10)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testbatch[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "notify(\"Superclass: %g\"%(testAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare it to k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "tb0 = [tb[0][np.argmax(tb[1],1)<5],tb[1][np.argmax(tb[1],1)<5]]\n",
    "tb1 = [tb[0][np.argmax(tb[1],1)>4],tb[1][np.argmax(tb[1],1)>4]]\n",
    "#<5\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "km0_ypred = kmeans.fit_transform(tb0[0])\n",
    "km0_ypred = np.argmax(km0_ypred,1)\n",
    "km_ylookup = [np.argmax(np.sum(tb0[1][km0_ypred==i],0)).astype('int32') for i in range(clustCount)]\n",
    "km_yconverted = [km_ylookup[i] for i in km0_ypred]\n",
    "km_correct_prediction = tf.equal(km_yconverted, np.argmax(tb0[1],1).astype('int32'))\n",
    "km0_accuracy = tf.reduce_mean(tf.cast(km_correct_prediction, tf.float32)).eval()\n",
    "#>4\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "km1_ypred = kmeans.fit_transform(tb1[0])\n",
    "km1_ypred = np.argmax(km1_ypred,1)\n",
    "km_ylookup = [np.argmax(np.sum(tb1[1][km1_ypred==i],0)).astype('int32') for i in range(clustCount)]\n",
    "km_yconverted = [km_ylookup[i] for i in km1_ypred]\n",
    "km_correct_prediction = tf.equal(km_yconverted, np.argmax(tb1[1],1).astype('int32'))\n",
    "km1_accuracy = tf.reduce_mean(tf.cast(km_correct_prediction, tf.float32)).eval()\n",
    "print('ACOL Accuracy: %g'%(accuracy))\n",
    "print('KMeans Accuracy: %g'%((km0_accuracy+km1_accuracy)/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#visualise kmeans\n",
    "digitTrace = np.concatenate([[np.sum(tb0[0][km0_ypred==i,:],axis=0) for i in range(clustCount)],\n",
    "                       [np.sum(tb1[0][km1_ypred==i,:],axis=0) for i in range(clustCount)]])\n",
    "digitTrace = digitTrace/np.max(digitTrace)\n",
    "f,ax=plt.subplots(nrows=classCount, ncols=clustCount, figsize=(1.5*clustCount,1.5*classCount))\n",
    "gs = gridspec.GridSpec(classCount,clustCount)\n",
    "gs.update(wspace=0.025, hspace=0.025)\n",
    "f.subplots_adjust(wspace=0,hspace=0)\n",
    "for i in range(digitTrace.shape[0]):\n",
    "    sp = plt.subplot(gs[i])\n",
    "    sp.set_xticklabels([])\n",
    "    sp.set_yticklabels([])\n",
    "    sp.set_aspect('equal')\n",
    "    sp.grid = False\n",
    "    plt.imshow(np.reshape(digitTrace[i,:],(28,28)),cmap='bone')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
