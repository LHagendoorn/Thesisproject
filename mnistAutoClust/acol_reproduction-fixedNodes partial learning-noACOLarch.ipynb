{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACOL replication tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#imports and settings:\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "#from jupyterthemes import jtplot\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import threshold\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "from tensorflow.contrib.learn.python.learn.datasets import mnist\n",
    "#jtplot.style()\n",
    "%matplotlib inline\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "TRAIN_IMAGES = 'train-images-idx3-ubyte.gz'\n",
    "TRAIN_LABELS = 'train-labels-idx1-ubyte.gz'\n",
    "TEST_IMAGES = 't10k-images-idx3-ubyte.gz'\n",
    "TEST_LABELS = 't10k-labels-idx1-ubyte.gz'\n",
    "SOURCE_URL = 'https://storage.googleapis.com/cvdf-datasets/mnist/'\n",
    "\n",
    "local_file = base.maybe_download(TRAIN_IMAGES, './MNIST_data',\n",
    "                                   SOURCE_URL + TRAIN_IMAGES)\n",
    "with open(local_file, 'rb') as f:\n",
    "    train_images = mnist.extract_images(f)\n",
    "    \n",
    "local_file = base.maybe_download(TRAIN_LABELS, './MNIST_data',\n",
    "                                   SOURCE_URL + TRAIN_LABELS)\n",
    "with open(local_file, 'rb') as f:\n",
    "    train_labels = mnist.extract_labels(f, one_hot=True)\n",
    "\n",
    "local_file = base.maybe_download(TEST_IMAGES, './MNIST_data',\n",
    "                                   SOURCE_URL + TEST_IMAGES)\n",
    "with open(local_file, 'rb') as f:\n",
    "    test_images = mnist.extract_images(f)\n",
    "\n",
    "local_file = base.maybe_download(TEST_LABELS, './MNIST_data',\n",
    "                                   SOURCE_URL + TEST_LABELS)\n",
    "with open(local_file, 'rb') as f:\n",
    "    test_labels = mnist.extract_labels(f, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clustCount = 5\n",
    "classCount = 10\n",
    "net = 0\n",
    "trainsteps = 50000\n",
    "#trainsteps = 30000\n",
    "perc = 0.1\n",
    "validation_size=5000\n",
    "_epochs_completed_train = 0\n",
    "_index_in_epoch_train = 0\n",
    "_epochs_completed_val = 0\n",
    "_index_in_epoch_val = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_super_labels = np.array([y[np.argmax(train_labels[j])] for j in range(60000)])\n",
    "train_labels_clipped = np.array([train_labels[j] for j in range(int(60000*perc))])\n",
    "train_labels_clipped = np.concatenate([train_labels_clipped,np.array([train_labels[j] for j in range(int(60000*perc),60000)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not 0 <= validation_size <= len(train_images):\n",
    "    raise ValueError(\n",
    "        'Validation size should be between 0 and {}. Received: {}.'\n",
    "        .format(len(train_images), validation_size))\n",
    "\n",
    "validation_images = train_images[:validation_size]\n",
    "validation_labels = train_labels[:validation_size]\n",
    "#validation_super_labels = train_super_labels[:validation_size]\n",
    "validation_labels_clipped = train_labels_clipped[:validation_size]\n",
    "train_images = train_images[validation_size:]\n",
    "train_labels = train_labels[validation_size:]\n",
    "#train_super_labels = train_super_labels[validation_size:]\n",
    "train_labels_clipped = train_labels_clipped[validation_size:]\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0],train_images.shape[1] * train_images.shape[2])\n",
    "train_images = train_images.astype(np.float32)\n",
    "train_images = np.multiply(train_images, 1.0 / 255.0)\n",
    "\n",
    "validation_images = validation_images.reshape(validation_images.shape[0],validation_images.shape[1] * validation_images.shape[2])\n",
    "validation_images = validation_images.astype(np.float32)\n",
    "validation_images = np.multiply(validation_images, 1.0 / 255.0)\n",
    "\n",
    "#    options = dict(dtype=dtypes.float32, reshape=True, seed=None)\n",
    "  \n",
    "#    train = DataSet(train_images, train_labels, **options)\n",
    "#    validation = DataSet(validation_images, validation_labels, **options)\n",
    "#    test = DataSet(test_images, test_labels, **options)\n",
    "  \n",
    "#    mnist = base.Datasets(train=train, validation=validation, test=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def next_batch(batch_size, shuffle, images, labels, ep_compl, ep_ind):\n",
    "    _epochs_completed = ep_compl\n",
    "    _index_in_epoch = ep_ind\n",
    "    \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "    start = _index_in_epoch\n",
    "    _num_examples = images.shape[0]\n",
    "    # Shuffle for the first epoch\n",
    "    if _epochs_completed == 0 and start == 0 and shuffle:\n",
    "      perm0 = np.arange(_num_examples)\n",
    "      np.random.shuffle(perm0)\n",
    "      _images = images[perm0]\n",
    "      _labels = labels[perm0]\n",
    "    # Go to the next epoch\n",
    "    if start + batch_size > _num_examples:\n",
    "      # Finished epoch\n",
    "      _epochs_completed += 1\n",
    "      # Get the rest examples in this epoch\n",
    "      rest_num_examples = _num_examples - start\n",
    "      images_rest_part = _images[start:_num_examples]\n",
    "      labels_rest_part = _labels[start:_num_examples]\n",
    "      # Shuffle the data\n",
    "      if shuffle:\n",
    "        perm = np.arange(_num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        _images = images[perm]\n",
    "        print(_images)\n",
    "        _labels = labels[perm]\n",
    "      # Start next epoch\n",
    "      start = 0\n",
    "      _index_in_epoch = batch_size - rest_num_examples\n",
    "      end = _index_in_epoch\n",
    "      images_new_part = _images[start:end]\n",
    "      labels_new_part = _labels[start:end]\n",
    "      return np.concatenate((images_rest_part, images_new_part), axis=0) , np.concatenate((labels_rest_part, labels_new_part), axis=0)\n",
    "    else:\n",
    "      _index_in_epoch += batch_size\n",
    "      end = _index_in_epoch\n",
    "      return _images[start:end], _labels[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#helper funcs\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "def matrix_softmax(x):\n",
    "    shape = x.get_shape().as_list()\n",
    "    shape[0] = int(-1)\n",
    "    return tf.reshape(tf.nn.softmax(tf.contrib.layers.flatten(x)),shape)\n",
    "\n",
    "def avg_softmax(x):\n",
    "    shape = x.get_shape().as_list()\n",
    "    shape[0] = int(-1)\n",
    "    totalSoft = tf.reshape(tf.nn.softmax(tf.contrib.layers.flatten(x)),shape)\n",
    "    return tf.reduce_sum(totalSoft,2)\n",
    "\n",
    "def max_softmax(x):\n",
    "    shape = x.get_shape().as_list()\n",
    "    shape[0] = int(-1)\n",
    "    totalSoft = tf.reshape(tf.nn.softmax(tf.contrib.layers.flatten(x)),shape)\n",
    "    return tf.reduce_max(totalSoft,2)\n",
    "\n",
    "def initACOL(in_size,clust,clss):\n",
    "    acolLayers = []\n",
    "    for i in range(clss):\n",
    "        acolLayers.append([\n",
    "            weight_variable([in_size, clustCount]),\n",
    "            bias_variable([clustCount])\n",
    "        ])\n",
    "    return acolLayers\n",
    "        \n",
    "def connectACOL(inLayer,acol):\n",
    "    clust = []\n",
    "    for l in range(0,len(acol)):\n",
    "        clust.append(tf.matmul(inLayer, acol[l][0]) + acol[l][1])\n",
    "    return clust\n",
    "        \n",
    "def acol(input,clust_count, class_count):\n",
    "    acolLayers = []\n",
    "    for i in range(class_count):\n",
    "        if isinstance(input, tuple):\n",
    "                input = input[0]\n",
    "\n",
    "        #I don't know what this bit does, but I don't think it'll hurt anything\n",
    "        #Or maybe it does, who knows\n",
    "        input_shape = input.get_shape()\n",
    "        if input_shape.ndims == 4:\n",
    "            dim = 1\n",
    "            for d in input_shape[1:].as_list():\n",
    "                dim *= d\n",
    "        #    feed_in = tf.reshape(tf.transpose(input,[0,3,1,2]), [-1, dim])\n",
    "        else:\n",
    "            feed_in, dim = (input, int(input_shape[-1]))\n",
    "\n",
    "        init_weights = tf.truncated_normal_initializer(0.0, stddev=0.1)#(0.0, stddev=0.01)\n",
    "        init_biases = tf.constant_initializer(1.0)#(0.1)\n",
    "\n",
    "        weights = weight_variable([dim, clust_count])\n",
    "        biases = bias_variable([clust_count])\n",
    "\n",
    "        acoll = tf.nn.xw_plus_b(input,weights,biases)\n",
    "        acolLayers.append(acol)\n",
    "    return acolLayers    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create placeholders (weights&biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if net==0:\n",
    "    #Init model weights & biases\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    x_image = tf.reshape(x, [-1,28,28,1])\n",
    "    #y_ = tf.placeholder(tf.float32, shape=[None, classCount])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None,classCount])\n",
    "    \n",
    "    #conv_layer1\n",
    "    W_conv1 = weight_variable([5,5,1,32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "\n",
    "    #conv_layer2\n",
    "    W_conv2 = weight_variable([5,5,32,64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "\n",
    "    #fc layer 1\n",
    "    W_fc1 = weight_variable([7*7*64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "\n",
    "    #acol = initACOL(1024,clustCount,classCount)\n",
    "\n",
    "    #final fc layer\n",
    "    W_fc2 = weight_variable([1024, classCount])\n",
    "    b_fc2 = bias_variable([classCount])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if net==0:\n",
    "    dropout=0.3\n",
    "    #Define net\n",
    "    #conv 1\n",
    "    l_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    l_pool1 = max_pool_2x2(l_conv1)\n",
    "\n",
    "    #conv 2\n",
    "    l_conv2 = tf.nn.relu(conv2d(l_pool1, W_conv2) + b_conv2)\n",
    "    l_pool2 = max_pool_2x2(l_conv2)\n",
    "\n",
    "    #fc 1\n",
    "    l_pool2_flat = tf.reshape(l_pool2, [-1, 7*7*64])\n",
    "    l_fc1 = tf.nn.relu(tf.matmul(l_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    l_fc1_drop = tf.nn.dropout(l_fc1, tf.constant(dropout))\n",
    "\n",
    "    y_conv = tf.matmul(l_fc1_drop, W_fc2) + b_fc2\n",
    "    \n",
    "    #l_acol = connectACOL(l_fc1_drop,acol)\n",
    "    #l_acol = acol(l_fc1_drop,clustCount,classCount)\n",
    "\n",
    "    #Classification layer\n",
    "    #stackedClusts = tf.stack(l_acol,1)\n",
    "    #softmaxMat = matrix_softmax(stackedClusts)\n",
    "    #smStacked = tf.reduce_max(softmaxMat,2)\n",
    "\n",
    "    #y_conv = smStacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helperfunctions for loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#helper loss funcs\n",
    "def zBar(x):\n",
    "    xshape = x.shape.as_list()\n",
    "    s=[-1,xshape[1]*xshape[2]]\n",
    "    return tf.maximum(tf.reshape(x,s),0)\n",
    "    \n",
    "def bigU(zb):\n",
    "    return tf.matmul(tf.transpose(zb),zb)\n",
    "\n",
    "def selectNonDiag(x):\n",
    "    selection = np.ones(x.shape.as_list()[0],dtype='float32') - np.eye(x.shape.as_list()[0],dtype='float32')\n",
    "    return tf.reduce_sum(tf.multiply(x,selection))\n",
    "\n",
    "def bigV(x):\n",
    "    smallNu=tf.reshape(tf.reduce_sum(x,axis=0),[1,-1])\n",
    "    return tf.multiply(tf.transpose(smallNu),smallNu)\n",
    "\n",
    "def specialNormalise(x):\n",
    "    top = selectNonDiag(x)\n",
    "    bottom = tf.multiply(tf.to_float(x.shape[1]-1),tf.reduce_sum(tf.multiply(x,np.eye(x.shape[1],dtype='float32'))))\n",
    "    return tf.divide(top,bottom)\n",
    "\n",
    "def frobNorm(x):\n",
    "    return tf.sqrt(tf.reduce_sum(tf.square(x)))\n",
    "\n",
    "tresh = tf.constant(0.03)\n",
    "cc0=1.0\n",
    "cc1=1.0\n",
    "cc2=1.0\n",
    "cc3=0.0003\n",
    "cc4=0.000001\n",
    "cc5=1.0\n",
    "c0 = tf.constant(cc0)\n",
    "c1 = tf.constant(cc1)\n",
    "c2 = tf.constant(cc2)\n",
    "c3val = tf.constant(cc3)\n",
    "c3 = lambda affinity: tf.cond(tf.less(affinity,tresh),lambda: c3val,lambda: tf.constant(0.0))\n",
    "c4 =tf.constant(cc4)\n",
    "c5 = tf.constant(cc5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#calculate losses\n",
    "#affinity\n",
    "#bZ = zBar(stackedClusts)#softmaxMat)\n",
    "#bU = bigU(bZ)\n",
    "#coact = selectNonDiag(bU)\n",
    "#affinity = specialNormalise(bU)\n",
    "\n",
    "#balance\n",
    "#bV=bigV(bZ)\n",
    "#balance = specialNormalise(bV)\n",
    "\n",
    "#cluster cross entropy (added if secondary label is set for that input, hard to do with batches?)\n",
    "#clust_cross_entropy = tf.reduce_mean(-tf.reduce_sum(y2_ * tf.log(tf.clip_by_value(softmaxMat,1e-10,1.0)), reduction_indices=[1,2]))\n",
    "\n",
    "#cross entropy\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y_conv,1e-10,1.0)), reduction_indices=[1]))\n",
    "\n",
    "#frob = frobNorm(stackedClusts)#softmaxMat)\n",
    "\n",
    "loss = c0*cross_entropy# + c5*clust_cross_entropy# + c1*affinity + c2*tf.subtract(tf.constant(1.0),balance) + c3(affinity)*coact + c4*frob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "totalSteps = trainsteps\n",
    "stepCount=0\n",
    "batchSize = 128\n",
    "hist = {\n",
    "    'train_acc':[],\n",
    "    'val_acc':[],\n",
    "    'train_loss':[],\n",
    "    'val_loss':[],\n",
    "    'affinity':[],\n",
    "    'balance':[],\n",
    "    'coactivity':[]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "step 0/50000 \n",
      " Train: accuracy: 0.0859375, loss: 10.9958 \n",
      " Validation: accuracy: 0.078125 loss: 10.8645\n",
      " cross_entropy: 10.1027\n",
      "step 100/50000 \n",
      " Train: accuracy: 0.0703125, loss: 6.52485 \n",
      " Validation: accuracy: 0.09375 loss: 6.68676\n",
      " cross_entropy: 6.47988\n",
      "step 200/50000 \n",
      " Train: accuracy: 0.109375, loss: 5.49042 \n",
      " Validation: accuracy: 0.0703125 loss: 4.88554\n",
      " cross_entropy: 5.08993\n",
      "step 300/50000 \n",
      " Train: accuracy: 0.09375, loss: 3.98541 \n",
      " Validation: accuracy: 0.0859375 loss: 4.33678\n",
      " cross_entropy: 3.98258\n",
      "step 400/50000 \n",
      " Train: accuracy: 0.078125, loss: 3.0601 \n",
      " Validation: accuracy: 0.078125 loss: 4.16521\n",
      " cross_entropy: 3.80856\n",
      "step 500/50000 \n",
      " Train: accuracy: 0.15625, loss: 2.55537 \n",
      " Validation: accuracy: 0.09375 loss: 2.53462\n",
      " cross_entropy: 2.74959\n",
      "step 600/50000 \n",
      " Train: accuracy: 0.09375, loss: 1.68998 \n",
      " Validation: accuracy: 0.0859375 loss: 1.91514\n",
      " cross_entropy: 1.63572\n",
      "step 700/50000 \n",
      " Train: accuracy: 0.203125, loss: 1.25923 \n",
      " Validation: accuracy: 0.0703125 loss: 1.61901\n",
      " cross_entropy: 1.63655\n",
      "step 800/50000 \n",
      " Train: accuracy: 0.125, loss: 0.899447 \n",
      " Validation: accuracy: 0.164062 loss: 1.26579\n",
      " cross_entropy: 2.01587\n",
      "step 900/50000 \n",
      " Train: accuracy: 0.117188, loss: 2.18279 \n",
      " Validation: accuracy: 0.078125 loss: 0.56011\n",
      " cross_entropy: 1.4463\n",
      "step 1000/50000 \n",
      " Train: accuracy: 0.109375, loss: 1.46521 \n",
      " Validation: accuracy: 0.148438 loss: 1.28022\n",
      " cross_entropy: 1.46249\n",
      "step 1100/50000 \n",
      " Train: accuracy: 0.148438, loss: 0.187833 \n",
      " Validation: accuracy: 0.117188 loss: 1.09705\n",
      " cross_entropy: 1.08713\n",
      "step 1200/50000 \n",
      " Train: accuracy: 0.140625, loss: 0.899447 \n",
      " Validation: accuracy: 0.164062 loss: 1.449\n",
      " cross_entropy: 0.738361\n",
      "step 1300/50000 \n",
      " Train: accuracy: 0.117188, loss: 0.539668 \n",
      " Validation: accuracy: 0.125 loss: 0.359779\n",
      " cross_entropy: 0.787131\n",
      "step 1400/50000 \n",
      " Train: accuracy: 0.132812, loss: 0.195729 \n",
      " Validation: accuracy: 0.117188 loss: 0.00639526\n",
      " cross_entropy: 1.11196\n",
      "step 1500/50000 \n",
      " Train: accuracy: 0.109375, loss: 1.07934 \n",
      " Validation: accuracy: 0.125 loss: 0.364715\n",
      " cross_entropy: 0.899447\n",
      "step 1600/50000 \n",
      " Train: accuracy: 0.09375, loss: 0.186081 \n",
      " Validation: accuracy: 0.09375 loss: 0.184531\n",
      " cross_entropy: 0.549288\n",
      "step 1700/50000 \n",
      " Train: accuracy: 0.101562, loss: 0.567206 \n",
      " Validation: accuracy: 0.109375 loss: 0.359779\n",
      " cross_entropy: 0.191552\n",
      "step 1800/50000 \n",
      " Train: accuracy: 0.125, loss: 0.539668 \n",
      " Validation: accuracy: 0.117188 loss: 0\n",
      " cross_entropy: 0.359779\n",
      "step 1900/50000 \n",
      " Train: accuracy: 0.171875, loss: 0.203724 \n",
      " Validation: accuracy: 0.171875 loss: 0.359779\n",
      " cross_entropy: 0.539668\n",
      "step 2000/50000 \n",
      " Train: accuracy: 0.125, loss: 0.36801 \n",
      " Validation: accuracy: 0.09375 loss: 0.550719\n",
      " cross_entropy: 0.378889\n",
      "step 2100/50000 \n",
      " Train: accuracy: 0.09375, loss: 0.3612 \n",
      " Validation: accuracy: 0.132812 loss: 0.363604\n",
      " cross_entropy: 0.179889\n",
      "step 2200/50000 \n",
      " Train: accuracy: 0.09375, loss: 0.54034 \n",
      " Validation: accuracy: 0.132812 loss: 0.179889\n",
      " cross_entropy: 0.180794\n",
      "step 2300/50000 \n",
      " Train: accuracy: 0.09375, loss: 0.182671 \n",
      " Validation: accuracy: 0.132812 loss: 0\n",
      " cross_entropy: 0.539668\n",
      "step 2400/50000 \n",
      " Train: accuracy: 0.109375, loss: 0.000769837 \n",
      " Validation: accuracy: 0.125 loss: 0.539668\n",
      " cross_entropy: 0.39389\n",
      "step 2500/50000 \n",
      " Train: accuracy: 0.109375, loss: 0.183343 \n",
      " Validation: accuracy: 0.148438 loss: 0.027635\n",
      " cross_entropy: 0.181979\n",
      "step 2600/50000 \n",
      " Train: accuracy: 0.109375, loss: 0.00986326 \n",
      " Validation: accuracy: 0.164062 loss: 0.179889\n",
      " cross_entropy: 0.179889\n",
      "step 2700/50000 \n",
      " Train: accuracy: 0.109375, loss: 0 \n",
      " Validation: accuracy: 0.171875 loss: 0.179889\n",
      " cross_entropy: 0.359779\n",
      "step 2800/50000 \n",
      " Train: accuracy: 0.117188, loss: 0.179889 \n",
      " Validation: accuracy: 0.148438 loss: 0.180669\n",
      " cross_entropy: 1.07934\n",
      "step 2900/50000 \n",
      " Train: accuracy: 0.101562, loss: 0.179889 \n",
      " Validation: accuracy: 0.117188 loss: 0.359779\n",
      " cross_entropy: 0.0381399\n",
      "step 3000/50000 \n",
      " Train: accuracy: 0.101562, loss: 0.179889 \n",
      " Validation: accuracy: 0.132812 loss: 0\n",
      " cross_entropy: 0.00239943\n",
      "step 3100/50000 \n",
      " Train: accuracy: 0.21875, loss: 0.00158728 \n",
      " Validation: accuracy: 0.109375 loss: 0.00368482\n",
      " cross_entropy: 0.899447\n",
      "step 3200/50000 \n",
      " Train: accuracy: 0.210938, loss: 0.179889 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0.179889\n",
      "step 3300/50000 \n",
      " Train: accuracy: 0.09375, loss: 0.359779 \n",
      " Validation: accuracy: 0.1875 loss: 0.00643798\n",
      " cross_entropy: 0.00674029\n",
      "step 3400/50000 \n",
      " Train: accuracy: 0.203125, loss: 0.00481703 \n",
      " Validation: accuracy: 0.101562 loss: 0.179889\n",
      " cross_entropy: 0.179889\n",
      "step 3500/50000 \n",
      " Train: accuracy: 0.132812, loss: 0.366208 \n",
      " Validation: accuracy: 0.179688 loss: 0.179889\n",
      " cross_entropy: 0\n",
      "step 3600/50000 \n",
      " Train: accuracy: 0.179688, loss: 0.539668 \n",
      " Validation: accuracy: 0.125 loss: 0.180115\n",
      " cross_entropy: 0.179889\n",
      "step 3700/50000 \n",
      " Train: accuracy: 0.109375, loss: 0.00980853 \n",
      " Validation: accuracy: 0.148438 loss: 0.359779\n",
      " cross_entropy: 0.00429078\n",
      "step 3800/50000 \n",
      " Train: accuracy: 0.164062, loss: 0 \n",
      " Validation: accuracy: 0.101562 loss: 0.214633\n",
      " cross_entropy: 0.371441\n",
      "step 3900/50000 \n",
      " Train: accuracy: 0.148438, loss: 0.203838 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 4000/50000 \n",
      " Train: accuracy: 0.117188, loss: 0.186951 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0.372828\n",
      "step 4100/50000 \n",
      " Train: accuracy: 0.117188, loss: 0.179889 \n",
      " Validation: accuracy: 0.101562 loss: 0.179889\n",
      " cross_entropy: 0.179889\n",
      "step 4200/50000 \n",
      " Train: accuracy: 0.164062, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0.359779\n",
      "step 4300/50000 \n",
      " Train: accuracy: 0.1875, loss: 0.179889 \n",
      " Validation: accuracy: 0.109375 loss: 0.359779\n",
      " cross_entropy: 0.0105992\n",
      "step 4400/50000 \n",
      " Train: accuracy: 0.0625, loss: 0.179889 \n",
      " Validation: accuracy: 0.140625 loss: 0\n",
      " cross_entropy: 0.00506702\n",
      "step 4500/50000 \n",
      " Train: accuracy: 0.109375, loss: 0.0101507 \n",
      " Validation: accuracy: 0.140625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 4600/50000 \n",
      " Train: accuracy: 0.171875, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0.179889\n",
      "step 4700/50000 \n",
      " Train: accuracy: 0.132812, loss: 0.359779 \n",
      " Validation: accuracy: 0.132812 loss: 0.179889\n",
      " cross_entropy: 0.179889\n",
      "step 4800/50000 \n",
      " Train: accuracy: 0.140625, loss: 0 \n",
      " Validation: accuracy: 0.101562 loss: 0.0027964\n",
      " cross_entropy: 0.179889\n",
      "step 4900/50000 \n",
      " Train: accuracy: 0.09375, loss: 0.179889 \n",
      " Validation: accuracy: 0.148438 loss: 0.181571\n",
      " cross_entropy: 0.179889\n",
      "step 5000/50000 \n",
      " Train: accuracy: 0.171875, loss: 0 \n",
      " Validation: accuracy: 0.09375 loss: 0.179889\n",
      " cross_entropy: 0.0146744\n",
      "step 5100/50000 \n",
      " Train: accuracy: 0.164062, loss: 0.179889 \n",
      " Validation: accuracy: 0.132812 loss: 0.179889\n",
      " cross_entropy: 0.00986813\n",
      "step 5200/50000 \n",
      " Train: accuracy: 0.117188, loss: 0 \n",
      " Validation: accuracy: 0.132812 loss: 0\n",
      " cross_entropy: 0\n",
      "step 5300/50000 \n",
      " Train: accuracy: 0.140625, loss: 0 \n",
      " Validation: accuracy: 0.132812 loss: 0\n",
      " cross_entropy: 0.00543133\n",
      "step 5400/50000 \n",
      " Train: accuracy: 0.203125, loss: 0.179889 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 5500/50000 \n",
      " Train: accuracy: 0.0859375, loss: 0 \n",
      " Validation: accuracy: 0.117188 loss: 0.00876201\n",
      " cross_entropy: 0\n",
      "step 5600/50000 \n",
      " Train: accuracy: 0.171875, loss: 0 \n",
      " Validation: accuracy: 0.140625 loss: 0\n",
      " cross_entropy: 0.179889\n",
      "step 5700/50000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.117188 loss: 0\n",
      " cross_entropy: 0.359779\n",
      "step 5800/50000 \n",
      " Train: accuracy: 0.171875, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0.0163705\n",
      " cross_entropy: 0\n",
      "step 5900/50000 \n",
      " Train: accuracy: 0.15625, loss: 0.179889 \n",
      " Validation: accuracy: 0.109375 loss: 0.179889\n",
      " cross_entropy: 0.179889\n",
      "step 6000/50000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0.00108915\n",
      "step 6100/50000 \n",
      " Train: accuracy: 0.132812, loss: 0.359779 \n",
      " Validation: accuracy: 0.179688 loss: 0.179889\n",
      " cross_entropy: 0\n",
      "step 6200/50000 \n",
      " Train: accuracy: 0.117188, loss: 0 \n",
      " Validation: accuracy: 0.132812 loss: 0\n",
      " cross_entropy: 0.179889\n",
      "step 6300/50000 \n",
      " Train: accuracy: 0.09375, loss: 0.179889 \n",
      " Validation: accuracy: 0.140625 loss: 0.00302271\n",
      " cross_entropy: 0.179889\n",
      "step 6400/50000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0.179889\n",
      "step 6500/50000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 6600/50000 \n",
      " Train: accuracy: 0.109375, loss: 0 \n",
      " Validation: accuracy: 0.132812 loss: 0.179889\n",
      " cross_entropy: 0\n",
      "step 6700/50000 \n",
      " Train: accuracy: 0.164062, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0.179889\n",
      "step 6800/50000 \n",
      " Train: accuracy: 0.117188, loss: 0 \n",
      " Validation: accuracy: 0.117188 loss: 0\n",
      " cross_entropy: 0\n",
      "step 6900/50000 \n",
      " Train: accuracy: 0.140625, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0.359779\n",
      "step 7000/50000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.09375 loss: 0\n",
      " cross_entropy: 0\n",
      "step 7100/50000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.171875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 7200/50000 \n",
      " Train: accuracy: 0.179688, loss: 0 \n",
      " Validation: accuracy: 0.171875 loss: 0.179889\n",
      " cross_entropy: 0\n",
      "step 7300/50000 \n",
      " Train: accuracy: 0.140625, loss: 0 \n",
      " Validation: accuracy: 0.179688 loss: 0.179889\n",
      " cross_entropy: 0\n",
      "step 7400/50000 \n",
      " Train: accuracy: 0.203125, loss: 0.179889 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0\n",
      "step 7500/50000 \n",
      " Train: accuracy: 0.132812, loss: 0.179889 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 7600/50000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.140625 loss: 0.179889\n",
      " cross_entropy: 0\n",
      "step 7700/50000 \n",
      " Train: accuracy: 0.132812, loss: 0.179889 \n",
      " Validation: accuracy: 0.125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 7800/50000 \n",
      " Train: accuracy: 0.140625, loss: 0.179889 \n",
      " Validation: accuracy: 0.117188 loss: 0\n",
      " cross_entropy: 0\n",
      "step 7900/50000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 8000/50000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.125 loss: 0\n",
      " cross_entropy: 0.179889\n",
      "step 8100/50000 \n",
      " Train: accuracy: 0.125, loss: 0 \n",
      " Validation: accuracy: 0.203125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 8200/50000 \n",
      " Train: accuracy: 0.140625, loss: 0 \n",
      " Validation: accuracy: 0.132812 loss: 0\n",
      " cross_entropy: 0\n",
      "step 8300/50000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0\n",
      "step 8400/50000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.09375 loss: 0\n",
      " cross_entropy: 0\n",
      "step 8500/50000 \n",
      " Train: accuracy: 0.140625, loss: 0.0277522 \n",
      " Validation: accuracy: 0.140625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 8600/50000 \n",
      " Train: accuracy: 0.109375, loss: 0.179889 \n",
      " Validation: accuracy: 0.1875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 8700/50000 \n",
      " Train: accuracy: 0.179688, loss: 0.179889 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0.0031904\n",
      "step 8800/50000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 8900/50000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 9000/50000 \n",
      " Train: accuracy: 0.148438, loss: 0.00359034 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 9100/50000 \n",
      " Train: accuracy: 0.132812, loss: 0.179889 \n",
      " Validation: accuracy: 0.203125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 9200/50000 \n",
      " Train: accuracy: 0.125, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0\n",
      "step 9300/50000 \n",
      " Train: accuracy: 0.203125, loss: 0 \n",
      " Validation: accuracy: 0.195312 loss: 0\n",
      " cross_entropy: 0\n",
      "step 9400/50000 \n",
      " Train: accuracy: 0.125, loss: 0 \n",
      " Validation: accuracy: 0.1875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 9500/50000 \n",
      " Train: accuracy: 0.125, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0\n",
      "step 9600/50000 \n",
      " Train: accuracy: 0.171875, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0\n",
      "step 9700/50000 \n",
      " Train: accuracy: 0.164062, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 9800/50000 \n",
      " Train: accuracy: 0.203125, loss: 0 \n",
      " Validation: accuracy: 0.171875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 9900/50000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.09375 loss: 0\n",
      " cross_entropy: 0\n",
      "step 10000/50000 \n",
      " Train: accuracy: 0.078125, loss: 0 \n",
      " Validation: accuracy: 0.195312 loss: 0\n",
      " cross_entropy: 0\n",
      "step 10100/50000 \n",
      " Train: accuracy: 0.101562, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0\n",
      "step 10200/50000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.09375 loss: 0\n",
      " cross_entropy: 0\n",
      "step 10300/50000 \n",
      " Train: accuracy: 0.140625, loss: 0 \n",
      " Validation: accuracy: 0.195312 loss: 0\n",
      " cross_entropy: 0\n",
      "step 10400/50000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 10500/50000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.132812 loss: 0\n",
      " cross_entropy: 0\n",
      "step 10600/50000 \n",
      " Train: accuracy: 0.0859375, loss: 0 \n",
      " Validation: accuracy: 0.117188 loss: 0\n",
      " cross_entropy: 0\n",
      "step 10700/50000 \n",
      " Train: accuracy: 0.164062, loss: 0 \n",
      " Validation: accuracy: 0.09375 loss: 0\n",
      " cross_entropy: 0\n",
      "step 10800/50000 \n",
      " Train: accuracy: 0.179688, loss: 0 \n",
      " Validation: accuracy: 0.195312 loss: 0\n",
      " cross_entropy: 0\n",
      "step 10900/50000 \n",
      " Train: accuracy: 0.125, loss: 0 \n",
      " Validation: accuracy: 0.179688 loss: 0\n",
      " cross_entropy: 0\n",
      "step 11000/50000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.171875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 11100/50000 \n",
      " Train: accuracy: 0.164062, loss: 0 \n",
      " Validation: accuracy: 0.140625 loss: 0\n",
      " cross_entropy: 0\n"
     ]
    }
   ],
   "source": [
    "#totalSteps = int(totalSteps*perc)\n",
    "print totalSteps\n",
    "for i in range(totalSteps):\n",
    "    #if i > totalSteps*perc:\n",
    "    #   convy2 = emptyy2\n",
    "        \n",
    "    trainbatch = next_batch(batchSize,True,train_images, train_labels,_epochs_completed_train,_index_in_epoch_train)\n",
    "    trainbatch = (trainbatch[0],trainbatch[1])\n",
    "    valbatch = next_batch(batchSize,True,validation_images, validation_labels,_epochs_completed_val,_index_in_epoch_val)\n",
    "    valbatch = (valbatch[0],valbatch[1])\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        train_loss,train_acc = sess.run([loss, accuracy],feed_dict={x: trainbatch[0], y_: trainbatch[1]})\n",
    "        val_loss, val_acc = sess.run([loss, accuracy],feed_dict={x: valbatch[0], y_: valbatch[1]})\n",
    "        hist['train_acc'].append(train_acc)\n",
    "        hist['val_acc'].append(val_acc)\n",
    "        hist['train_loss'].append(train_loss)\n",
    "        hist['val_loss'].append(val_loss)\n",
    "        print(\"step %d/%d \\n Train: accuracy: %g, loss: %g \\n Validation: accuracy: %g loss: %g\"%(i,totalSteps, train_acc, train_loss, val_acc, val_loss))\n",
    "        #hist['affinity'].append(affinity.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1]}))\n",
    "        #hist['balance'].append(balance.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1]}))\n",
    "        #hist['coactivity'].append(coact.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1]}))\n",
    "        entr = cross_entropy.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1]})\n",
    "        #entr2 = clust_cross_entropy.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1]})\n",
    "        #frb = frob.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1]})\n",
    "        \n",
    "        #print bV.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1]})\n",
    "        \n",
    "        print(\" cross_entropy: %g\"%(cc0*entr))\n",
    "    feed_dict={x: trainbatch[0], y_: trainbatch[1]}\n",
    "    _ = sess.run([train_step],feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing & visualising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testSize = 1000\n",
    "testbatch = next_batch(testSize,True,test_images, test_labels, _epochs_completed_test,_index_in_epoch_test)\n",
    "test_loss,test_acc = sess.run([loss,accuracy],{x: testbatch[0], y_: testbatch[1]})\n",
    "print('Test: accuracy: %g, loss: %g'%(test_acc,test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plot accuracy and loss\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(211)\n",
    "plt.plot(hist['train_acc'],'-b',label='train_acc')\n",
    "plt.plot(hist['val_acc'],'-r',label='val_acc')\n",
    "plt.legend()\n",
    "plt.subplot(212)\n",
    "plt.plot(hist['train_loss'],'-b',label='train_loss')\n",
    "plt.plot(hist['val_loss'],'-r',label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "#plt.plot(hist['train_loss'],'-b',label='train_loss')\n",
    "plt.plot(hist['affinity'],'-r',label='affinity')\n",
    "plt.plot(np.subtract(1,hist['balance']),'-y',label='balance')\n",
    "plt.plot(hist['coactivity'],'-g',label='coactivity')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "digitTrace = np.zeros((classCount*clustCount,784))\n",
    "digitTraceCount = np.zeros((classCount*clustCount))\n",
    "digitCount = np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#sess.run(tf.global_variables_initializer())\n",
    "for i in range(1000):\n",
    "    tb = mnist.test.next_batch(1)\n",
    "    digitCount[np.argmax(tb[1])]+=1\n",
    "    testbatch = (tb[0],np.array([y[np.argmax(tb[1][j])] for j in range(len(tb[1]))]))\n",
    "    smMat, acc = sess.run([softmaxMat,accuracy],feed_dict={x: testbatch[0], y_: testbatch[1]})\n",
    "    ypred = softmaxMat.eval({x: testbatch[0], y_: testbatch[1]})\n",
    "    digitTrace[np.argmax(ypred),:] += tb[0].ravel()\n",
    "    digitTraceCount[np.argmax(ypred)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(digitCount)\n",
    "print(digitTraceCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digitTrace = digitTrace/np.max(digitTrace)\n",
    "f,ax=plt.subplots(nrows=classCount, ncols=clustCount, figsize=(1.5*clustCount,1.5*classCount))\n",
    "gs = gridspec.GridSpec(classCount,clustCount)\n",
    "gs.update(wspace=0.025, hspace=0.025)\n",
    "f.subplots_adjust(wspace=0,hspace=0)\n",
    "for i in range(digitTrace.shape[0]):\n",
    "    sp = plt.subplot(gs[i])\n",
    "    sp.set_xticklabels([])\n",
    "    sp.set_yticklabels([])\n",
    "    sp.set_aspect('equal')\n",
    "    sp.grid = False\n",
    "    plt.imshow(np.reshape(digitTrace[i,:],(28,28)),cmap='bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stepCount = len(hist['train_acc'])*100\n",
    "with open('./trainlog.txt','ab') as f:\n",
    "    f.write('lr: %g, batchsize: %i, steps: %i, thresh: %g, c1: %g, c2: %g, c3: %g, c4: %g, test_acc: %g, test_loss: %g\\n'%\n",
    "            (lr,batchSize,stepCount,tresh.eval(), cc1, cc2, cc3, cc4, test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tb = mnist.test.next_batch(10000)\n",
    "#tb[0][:,:] = threshold(tb[0][:,:],threshmin=0.5,newval=0)\n",
    "#tb[0][:,:] = threshold(tb[0][:,:],threshmax=0.49,newval=1)\n",
    "testbatch = (tb[0],np.array([y[np.argmax(tb[1][j])>4] for j in range(len(tb[1]))]))\n",
    "testFeed = {x: testbatch[0], y_: testbatch[1]}\n",
    "ypred = softmaxMat.eval(testFeed)\n",
    "ypred = ypred.reshape((tb[0].shape[0],clustCount*classCount))\n",
    "ypred = tf.argmax(ypred,1).eval()\n",
    "ylookup = [np.argmax(np.sum(tb[1][ypred==i],0)).astype('int32') for i in range(clustCount*classCount)]\n",
    "yconverted = [ylookup[i] for i in ypred]\n",
    "correct_prediction = tf.equal(yconverted, np.argmax(tb[1],1).astype('int32'))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)).eval()\n",
    "print('Clustering Accuracy: %g'%(accuracy))\n",
    "print(ylookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare it to k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tb0 = [tb[0][np.argmax(tb[1],1)<5],tb[1][np.argmax(tb[1],1)<5]]\n",
    "tb1 = [tb[0][np.argmax(tb[1],1)>4],tb[1][np.argmax(tb[1],1)>4]]\n",
    "#<5\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "km0_ypred = kmeans.fit_transform(tb0[0])\n",
    "km0_ypred = np.argmax(km0_ypred,1)\n",
    "km_ylookup = [np.argmax(np.sum(tb0[1][km0_ypred==i],0)).astype('int32') for i in range(clustCount)]\n",
    "km_yconverted = [km_ylookup[i] for i in km0_ypred]\n",
    "km_correct_prediction = tf.equal(km_yconverted, np.argmax(tb0[1],1).astype('int32'))\n",
    "km0_accuracy = tf.reduce_mean(tf.cast(km_correct_prediction, tf.float32)).eval()\n",
    "#>4\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "km1_ypred = kmeans.fit_transform(tb1[0])\n",
    "km1_ypred = np.argmax(km1_ypred,1)\n",
    "km_ylookup = [np.argmax(np.sum(tb1[1][km1_ypred==i],0)).astype('int32') for i in range(clustCount)]\n",
    "km_yconverted = [km_ylookup[i] for i in km1_ypred]\n",
    "km_correct_prediction = tf.equal(km_yconverted, np.argmax(tb1[1],1).astype('int32'))\n",
    "km1_accuracy = tf.reduce_mean(tf.cast(km_correct_prediction, tf.float32)).eval()\n",
    "print('ACOL Accuracy: %g'%(accuracy))\n",
    "print('KMeans Accuracy: %g'%((km0_accuracy+km1_accuracy)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#visualise kmeans\n",
    "digitTrace = np.concatenate([[np.sum(tb0[0][km0_ypred==i,:],axis=0) for i in range(clustCount)],\n",
    "                       [np.sum(tb1[0][km1_ypred==i,:],axis=0) for i in range(clustCount)]])\n",
    "digitTrace = digitTrace/np.max(digitTrace)\n",
    "f,ax=plt.subplots(nrows=classCount, ncols=clustCount, figsize=(1.5*clustCount,1.5*classCount))\n",
    "gs = gridspec.GridSpec(classCount,clustCount)\n",
    "gs.update(wspace=0.025, hspace=0.025)\n",
    "f.subplots_adjust(wspace=0,hspace=0)\n",
    "for i in range(digitTrace.shape[0]):\n",
    "    sp = plt.subplot(gs[i])\n",
    "    sp.set_xticklabels([])\n",
    "    sp.set_yticklabels([])\n",
    "    sp.set_aspect('equal')\n",
    "    sp.grid = False\n",
    "    plt.imshow(np.reshape(digitTrace[i,:],(28,28)),cmap='bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
