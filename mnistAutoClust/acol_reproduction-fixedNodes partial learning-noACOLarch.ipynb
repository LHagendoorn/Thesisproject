{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACOL replication tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#imports and settings:\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "#from jupyterthemes import jtplot\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import threshold\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "from tensorflow.contrib.learn.python.learn.datasets import mnist\n",
    "#jtplot.style()\n",
    "%matplotlib inline\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "TRAIN_IMAGES = 'train-images-idx3-ubyte.gz'\n",
    "TRAIN_LABELS = 'train-labels-idx1-ubyte.gz'\n",
    "TEST_IMAGES = 't10k-images-idx3-ubyte.gz'\n",
    "TEST_LABELS = 't10k-labels-idx1-ubyte.gz'\n",
    "SOURCE_URL = 'https://storage.googleapis.com/cvdf-datasets/mnist/'\n",
    "\n",
    "local_file = base.maybe_download(TRAIN_IMAGES, './MNIST_data',\n",
    "                                   SOURCE_URL + TRAIN_IMAGES)\n",
    "with open(local_file, 'rb') as f:\n",
    "    train_images = mnist.extract_images(f)\n",
    "    \n",
    "local_file = base.maybe_download(TRAIN_LABELS, './MNIST_data',\n",
    "                                   SOURCE_URL + TRAIN_LABELS)\n",
    "with open(local_file, 'rb') as f:\n",
    "    train_labels = mnist.extract_labels(f, one_hot=True)\n",
    "\n",
    "local_file = base.maybe_download(TEST_IMAGES, './MNIST_data',\n",
    "                                   SOURCE_URL + TEST_IMAGES)\n",
    "with open(local_file, 'rb') as f:\n",
    "    test_images = mnist.extract_images(f)\n",
    "\n",
    "local_file = base.maybe_download(TEST_LABELS, './MNIST_data',\n",
    "                                   SOURCE_URL + TEST_LABELS)\n",
    "with open(local_file, 'rb') as f:\n",
    "    test_labels = mnist.extract_labels(f, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clustCount = 5\n",
    "classCount = 10\n",
    "net = 0\n",
    "trainsteps = 50000\n",
    "trainsteps = 30000\n",
    "perc = 0.01\n",
    "validation_size=5000\n",
    "_epochs_completed_train = 0\n",
    "_index_in_epoch_train = 0\n",
    "_epochs_completed_test = 0\n",
    "_index_in_epoch_test = 0\n",
    "_epochs_completed_val = 0\n",
    "_index_in_epoch_val = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_super_labels = np.array([y[np.argmax(train_labels[j])] for j in range(60000)])\n",
    "train_labels_clipped = np.array([train_labels[j] for j in range(int(60000*perc))])\n",
    "train_labels_clipped = np.concatenate([train_labels_clipped,np.array([train_labels[j] for j in range(int(60000*perc),60000)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not 0 <= validation_size <= len(train_images):\n",
    "    raise ValueError(\n",
    "        'Validation size should be between 0 and {}. Received: {}.'\n",
    "        .format(len(train_images), validation_size))\n",
    "\n",
    "validation_images = train_images[:validation_size]\n",
    "validation_labels = train_labels[:validation_size]\n",
    "#validation_super_labels = train_super_labels[:validation_size]\n",
    "validation_labels_clipped = train_labels_clipped[:validation_size]\n",
    "train_images = train_images[validation_size:]\n",
    "train_labels = train_labels[validation_size:]\n",
    "#train_super_labels = train_super_labels[validation_size:]\n",
    "train_labels_clipped = train_labels_clipped[validation_size:]\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0],train_images.shape[1] * train_images.shape[2])\n",
    "train_images = train_images.astype(np.float32)\n",
    "train_images = np.multiply(train_images, 1.0 / 255.0)\n",
    "\n",
    "validation_images = validation_images.reshape(validation_images.shape[0],validation_images.shape[1] * validation_images.shape[2])\n",
    "validation_images = validation_images.astype(np.float32)\n",
    "validation_images = np.multiply(validation_images, 1.0 / 255.0)\n",
    "\n",
    "#    options = dict(dtype=dtypes.float32, reshape=True, seed=None)\n",
    "  \n",
    "#    train = DataSet(train_images, train_labels, **options)\n",
    "#    validation = DataSet(validation_images, validation_labels, **options)\n",
    "#    test = DataSet(test_images, test_labels, **options)\n",
    "  \n",
    "#    mnist = base.Datasets(train=train, validation=validation, test=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def next_batch(batch_size, shuffle, images, labels, ep_compl, ep_ind):\n",
    "    _epochs_completed = ep_compl\n",
    "    _index_in_epoch = ep_ind\n",
    "    \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "    start = _index_in_epoch\n",
    "    _num_examples = images.shape[0]\n",
    "    # Shuffle for the first epoch\n",
    "    if _epochs_completed == 0 and start == 0 and shuffle:\n",
    "      perm0 = np.arange(_num_examples)\n",
    "      np.random.shuffle(perm0)\n",
    "      _images = images[perm0]\n",
    "      _labels = labels[perm0]\n",
    "    # Go to the next epoch\n",
    "    if start + batch_size > _num_examples:\n",
    "      # Finished epoch\n",
    "      _epochs_completed += 1\n",
    "      # Get the rest examples in this epoch\n",
    "      rest_num_examples = _num_examples - start\n",
    "      images_rest_part = _images[start:_num_examples]\n",
    "      labels_rest_part = _labels[start:_num_examples]\n",
    "      # Shuffle the data\n",
    "      if shuffle:\n",
    "        perm = np.arange(_num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        _images = images[perm]\n",
    "        print(_images)\n",
    "        _labels = labels[perm]\n",
    "      # Start next epoch\n",
    "      start = 0\n",
    "      _index_in_epoch = batch_size - rest_num_examples\n",
    "      end = _index_in_epoch\n",
    "      images_new_part = _images[start:end]\n",
    "      labels_new_part = _labels[start:end]\n",
    "      return np.concatenate((images_rest_part, images_new_part), axis=0) , np.concatenate((labels_rest_part, labels_new_part), axis=0)\n",
    "    else:\n",
    "      _index_in_epoch += batch_size\n",
    "      end = _index_in_epoch\n",
    "      return _images[start:end], _labels[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#helper funcs\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "def matrix_softmax(x):\n",
    "    shape = x.get_shape().as_list()\n",
    "    shape[0] = int(-1)\n",
    "    return tf.reshape(tf.nn.softmax(tf.contrib.layers.flatten(x)),shape)\n",
    "\n",
    "def avg_softmax(x):\n",
    "    shape = x.get_shape().as_list()\n",
    "    shape[0] = int(-1)\n",
    "    totalSoft = tf.reshape(tf.nn.softmax(tf.contrib.layers.flatten(x)),shape)\n",
    "    return tf.reduce_sum(totalSoft,2)\n",
    "\n",
    "def max_softmax(x):\n",
    "    shape = x.get_shape().as_list()\n",
    "    shape[0] = int(-1)\n",
    "    totalSoft = tf.reshape(tf.nn.softmax(tf.contrib.layers.flatten(x)),shape)\n",
    "    return tf.reduce_max(totalSoft,2)\n",
    "\n",
    "def initACOL(in_size,clust,clss):\n",
    "    acolLayers = []\n",
    "    for i in range(clss):\n",
    "        acolLayers.append([\n",
    "            weight_variable([in_size, clustCount]),\n",
    "            bias_variable([clustCount])\n",
    "        ])\n",
    "    return acolLayers\n",
    "        \n",
    "def connectACOL(inLayer,acol):\n",
    "    clust = []\n",
    "    for l in range(0,len(acol)):\n",
    "        clust.append(tf.matmul(inLayer, acol[l][0]) + acol[l][1])\n",
    "    return clust\n",
    "        \n",
    "def acol(input,clust_count, class_count):\n",
    "    acolLayers = []\n",
    "    for i in range(class_count):\n",
    "        if isinstance(input, tuple):\n",
    "                input = input[0]\n",
    "\n",
    "        #I don't know what this bit does, but I don't think it'll hurt anything\n",
    "        #Or maybe it does, who knows\n",
    "        input_shape = input.get_shape()\n",
    "        if input_shape.ndims == 4:\n",
    "            dim = 1\n",
    "            for d in input_shape[1:].as_list():\n",
    "                dim *= d\n",
    "        #    feed_in = tf.reshape(tf.transpose(input,[0,3,1,2]), [-1, dim])\n",
    "        else:\n",
    "            feed_in, dim = (input, int(input_shape[-1]))\n",
    "\n",
    "        init_weights = tf.truncated_normal_initializer(0.0, stddev=0.1)#(0.0, stddev=0.01)\n",
    "        init_biases = tf.constant_initializer(1.0)#(0.1)\n",
    "\n",
    "        weights = weight_variable([dim, clust_count])\n",
    "        biases = bias_variable([clust_count])\n",
    "\n",
    "        acoll = tf.nn.xw_plus_b(input,weights,biases)\n",
    "        acolLayers.append(acol)\n",
    "    return acolLayers    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create placeholders (weights&biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if net==0:\n",
    "    #Init model weights & biases\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    x_image = tf.reshape(x, [-1,28,28,1])\n",
    "    #y_ = tf.placeholder(tf.float32, shape=[None, classCount])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None,classCount])\n",
    "    \n",
    "    #conv_layer1\n",
    "    W_conv1 = weight_variable([5,5,1,32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "\n",
    "    #conv_layer2\n",
    "    W_conv2 = weight_variable([5,5,32,64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "\n",
    "    #fc layer 1\n",
    "    W_fc1 = weight_variable([7*7*64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "\n",
    "    #acol = initACOL(1024,clustCount,classCount)\n",
    "\n",
    "    #final fc layer\n",
    "    W_fc2 = weight_variable([1024, classCount])\n",
    "    b_fc2 = bias_variable([classCount])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if net==0:\n",
    "    dropout=0.3\n",
    "    #Define net\n",
    "    #conv 1\n",
    "    l_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    l_pool1 = max_pool_2x2(l_conv1)\n",
    "\n",
    "    #conv 2\n",
    "    l_conv2 = tf.nn.relu(conv2d(l_pool1, W_conv2) + b_conv2)\n",
    "    l_pool2 = max_pool_2x2(l_conv2)\n",
    "\n",
    "    #fc 1\n",
    "    l_pool2_flat = tf.reshape(l_pool2, [-1, 7*7*64])\n",
    "    l_fc1 = tf.nn.relu(tf.matmul(l_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    l_fc1_drop = tf.nn.dropout(l_fc1, tf.constant(dropout))\n",
    "\n",
    "    y_conv = tf.matmul(l_fc1_drop, W_fc2) + b_fc2\n",
    "    \n",
    "    #l_acol = connectACOL(l_fc1_drop,acol)\n",
    "    #l_acol = acol(l_fc1_drop,clustCount,classCount)\n",
    "\n",
    "    #Classification layer\n",
    "    #stackedClusts = tf.stack(l_acol,1)\n",
    "    #softmaxMat = matrix_softmax(stackedClusts)\n",
    "    #smStacked = tf.reduce_max(softmaxMat,2)\n",
    "\n",
    "    #y_conv = smStacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helperfunctions for loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#helper loss funcs\n",
    "def zBar(x):\n",
    "    xshape = x.shape.as_list()\n",
    "    s=[-1,xshape[1]*xshape[2]]\n",
    "    return tf.maximum(tf.reshape(x,s),0)\n",
    "    \n",
    "def bigU(zb):\n",
    "    return tf.matmul(tf.transpose(zb),zb)\n",
    "\n",
    "def selectNonDiag(x):\n",
    "    selection = np.ones(x.shape.as_list()[0],dtype='float32') - np.eye(x.shape.as_list()[0],dtype='float32')\n",
    "    return tf.reduce_sum(tf.multiply(x,selection))\n",
    "\n",
    "def bigV(x):\n",
    "    smallNu=tf.reshape(tf.reduce_sum(x,axis=0),[1,-1])\n",
    "    return tf.multiply(tf.transpose(smallNu),smallNu)\n",
    "\n",
    "def specialNormalise(x):\n",
    "    top = selectNonDiag(x)\n",
    "    bottom = tf.multiply(tf.to_float(x.shape[1]-1),tf.reduce_sum(tf.multiply(x,np.eye(x.shape[1],dtype='float32'))))\n",
    "    return tf.divide(top,bottom)\n",
    "\n",
    "def frobNorm(x):\n",
    "    return tf.sqrt(tf.reduce_sum(tf.square(x)))\n",
    "\n",
    "tresh = tf.constant(0.03)\n",
    "cc0=1.0\n",
    "cc1=1.0\n",
    "cc2=1.0\n",
    "cc3=0.0003\n",
    "cc4=0.000001\n",
    "cc5=1.0\n",
    "c0 = tf.constant(cc0)\n",
    "c1 = tf.constant(cc1)\n",
    "c2 = tf.constant(cc2)\n",
    "c3val = tf.constant(cc3)\n",
    "c3 = lambda affinity: tf.cond(tf.less(affinity,tresh),lambda: c3val,lambda: tf.constant(0.0))\n",
    "c4 =tf.constant(cc4)\n",
    "c5 = tf.constant(cc5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#calculate losses\n",
    "#affinity\n",
    "#bZ = zBar(stackedClusts)#softmaxMat)\n",
    "#bU = bigU(bZ)\n",
    "#coact = selectNonDiag(bU)\n",
    "#affinity = specialNormalise(bU)\n",
    "\n",
    "#balance\n",
    "#bV=bigV(bZ)\n",
    "#balance = specialNormalise(bV)\n",
    "\n",
    "#cluster cross entropy (added if secondary label is set for that input, hard to do with batches?)\n",
    "#clust_cross_entropy = tf.reduce_mean(-tf.reduce_sum(y2_ * tf.log(tf.clip_by_value(softmaxMat,1e-10,1.0)), reduction_indices=[1,2]))\n",
    "\n",
    "#cross entropy\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y_conv,1e-10,1.0)), reduction_indices=[1]))\n",
    "\n",
    "#frob = frobNorm(stackedClusts)#softmaxMat)\n",
    "\n",
    "loss = c0*cross_entropy# + c5*clust_cross_entropy# + c1*affinity + c2*tf.subtract(tf.constant(1.0),balance) + c3(affinity)*coact + c4*frob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "totalSteps = trainsteps\n",
    "stepCount=0\n",
    "batchSize = 128\n",
    "hist = {\n",
    "    'train_acc':[],\n",
    "    'val_acc':[],\n",
    "    'train_loss':[],\n",
    "    'val_loss':[],\n",
    "    'affinity':[],\n",
    "    'balance':[],\n",
    "    'coactivity':[]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "step 0/30000 \n",
      " Train: accuracy: 0.0625, loss: 10.0995 \n",
      " Validation: accuracy: 0.109375 loss: 11.1983\n",
      " cross_entropy: 11.9162\n",
      "step 100/30000 \n",
      " Train: accuracy: 0.078125, loss: 7.21925 \n",
      " Validation: accuracy: 0.09375 loss: 7.93499\n",
      " cross_entropy: 6.89338\n",
      "step 200/30000 \n",
      " Train: accuracy: 0.109375, loss: 4.01159 \n",
      " Validation: accuracy: 0.09375 loss: 4.32057\n",
      " cross_entropy: 3.98\n",
      "step 300/30000 \n",
      " Train: accuracy: 0.148438, loss: 2.55838 \n",
      " Validation: accuracy: 0.109375 loss: 1.99834\n",
      " cross_entropy: 2.16841\n",
      "step 400/30000 \n",
      " Train: accuracy: 0.078125, loss: 1.80845 \n",
      " Validation: accuracy: 0.117188 loss: 0.763439\n",
      " cross_entropy: 1.97987\n",
      "step 500/30000 \n",
      " Train: accuracy: 0.101562, loss: 1.09843 \n",
      " Validation: accuracy: 0.0859375 loss: 1.28141\n",
      " cross_entropy: 1.32811\n",
      "step 600/30000 \n",
      " Train: accuracy: 0.078125, loss: 1.62941 \n",
      " Validation: accuracy: 0.164062 loss: 0.924711\n",
      " cross_entropy: 0.361135\n",
      "step 700/30000 \n",
      " Train: accuracy: 0.0859375, loss: 0.544847 \n",
      " Validation: accuracy: 0.117188 loss: 0.727507\n",
      " cross_entropy: 0.540189\n",
      "step 800/30000 \n",
      " Train: accuracy: 0.117188, loss: 0.742922 \n",
      " Validation: accuracy: 0.164062 loss: 0.179889\n",
      " cross_entropy: 0.359779\n",
      "step 900/30000 \n",
      " Train: accuracy: 0.0859375, loss: 0.552091 \n",
      " Validation: accuracy: 0.117188 loss: 0.539668\n",
      " cross_entropy: 0.359779\n",
      "step 1000/30000 \n",
      " Train: accuracy: 0.15625, loss: 0.193682 \n",
      " Validation: accuracy: 0.0859375 loss: 0.362769\n",
      " cross_entropy: 0.54323\n",
      "step 1100/30000 \n",
      " Train: accuracy: 0.125, loss: 0.914154 \n",
      " Validation: accuracy: 0.148438 loss: 0.192292\n",
      " cross_entropy: 0.721783\n",
      "step 1200/30000 \n",
      " Train: accuracy: 0.0859375, loss: 0.732307 \n",
      " Validation: accuracy: 0.09375 loss: 0.539668\n",
      " cross_entropy: 0.191373\n",
      "step 1300/30000 \n",
      " Train: accuracy: 0.0703125, loss: 0.539668 \n",
      " Validation: accuracy: 0.148438 loss: 0.899687\n",
      " cross_entropy: 0.00484931\n",
      "step 1400/30000 \n",
      " Train: accuracy: 0.117188, loss: 0.359779 \n",
      " Validation: accuracy: 0.109375 loss: 0.543187\n",
      " cross_entropy: 0.179889\n",
      "step 1500/30000 \n",
      " Train: accuracy: 0.101562, loss: 0.226332 \n",
      " Validation: accuracy: 0.140625 loss: 0.183583\n",
      " cross_entropy: 0.359779\n",
      "step 1600/30000 \n",
      " Train: accuracy: 0.132812, loss: 0.00408705 \n",
      " Validation: accuracy: 0.109375 loss: 0.19294\n",
      " cross_entropy: 0.379958\n",
      "step 1700/30000 \n",
      " Train: accuracy: 0.078125, loss: 0.719558 \n",
      " Validation: accuracy: 0.109375 loss: 0.179889\n",
      " cross_entropy: 0.556055\n",
      "step 1800/30000 \n",
      " Train: accuracy: 0.132812, loss: 0.184179 \n",
      " Validation: accuracy: 0.0703125 loss: 0.539668\n",
      " cross_entropy: 0.179889\n",
      "step 1900/30000 \n",
      " Train: accuracy: 0.125, loss: 0.179889 \n",
      " Validation: accuracy: 0.171875 loss: 0.179889\n",
      " cross_entropy: 0.179889\n",
      "step 2000/30000 \n",
      " Train: accuracy: 0.109375, loss: 0.0118516 \n",
      " Validation: accuracy: 0.179688 loss: 0.363015\n",
      " cross_entropy: 0.00257312\n",
      "step 2100/30000 \n",
      " Train: accuracy: 0.09375, loss: 0.190474 \n",
      " Validation: accuracy: 0.117188 loss: 0.179889\n",
      " cross_entropy: 0.359779\n",
      "step 2200/30000 \n",
      " Train: accuracy: 0.15625, loss: 0.181365 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0.00829606\n",
      "step 2300/30000 \n",
      " Train: accuracy: 0.109375, loss: 0.20623 \n",
      " Validation: accuracy: 0.109375 loss: 0.359779\n",
      " cross_entropy: 0.363367\n",
      "step 2400/30000 \n",
      " Train: accuracy: 0.101562, loss: 0.359779 \n",
      " Validation: accuracy: 0.132812 loss: 0.359779\n",
      " cross_entropy: 0.359779\n",
      "step 2500/30000 \n",
      " Train: accuracy: 0.078125, loss: 0.719804 \n",
      " Validation: accuracy: 0.125 loss: 0.189958\n",
      " cross_entropy: 0\n",
      "step 2600/30000 \n",
      " Train: accuracy: 0.117188, loss: 0 \n",
      " Validation: accuracy: 0.0859375 loss: 0.179889\n",
      " cross_entropy: 0.365907\n",
      "step 2700/30000 \n",
      " Train: accuracy: 0.132812, loss: 0.0127957 \n",
      " Validation: accuracy: 0.132812 loss: 0.359779\n",
      " cross_entropy: 0.183304\n",
      "step 2800/30000 \n",
      " Train: accuracy: 0.164062, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0.00196481\n",
      " cross_entropy: 0\n",
      "step 2900/30000 \n",
      " Train: accuracy: 0.078125, loss: 0 \n",
      " Validation: accuracy: 0.0859375 loss: 0\n",
      " cross_entropy: 0.00849737\n",
      "step 3000/30000 \n",
      " Train: accuracy: 0.195312, loss: 0 \n",
      " Validation: accuracy: 0.132812 loss: 0.398325\n",
      " cross_entropy: 0\n",
      "step 3100/30000 \n",
      " Train: accuracy: 0.140625, loss: 0.179889 \n",
      " Validation: accuracy: 0.171875 loss: 0\n",
      " cross_entropy: 0.37055\n",
      "step 3200/30000 \n",
      " Train: accuracy: 0.101562, loss: 0.179889 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0.00630338\n",
      "step 3300/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.171875 loss: 0.00408745\n",
      " cross_entropy: 0.180558\n",
      "step 3400/30000 \n",
      " Train: accuracy: 0.09375, loss: 0 \n",
      " Validation: accuracy: 0.117188 loss: 0.180611\n",
      " cross_entropy: 0\n",
      "step 3500/30000 \n",
      " Train: accuracy: 0.0703125, loss: 0.188423 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0.179889\n",
      "step 3600/30000 \n",
      " Train: accuracy: 0.09375, loss: 0.179889 \n",
      " Validation: accuracy: 0.078125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 3700/30000 \n",
      " Train: accuracy: 0.125, loss: 0 \n",
      " Validation: accuracy: 0.109375 loss: 0\n",
      " cross_entropy: 0\n",
      "step 3800/30000 \n",
      " Train: accuracy: 0.140625, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 3900/30000 \n",
      " Train: accuracy: 0.179688, loss: 0 \n",
      " Validation: accuracy: 0.109375 loss: 0.000655164\n",
      " cross_entropy: 0.00509936\n",
      "step 4000/30000 \n",
      " Train: accuracy: 0.125, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0.179889\n",
      " cross_entropy: 0.179889\n",
      "step 4100/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.140625 loss: 0.179889\n",
      " cross_entropy: 0\n",
      "step 4200/30000 \n",
      " Train: accuracy: 0.109375, loss: 0 \n",
      " Validation: accuracy: 0.117188 loss: 0\n",
      " cross_entropy: 0\n",
      "step 4300/30000 \n",
      " Train: accuracy: 0.117188, loss: 0 \n",
      " Validation: accuracy: 0.101562 loss: 0\n",
      " cross_entropy: 0\n",
      "step 4400/30000 \n",
      " Train: accuracy: 0.109375, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0.179889\n",
      "step 4500/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.171875 loss: 0.180987\n",
      " cross_entropy: 0\n",
      "step 4600/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.0546875 loss: 0.179889\n",
      " cross_entropy: 0\n",
      "step 4700/30000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.101562 loss: 0\n",
      " cross_entropy: 0\n",
      "step 4800/30000 \n",
      " Train: accuracy: 0.179688, loss: 0 \n",
      " Validation: accuracy: 0.117188 loss: 0.179889\n",
      " cross_entropy: 0\n",
      "step 4900/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0.00158077\n",
      "step 5000/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.179688 loss: 0\n",
      " cross_entropy: 0\n",
      "step 5100/30000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.171875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 5200/30000 \n",
      " Train: accuracy: 0.140625, loss: 0.00268354 \n",
      " Validation: accuracy: 0.148438 loss: 0.179889\n",
      " cross_entropy: 0\n",
      "step 5300/30000 \n",
      " Train: accuracy: 0.164062, loss: 0.00484768 \n",
      " Validation: accuracy: 0.125 loss: 0\n",
      " cross_entropy: 0.179889\n",
      "step 5400/30000 \n",
      " Train: accuracy: 0.117188, loss: 0 \n",
      " Validation: accuracy: 0.117188 loss: 0\n",
      " cross_entropy: 0\n",
      "step 5500/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.132812 loss: 0\n",
      " cross_entropy: 0.00322385\n",
      "step 5600/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.132812 loss: 0\n",
      " cross_entropy: 0\n",
      "step 5700/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.140625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 5800/30000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.140625 loss: 0\n",
      " cross_entropy: 1.56385e-06\n",
      "step 5900/30000 \n",
      " Train: accuracy: 0.109375, loss: 0 \n",
      " Validation: accuracy: 0.125 loss: 0.00145959\n",
      " cross_entropy: 0\n",
      "step 6000/30000 \n",
      " Train: accuracy: 0.078125, loss: 0 \n",
      " Validation: accuracy: 0.117188 loss: 0\n",
      " cross_entropy: 0\n",
      "step 6100/30000 \n",
      " Train: accuracy: 0.117188, loss: 0 \n",
      " Validation: accuracy: 0.109375 loss: 0\n",
      " cross_entropy: 0.179889\n",
      "step 6200/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.132812 loss: 0\n",
      " cross_entropy: 0.179889\n",
      "step 6300/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 6400/30000 \n",
      " Train: accuracy: 0.125, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0\n",
      "step 6500/30000 \n",
      " Train: accuracy: 0.171875, loss: 0 \n",
      " Validation: accuracy: 0.140625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 6600/30000 \n",
      " Train: accuracy: 0.132812, loss: 0.179889 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0.179889\n",
      "step 6700/30000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.195312 loss: 0\n",
      " cross_entropy: 0\n",
      "step 6800/30000 \n",
      " Train: accuracy: 0.117188, loss: 0 \n",
      " Validation: accuracy: 0.132812 loss: 0\n",
      " cross_entropy: 0\n",
      "step 6900/30000 \n",
      " Train: accuracy: 0.21875, loss: 0.179889 \n",
      " Validation: accuracy: 0.140625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 7000/30000 \n",
      " Train: accuracy: 0.109375, loss: 0 \n",
      " Validation: accuracy: 0.203125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 7100/30000 \n",
      " Train: accuracy: 0.117188, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0\n",
      "step 7200/30000 \n",
      " Train: accuracy: 0.140625, loss: 0 \n",
      " Validation: accuracy: 0.125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 7300/30000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0\n",
      "step 7400/30000 \n",
      " Train: accuracy: 0.117188, loss: 0 \n",
      " Validation: accuracy: 0.171875 loss: 0.000102408\n",
      " cross_entropy: 0\n",
      "step 7500/30000 \n",
      " Train: accuracy: 0.101562, loss: 0 \n",
      " Validation: accuracy: 0.132812 loss: 0\n",
      " cross_entropy: 0.179889\n",
      "step 7600/30000 \n",
      " Train: accuracy: 0.140625, loss: 0 \n",
      " Validation: accuracy: 0.132812 loss: 0\n",
      " cross_entropy: 0\n",
      "step 7700/30000 \n",
      " Train: accuracy: 0.101562, loss: 0 \n",
      " Validation: accuracy: 0.101562 loss: 0\n",
      " cross_entropy: 0\n",
      "step 7800/30000 \n",
      " Train: accuracy: 0.140625, loss: 0 \n",
      " Validation: accuracy: 0.0625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 7900/30000 \n",
      " Train: accuracy: 0.0859375, loss: 0 \n",
      " Validation: accuracy: 0.117188 loss: 0\n",
      " cross_entropy: 0\n",
      "step 8000/30000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.179688 loss: 0.179889\n",
      " cross_entropy: 0\n",
      "step 8100/30000 \n",
      " Train: accuracy: 0.117188, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 8200/30000 \n",
      " Train: accuracy: 0.117188, loss: 0 \n",
      " Validation: accuracy: 0.1875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 8300/30000 \n",
      " Train: accuracy: 0.179688, loss: 0 \n",
      " Validation: accuracy: 0.203125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 8400/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 8500/30000 \n",
      " Train: accuracy: 0.164062, loss: 0 \n",
      " Validation: accuracy: 0.125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 8600/30000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0\n",
      "step 8700/30000 \n",
      " Train: accuracy: 0.140625, loss: 0 \n",
      " Validation: accuracy: 0.21875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 8800/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.117188 loss: 0\n",
      " cross_entropy: 0\n",
      "step 8900/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.125 loss: 0\n",
      " cross_entropy: 0.00954264\n",
      "step 9000/30000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 9100/30000 \n",
      " Train: accuracy: 0.125, loss: 0 \n",
      " Validation: accuracy: 0.101562 loss: 0\n",
      " cross_entropy: 0\n",
      "step 9200/30000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.210938 loss: 0\n",
      " cross_entropy: 0\n",
      "step 9300/30000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 9400/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.132812 loss: 0\n",
      " cross_entropy: 0\n",
      "step 9500/30000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.101562 loss: 0\n",
      " cross_entropy: 0\n",
      "step 9600/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.179688 loss: 0\n",
      " cross_entropy: 0\n",
      "step 9700/30000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.203125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 9800/30000 \n",
      " Train: accuracy: 0.109375, loss: 0 \n",
      " Validation: accuracy: 0.171875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 9900/30000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.140625 loss: 0\n",
      " cross_entropy: 0.00222257\n",
      "step 10000/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.132812 loss: 0\n",
      " cross_entropy: 0\n",
      "step 10100/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.117188 loss: 0\n",
      " cross_entropy: 0\n",
      "step 10200/30000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.140625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 10300/30000 \n",
      " Train: accuracy: 0.140625, loss: 0 \n",
      " Validation: accuracy: 0.117188 loss: 0\n",
      " cross_entropy: 0\n",
      "step 10400/30000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0\n",
      "step 10500/30000 \n",
      " Train: accuracy: 0.210938, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 10600/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 10700/30000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0.179889\n",
      "step 10800/30000 \n",
      " Train: accuracy: 0.140625, loss: 0 \n",
      " Validation: accuracy: 0.132812 loss: 0\n",
      " cross_entropy: 0\n",
      "step 10900/30000 \n",
      " Train: accuracy: 0.101562, loss: 0 \n",
      " Validation: accuracy: 0.140625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 11000/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 11100/30000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.132812 loss: 0\n",
      " cross_entropy: 0\n",
      "step 11200/30000 \n",
      " Train: accuracy: 0.101562, loss: 0 \n",
      " Validation: accuracy: 0.171875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 11300/30000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0.179889\n",
      " cross_entropy: 0\n",
      "step 11400/30000 \n",
      " Train: accuracy: 0.140625, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0\n",
      "step 11500/30000 \n",
      " Train: accuracy: 0.117188, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 11600/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.101562 loss: 0\n",
      " cross_entropy: 0\n",
      "step 11700/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.132812 loss: 0\n",
      " cross_entropy: 0\n",
      "step 11800/30000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.109375 loss: 0\n",
      " cross_entropy: 0\n",
      "step 11900/30000 \n",
      " Train: accuracy: 0.125, loss: 0 \n",
      " Validation: accuracy: 0.203125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 12000/30000 \n",
      " Train: accuracy: 0.171875, loss: 0 \n",
      " Validation: accuracy: 0.132812 loss: 0\n",
      " cross_entropy: 0\n",
      "step 12100/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 12200/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.1875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 12300/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.203125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 12400/30000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0\n",
      "step 12500/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.203125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 12600/30000 \n",
      " Train: accuracy: 0.164062, loss: 0 \n",
      " Validation: accuracy: 0.140625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 12700/30000 \n",
      " Train: accuracy: 0.140625, loss: 0 \n",
      " Validation: accuracy: 0.210938 loss: 0\n",
      " cross_entropy: 0\n",
      "step 12800/30000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.171875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 12900/30000 \n",
      " Train: accuracy: 0.179688, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 13000/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 13100/30000 \n",
      " Train: accuracy: 0.101562, loss: 0 \n",
      " Validation: accuracy: 0.109375 loss: 0\n",
      " cross_entropy: 0\n",
      "step 13200/30000 \n",
      " Train: accuracy: 0.09375, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 13300/30000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.195312 loss: 0\n",
      " cross_entropy: 0\n",
      "step 13400/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0\n",
      "step 13500/30000 \n",
      " Train: accuracy: 0.15625, loss: 0.179889 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 13600/30000 \n",
      " Train: accuracy: 0.164062, loss: 0 \n",
      " Validation: accuracy: 0.109375 loss: 0\n",
      " cross_entropy: 0\n",
      "step 13700/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.195312 loss: 0\n",
      " cross_entropy: 0\n",
      "step 13800/30000 \n",
      " Train: accuracy: 0.210938, loss: 0 \n",
      " Validation: accuracy: 0.195312 loss: 0\n",
      " cross_entropy: 0\n",
      "step 13900/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 14000/30000 \n",
      " Train: accuracy: 0.171875, loss: 0 \n",
      " Validation: accuracy: 0.125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 14100/30000 \n",
      " Train: accuracy: 0.109375, loss: 0 \n",
      " Validation: accuracy: 0.1875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 14200/30000 \n",
      " Train: accuracy: 0.171875, loss: 0 \n",
      " Validation: accuracy: 0.195312 loss: 0\n",
      " cross_entropy: 0\n",
      "step 14300/30000 \n",
      " Train: accuracy: 0.125, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0\n",
      "step 14400/30000 \n",
      " Train: accuracy: 0.195312, loss: 0 \n",
      " Validation: accuracy: 0.234375 loss: 0\n",
      " cross_entropy: 0\n",
      "step 14500/30000 \n",
      " Train: accuracy: 0.171875, loss: 0 \n",
      " Validation: accuracy: 0.132812 loss: 0\n",
      " cross_entropy: 0\n",
      "step 14600/30000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.109375 loss: 0\n",
      " cross_entropy: 0\n",
      "step 14700/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.210938 loss: 0\n",
      " cross_entropy: 0\n",
      "step 14800/30000 \n",
      " Train: accuracy: 0.109375, loss: 0 \n",
      " Validation: accuracy: 0.179688 loss: 0\n",
      " cross_entropy: 0\n",
      "step 14900/30000 \n",
      " Train: accuracy: 0.109375, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 15000/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.09375 loss: 0\n",
      " cross_entropy: 0\n",
      "step 15100/30000 \n",
      " Train: accuracy: 0.09375, loss: 0 \n",
      " Validation: accuracy: 0.179688 loss: 0\n",
      " cross_entropy: 0\n",
      "step 15200/30000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.1875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 15300/30000 \n",
      " Train: accuracy: 0.171875, loss: 0 \n",
      " Validation: accuracy: 0.132812 loss: 0\n",
      " cross_entropy: 0\n",
      "step 15400/30000 \n",
      " Train: accuracy: 0.109375, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 15500/30000 \n",
      " Train: accuracy: 0.117188, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 15600/30000 \n",
      " Train: accuracy: 0.179688, loss: 0 \n",
      " Validation: accuracy: 0.117188 loss: 0\n",
      " cross_entropy: 0\n",
      "step 15700/30000 \n",
      " Train: accuracy: 0.125, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 15800/30000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.1875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 15900/30000 \n",
      " Train: accuracy: 0.171875, loss: 0 \n",
      " Validation: accuracy: 0.1875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 16000/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.101562 loss: 0\n",
      " cross_entropy: 0\n",
      "step 16100/30000 \n",
      " Train: accuracy: 0.140625, loss: 0 \n",
      " Validation: accuracy: 0.195312 loss: 0\n",
      " cross_entropy: 0\n",
      "step 16200/30000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.203125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 16300/30000 \n",
      " Train: accuracy: 0.164062, loss: 0 \n",
      " Validation: accuracy: 0.117188 loss: 0\n",
      " cross_entropy: 0\n",
      "step 16400/30000 \n",
      " Train: accuracy: 0.179688, loss: 0 \n",
      " Validation: accuracy: 0.1875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 16500/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.101562 loss: 0\n",
      " cross_entropy: 0\n",
      "step 16600/30000 \n",
      " Train: accuracy: 0.109375, loss: 0 \n",
      " Validation: accuracy: 0.171875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 16700/30000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 16800/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.21875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 16900/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.179688 loss: 0\n",
      " cross_entropy: 0\n",
      "step 17000/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 17100/30000 \n",
      " Train: accuracy: 0.179688, loss: 0 \n",
      " Validation: accuracy: 0.179688 loss: 0\n",
      " cross_entropy: 0\n",
      "step 17200/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.226562 loss: 0\n",
      " cross_entropy: 0\n",
      "step 17300/30000 \n",
      " Train: accuracy: 0.164062, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0\n",
      "step 17400/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 17500/30000 \n",
      " Train: accuracy: 0.101562, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 17600/30000 \n",
      " Train: accuracy: 0.164062, loss: 0 \n",
      " Validation: accuracy: 0.117188 loss: 0\n",
      " cross_entropy: 0\n",
      "step 17700/30000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.171875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 17800/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.140625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 17900/30000 \n",
      " Train: accuracy: 0.171875, loss: 0 \n",
      " Validation: accuracy: 0.101562 loss: 0\n",
      " cross_entropy: 0\n",
      "step 18000/30000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.179688 loss: 0\n",
      " cross_entropy: 0\n",
      "step 18100/30000 \n",
      " Train: accuracy: 0.109375, loss: 0 \n",
      " Validation: accuracy: 0.101562 loss: 0\n",
      " cross_entropy: 0\n",
      "step 18200/30000 \n",
      " Train: accuracy: 0.101562, loss: 0 \n",
      " Validation: accuracy: 0.195312 loss: 0\n",
      " cross_entropy: 0\n",
      "step 18300/30000 \n",
      " Train: accuracy: 0.140625, loss: 0 \n",
      " Validation: accuracy: 0.203125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 18400/30000 \n",
      " Train: accuracy: 0.125, loss: 0 \n",
      " Validation: accuracy: 0.132812 loss: 0\n",
      " cross_entropy: 0\n",
      "step 18500/30000 \n",
      " Train: accuracy: 0.164062, loss: 0 \n",
      " Validation: accuracy: 0.117188 loss: 0\n",
      " cross_entropy: 0\n",
      "step 18600/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 18700/30000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 18800/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.109375 loss: 0\n",
      " cross_entropy: 0\n",
      "step 18900/30000 \n",
      " Train: accuracy: 0.179688, loss: 0 \n",
      " Validation: accuracy: 0.1875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 19000/30000 \n",
      " Train: accuracy: 0.179688, loss: 0 \n",
      " Validation: accuracy: 0.0859375 loss: 0\n",
      " cross_entropy: 0\n",
      "step 19100/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0\n",
      "step 19200/30000 \n",
      " Train: accuracy: 0.203125, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 19300/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.117188 loss: 0\n",
      " cross_entropy: 0\n",
      "step 19400/30000 \n",
      " Train: accuracy: 0.195312, loss: 0 \n",
      " Validation: accuracy: 0.101562 loss: 0\n",
      " cross_entropy: 0\n",
      "step 19500/30000 \n",
      " Train: accuracy: 0.179688, loss: 0 \n",
      " Validation: accuracy: 0.140625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 19600/30000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0\n",
      "step 19700/30000 \n",
      " Train: accuracy: 0.109375, loss: 0 \n",
      " Validation: accuracy: 0.117188 loss: 0\n",
      " cross_entropy: 0\n",
      "step 19800/30000 \n",
      " Train: accuracy: 0.117188, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 19900/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.171875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 20000/30000 \n",
      " Train: accuracy: 0.195312, loss: 0 \n",
      " Validation: accuracy: 0.171875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 20100/30000 \n",
      " Train: accuracy: 0.164062, loss: 0 \n",
      " Validation: accuracy: 0.171875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 20200/30000 \n",
      " Train: accuracy: 0.164062, loss: 0 \n",
      " Validation: accuracy: 0.125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 20300/30000 \n",
      " Train: accuracy: 0.179688, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0\n",
      "step 20400/30000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0\n",
      "step 20500/30000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 20600/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 20700/30000 \n",
      " Train: accuracy: 0.164062, loss: 0 \n",
      " Validation: accuracy: 0.171875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 20800/30000 \n",
      " Train: accuracy: 0.179688, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 20900/30000 \n",
      " Train: accuracy: 0.140625, loss: 0 \n",
      " Validation: accuracy: 0.1875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 21000/30000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 21100/30000 \n",
      " Train: accuracy: 0.171875, loss: 0 \n",
      " Validation: accuracy: 0.179688 loss: 0\n",
      " cross_entropy: 0\n",
      "step 21200/30000 \n",
      " Train: accuracy: 0.195312, loss: 0 \n",
      " Validation: accuracy: 0.179688 loss: 0\n",
      " cross_entropy: 0\n",
      "step 21300/30000 \n",
      " Train: accuracy: 0.171875, loss: 0 \n",
      " Validation: accuracy: 0.1875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 21400/30000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.140625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 21500/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 21600/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.0859375 loss: 0\n",
      " cross_entropy: 0\n",
      "step 21700/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 21800/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.171875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 21900/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 22000/30000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 22100/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 22200/30000 \n",
      " Train: accuracy: 0.101562, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 22300/30000 \n",
      " Train: accuracy: 0.195312, loss: 0 \n",
      " Validation: accuracy: 0.179688 loss: 0\n",
      " cross_entropy: 0\n",
      "step 22400/30000 \n",
      " Train: accuracy: 0.164062, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0\n",
      "step 22500/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.140625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 22600/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 22700/30000 \n",
      " Train: accuracy: 0.203125, loss: 0 \n",
      " Validation: accuracy: 0.125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 22800/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0\n",
      "step 22900/30000 \n",
      " Train: accuracy: 0.164062, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 23000/30000 \n",
      " Train: accuracy: 0.171875, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 23100/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0\n",
      "step 23200/30000 \n",
      " Train: accuracy: 0.164062, loss: 0 \n",
      " Validation: accuracy: 0.101562 loss: 0\n",
      " cross_entropy: 0\n",
      "step 23300/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.117188 loss: 0\n",
      " cross_entropy: 0\n",
      "step 23400/30000 \n",
      " Train: accuracy: 0.195312, loss: 0 \n",
      " Validation: accuracy: 0.203125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 23500/30000 \n",
      " Train: accuracy: 0.164062, loss: 0 \n",
      " Validation: accuracy: 0.117188 loss: 0\n",
      " cross_entropy: 0\n",
      "step 23600/30000 \n",
      " Train: accuracy: 0.117188, loss: 0 \n",
      " Validation: accuracy: 0.179688 loss: 0\n",
      " cross_entropy: 0\n",
      "step 23700/30000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 23800/30000 \n",
      " Train: accuracy: 0.109375, loss: 0 \n",
      " Validation: accuracy: 0.179688 loss: 0\n",
      " cross_entropy: 0\n",
      "step 23900/30000 \n",
      " Train: accuracy: 0.140625, loss: 0 \n",
      " Validation: accuracy: 0.125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 24000/30000 \n",
      " Train: accuracy: 0.179688, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 24100/30000 \n",
      " Train: accuracy: 0.195312, loss: 0 \n",
      " Validation: accuracy: 0.125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 24200/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 24300/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.25 loss: 0\n",
      " cross_entropy: 0\n",
      "step 24400/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 24500/30000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.140625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 24600/30000 \n",
      " Train: accuracy: 0.171875, loss: 0 \n",
      " Validation: accuracy: 0.140625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 24700/30000 \n",
      " Train: accuracy: 0.125, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 24800/30000 \n",
      " Train: accuracy: 0.0859375, loss: 0 \n",
      " Validation: accuracy: 0.125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 24900/30000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.195312 loss: 0\n",
      " cross_entropy: 0\n",
      "step 25000/30000 \n",
      " Train: accuracy: 0.117188, loss: 0 \n",
      " Validation: accuracy: 0.109375 loss: 0\n",
      " cross_entropy: 0\n",
      "step 25100/30000 \n",
      " Train: accuracy: 0.171875, loss: 0 \n",
      " Validation: accuracy: 0.210938 loss: 0\n",
      " cross_entropy: 0\n",
      "step 25200/30000 \n",
      " Train: accuracy: 0.117188, loss: 0.00076574 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 25300/30000 \n",
      " Train: accuracy: 0.210938, loss: 0 \n",
      " Validation: accuracy: 0.171875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 25400/30000 \n",
      " Train: accuracy: 0.164062, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 25500/30000 \n",
      " Train: accuracy: 0.140625, loss: 0 \n",
      " Validation: accuracy: 0.140625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 25600/30000 \n",
      " Train: accuracy: 0.242188, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0\n",
      "step 25700/30000 \n",
      " Train: accuracy: 0.117188, loss: 0 \n",
      " Validation: accuracy: 0.09375 loss: 0\n",
      " cross_entropy: 0\n",
      "step 25800/30000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.140625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 25900/30000 \n",
      " Train: accuracy: 0.179688, loss: 0 \n",
      " Validation: accuracy: 0.1875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 26000/30000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 26100/30000 \n",
      " Train: accuracy: 0.179688, loss: 0 \n",
      " Validation: accuracy: 0.203125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 26200/30000 \n",
      " Train: accuracy: 0.21875, loss: 0 \n",
      " Validation: accuracy: 0.171875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 26300/30000 \n",
      " Train: accuracy: 0.164062, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 26400/30000 \n",
      " Train: accuracy: 0.195312, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 26500/30000 \n",
      " Train: accuracy: 0.203125, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 26600/30000 \n",
      " Train: accuracy: 0.164062, loss: 0 \n",
      " Validation: accuracy: 0.140625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 26700/30000 \n",
      " Train: accuracy: 0.203125, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 26800/30000 \n",
      " Train: accuracy: 0.117188, loss: 0 \n",
      " Validation: accuracy: 0.195312 loss: 0\n",
      " cross_entropy: 0\n",
      "step 26900/30000 \n",
      " Train: accuracy: 0.171875, loss: 0 \n",
      " Validation: accuracy: 0.1875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 27000/30000 \n",
      " Train: accuracy: 0.21875, loss: 0 \n",
      " Validation: accuracy: 0.195312 loss: 0\n",
      " cross_entropy: 0\n",
      "step 27100/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 27200/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.117188 loss: 0\n",
      " cross_entropy: 0\n",
      "step 27300/30000 \n",
      " Train: accuracy: 0.125, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 27400/30000 \n",
      " Train: accuracy: 0.140625, loss: 0 \n",
      " Validation: accuracy: 0.109375 loss: 0\n",
      " cross_entropy: 0\n",
      "step 27500/30000 \n",
      " Train: accuracy: 0.179688, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 27600/30000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.242188 loss: 0\n",
      " cross_entropy: 0\n",
      "step 27700/30000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.1875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 27800/30000 \n",
      " Train: accuracy: 0.117188, loss: 0 \n",
      " Validation: accuracy: 0.117188 loss: 0\n",
      " cross_entropy: 0\n",
      "step 27900/30000 \n",
      " Train: accuracy: 0.132812, loss: 0 \n",
      " Validation: accuracy: 0.1875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 28000/30000 \n",
      " Train: accuracy: 0.125, loss: 0 \n",
      " Validation: accuracy: 0.140625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 28100/30000 \n",
      " Train: accuracy: 0.140625, loss: 0 \n",
      " Validation: accuracy: 0.171875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 28200/30000 \n",
      " Train: accuracy: 0.109375, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 28300/30000 \n",
      " Train: accuracy: 0.148438, loss: 0 \n",
      " Validation: accuracy: 0.1875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 28400/30000 \n",
      " Train: accuracy: 0.179688, loss: 0 \n",
      " Validation: accuracy: 0.109375 loss: 0\n",
      " cross_entropy: 0\n",
      "step 28500/30000 \n",
      " Train: accuracy: 0.210938, loss: 0 \n",
      " Validation: accuracy: 0.117188 loss: 0\n",
      " cross_entropy: 0\n",
      "step 28600/30000 \n",
      " Train: accuracy: 0.117188, loss: 0 \n",
      " Validation: accuracy: 0.195312 loss: 0\n",
      " cross_entropy: 0\n",
      "step 28700/30000 \n",
      " Train: accuracy: 0.195312, loss: 0 \n",
      " Validation: accuracy: 0.171875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 28800/30000 \n",
      " Train: accuracy: 0.140625, loss: 0 \n",
      " Validation: accuracy: 0.109375 loss: 0\n",
      " cross_entropy: 0\n",
      "step 28900/30000 \n",
      " Train: accuracy: 0.15625, loss: 0 \n",
      " Validation: accuracy: 0.0859375 loss: 0\n",
      " cross_entropy: 0\n",
      "step 29000/30000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.15625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 29100/30000 \n",
      " Train: accuracy: 0.164062, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 29200/30000 \n",
      " Train: accuracy: 0.09375, loss: 0 \n",
      " Validation: accuracy: 0.148438 loss: 0\n",
      " cross_entropy: 0\n",
      "step 29300/30000 \n",
      " Train: accuracy: 0.171875, loss: 0 \n",
      " Validation: accuracy: 0.140625 loss: 0\n",
      " cross_entropy: 0\n",
      "step 29400/30000 \n",
      " Train: accuracy: 0.171875, loss: 0 \n",
      " Validation: accuracy: 0.132812 loss: 0\n",
      " cross_entropy: 0\n",
      "step 29500/30000 \n",
      " Train: accuracy: 0.125, loss: 0 \n",
      " Validation: accuracy: 0.125 loss: 0\n",
      " cross_entropy: 0\n",
      "step 29600/30000 \n",
      " Train: accuracy: 0.171875, loss: 0 \n",
      " Validation: accuracy: 0.210938 loss: 0\n",
      " cross_entropy: 0\n",
      "step 29700/30000 \n",
      " Train: accuracy: 0.1875, loss: 0 \n",
      " Validation: accuracy: 0.164062 loss: 0\n",
      " cross_entropy: 0\n",
      "step 29800/30000 \n",
      " Train: accuracy: 0.203125, loss: 0 \n",
      " Validation: accuracy: 0.1875 loss: 0\n",
      " cross_entropy: 0\n",
      "step 29900/30000 \n",
      " Train: accuracy: 0.125, loss: 0 \n",
      " Validation: accuracy: 0.109375 loss: 0\n",
      " cross_entropy: 0\n"
     ]
    }
   ],
   "source": [
    "#totalSteps = int(totalSteps*perc)\n",
    "print totalSteps\n",
    "for i in range(totalSteps):\n",
    "    #if i > totalSteps*perc:\n",
    "    #   convy2 = emptyy2\n",
    "        \n",
    "    trainbatch = next_batch(batchSize,True,train_images, train_labels,_epochs_completed_train,_index_in_epoch_train)\n",
    "    trainbatch = (trainbatch[0],trainbatch[1])\n",
    "    valbatch = next_batch(batchSize,True,validation_images, validation_labels,_epochs_completed_val,_index_in_epoch_val)\n",
    "    valbatch = (valbatch[0],valbatch[1])\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        train_loss,train_acc = sess.run([loss, accuracy],feed_dict={x: trainbatch[0], y_: trainbatch[1]})\n",
    "        val_loss, val_acc = sess.run([loss, accuracy],feed_dict={x: valbatch[0], y_: valbatch[1]})\n",
    "        hist['train_acc'].append(train_acc)\n",
    "        hist['val_acc'].append(val_acc)\n",
    "        hist['train_loss'].append(train_loss)\n",
    "        hist['val_loss'].append(val_loss)\n",
    "        print(\"step %d/%d \\n Train: accuracy: %g, loss: %g \\n Validation: accuracy: %g loss: %g\"%(i,totalSteps, train_acc, train_loss, val_acc, val_loss))\n",
    "        #hist['affinity'].append(affinity.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1]}))\n",
    "        #hist['balance'].append(balance.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1]}))\n",
    "        #hist['coactivity'].append(coact.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1]}))\n",
    "        entr = cross_entropy.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1]})\n",
    "        #entr2 = clust_cross_entropy.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1]})\n",
    "        #frb = frob.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1]})\n",
    "        \n",
    "        #print bV.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1]})\n",
    "        \n",
    "        print(\" cross_entropy: %g\"%(cc0*entr))\n",
    "    feed_dict={x: trainbatch[0], y_: trainbatch[1]}\n",
    "    _ = sess.run([train_step],feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing & visualising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_epochs_completed_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-38b40257e46a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtestSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtestbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestSize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_epochs_completed_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_index_in_epoch_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtestbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtestbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test: accuracy: %g, loss: %g'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_epochs_completed_test' is not defined"
     ]
    }
   ],
   "source": [
    "testSize = 1000\n",
    "testbatch = next_batch(testSize,True,test_images, test_labels, _epochs_completed_test,_index_in_epoch_test)\n",
    "test_loss,test_acc = sess.run([loss,accuracy],{x: testbatch[0], y_: testbatch[1]})\n",
    "print('Test: accuracy: %g, loss: %g'%(test_acc,test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plot accuracy and loss\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(211)\n",
    "plt.plot(hist['train_acc'],'-b',label='train_acc')\n",
    "plt.plot(hist['val_acc'],'-r',label='val_acc')\n",
    "plt.legend()\n",
    "plt.subplot(212)\n",
    "plt.plot(hist['train_loss'],'-b',label='train_loss')\n",
    "plt.plot(hist['val_loss'],'-r',label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "#plt.plot(hist['train_loss'],'-b',label='train_loss')\n",
    "plt.plot(hist['affinity'],'-r',label='affinity')\n",
    "plt.plot(np.subtract(1,hist['balance']),'-y',label='balance')\n",
    "plt.plot(hist['coactivity'],'-g',label='coactivity')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "digitTrace = np.zeros((classCount*clustCount,784))\n",
    "digitTraceCount = np.zeros((classCount*clustCount))\n",
    "digitCount = np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#sess.run(tf.global_variables_initializer())\n",
    "for i in range(1000):\n",
    "    tb = mnist.test.next_batch(1)\n",
    "    digitCount[np.argmax(tb[1])]+=1\n",
    "    testbatch = (tb[0],np.array([y[np.argmax(tb[1][j])] for j in range(len(tb[1]))]))\n",
    "    smMat, acc = sess.run([softmaxMat,accuracy],feed_dict={x: testbatch[0], y_: testbatch[1]})\n",
    "    ypred = softmaxMat.eval({x: testbatch[0], y_: testbatch[1]})\n",
    "    digitTrace[np.argmax(ypred),:] += tb[0].ravel()\n",
    "    digitTraceCount[np.argmax(ypred)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(digitCount)\n",
    "print(digitTraceCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digitTrace = digitTrace/np.max(digitTrace)\n",
    "f,ax=plt.subplots(nrows=classCount, ncols=clustCount, figsize=(1.5*clustCount,1.5*classCount))\n",
    "gs = gridspec.GridSpec(classCount,clustCount)\n",
    "gs.update(wspace=0.025, hspace=0.025)\n",
    "f.subplots_adjust(wspace=0,hspace=0)\n",
    "for i in range(digitTrace.shape[0]):\n",
    "    sp = plt.subplot(gs[i])\n",
    "    sp.set_xticklabels([])\n",
    "    sp.set_yticklabels([])\n",
    "    sp.set_aspect('equal')\n",
    "    sp.grid = False\n",
    "    plt.imshow(np.reshape(digitTrace[i,:],(28,28)),cmap='bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stepCount = len(hist['train_acc'])*100\n",
    "with open('./trainlog.txt','ab') as f:\n",
    "    f.write('lr: %g, batchsize: %i, steps: %i, thresh: %g, c1: %g, c2: %g, c3: %g, c4: %g, test_acc: %g, test_loss: %g\\n'%\n",
    "            (lr,batchSize,stepCount,tresh.eval(), cc1, cc2, cc3, cc4, test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tb = mnist.test.next_batch(10000)\n",
    "#tb[0][:,:] = threshold(tb[0][:,:],threshmin=0.5,newval=0)\n",
    "#tb[0][:,:] = threshold(tb[0][:,:],threshmax=0.49,newval=1)\n",
    "testbatch = (tb[0],np.array([y[np.argmax(tb[1][j])>4] for j in range(len(tb[1]))]))\n",
    "testFeed = {x: testbatch[0], y_: testbatch[1]}\n",
    "ypred = softmaxMat.eval(testFeed)\n",
    "ypred = ypred.reshape((tb[0].shape[0],clustCount*classCount))\n",
    "ypred = tf.argmax(ypred,1).eval()\n",
    "ylookup = [np.argmax(np.sum(tb[1][ypred==i],0)).astype('int32') for i in range(clustCount*classCount)]\n",
    "yconverted = [ylookup[i] for i in ypred]\n",
    "correct_prediction = tf.equal(yconverted, np.argmax(tb[1],1).astype('int32'))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)).eval()\n",
    "print('Clustering Accuracy: %g'%(accuracy))\n",
    "print(ylookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare it to k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tb0 = [tb[0][np.argmax(tb[1],1)<5],tb[1][np.argmax(tb[1],1)<5]]\n",
    "tb1 = [tb[0][np.argmax(tb[1],1)>4],tb[1][np.argmax(tb[1],1)>4]]\n",
    "#<5\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "km0_ypred = kmeans.fit_transform(tb0[0])\n",
    "km0_ypred = np.argmax(km0_ypred,1)\n",
    "km_ylookup = [np.argmax(np.sum(tb0[1][km0_ypred==i],0)).astype('int32') for i in range(clustCount)]\n",
    "km_yconverted = [km_ylookup[i] for i in km0_ypred]\n",
    "km_correct_prediction = tf.equal(km_yconverted, np.argmax(tb0[1],1).astype('int32'))\n",
    "km0_accuracy = tf.reduce_mean(tf.cast(km_correct_prediction, tf.float32)).eval()\n",
    "#>4\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "km1_ypred = kmeans.fit_transform(tb1[0])\n",
    "km1_ypred = np.argmax(km1_ypred,1)\n",
    "km_ylookup = [np.argmax(np.sum(tb1[1][km1_ypred==i],0)).astype('int32') for i in range(clustCount)]\n",
    "km_yconverted = [km_ylookup[i] for i in km1_ypred]\n",
    "km_correct_prediction = tf.equal(km_yconverted, np.argmax(tb1[1],1).astype('int32'))\n",
    "km1_accuracy = tf.reduce_mean(tf.cast(km_correct_prediction, tf.float32)).eval()\n",
    "print('ACOL Accuracy: %g'%(accuracy))\n",
    "print('KMeans Accuracy: %g'%((km0_accuracy+km1_accuracy)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#visualise kmeans\n",
    "digitTrace = np.concatenate([[np.sum(tb0[0][km0_ypred==i,:],axis=0) for i in range(clustCount)],\n",
    "                       [np.sum(tb1[0][km1_ypred==i,:],axis=0) for i in range(clustCount)]])\n",
    "digitTrace = digitTrace/np.max(digitTrace)\n",
    "f,ax=plt.subplots(nrows=classCount, ncols=clustCount, figsize=(1.5*clustCount,1.5*classCount))\n",
    "gs = gridspec.GridSpec(classCount,clustCount)\n",
    "gs.update(wspace=0.025, hspace=0.025)\n",
    "f.subplots_adjust(wspace=0,hspace=0)\n",
    "for i in range(digitTrace.shape[0]):\n",
    "    sp = plt.subplot(gs[i])\n",
    "    sp.set_xticklabels([])\n",
    "    sp.set_yticklabels([])\n",
    "    sp.set_aspect('equal')\n",
    "    sp.grid = False\n",
    "    plt.imshow(np.reshape(digitTrace[i,:],(28,28)),cmap='bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
