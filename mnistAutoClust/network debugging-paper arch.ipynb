{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#imports and settings:\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "mnist = input_data.read_data_sets('./MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper funcs\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "def matrix_softmax(x):\n",
    "    shape = x.get_shape().as_list()\n",
    "    shape[0] = int(-1)\n",
    "    return tf.reshape(tf.nn.softmax(tf.contrib.layers.flatten(x)),shape)\n",
    "\n",
    "def avg_softmax(x):\n",
    "    shape = x.get_shape().as_list()\n",
    "    shape[0] = int(-1)\n",
    "    totalSoft = tf.reshape(tf.nn.softmax(tf.contrib.layers.flatten(x)),shape)\n",
    "    return tf.reduce_sum(totalSoft,2)\n",
    "\n",
    "def max_softmax(x):\n",
    "    shape = x.get_shape().as_list()\n",
    "    shape[0] = int(-1)\n",
    "    totalSoft = tf.reshape(tf.nn.softmax(tf.contrib.layers.flatten(x)),shape)\n",
    "    return tf.reduce_max(totalSoft,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Init model weights & biases\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "\n",
    "#keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#conv_layer1\n",
    "W_conv1 = weight_variable([3,3,1,32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "#conv_layer2\n",
    "W_conv2 = weight_variable([3,3,32,32])\n",
    "b_conv2 = bias_variable([32])\n",
    "\n",
    "#conv_layer3\n",
    "W_conv3 = weight_variable([3,3,32,64])\n",
    "b_conv3 = bias_variable([64])\n",
    "\n",
    "#conv_layer4\n",
    "W_conv4 = weight_variable([3,3,64,64])\n",
    "b_conv4 = bias_variable([64])\n",
    "\n",
    "#fc layer 1\n",
    "W_fc1 = weight_variable([7*7*64, 2048])\n",
    "b_fc1 = bias_variable([2048])\n",
    "\n",
    "#softmax layer <5\n",
    "W_softmax1 = weight_variable([2048, 5])\n",
    "b_softmax1 = bias_variable([5])\n",
    "\n",
    "#softmax layer >5\n",
    "W_softmax2 = weight_variable([2048, 5])\n",
    "b_softmax2 = bias_variable([5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define net\n",
    "#conv 1\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "#conv 2\n",
    "h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "h_drop1 = tf.nn.dropout(h_pool2, tf.constant(0.25))\n",
    "\n",
    "#conv 3\n",
    "h_conv3 = tf.nn.relu(conv2d(h_drop1, W_conv3) + b_conv3)\n",
    "#conv 4\n",
    "h_conv4 = tf.nn.relu(conv2d(h_conv3, W_conv4) + b_conv4)\n",
    "h_pool4 = max_pool_2x2(h_conv4)\n",
    "\n",
    "h_drop2 = tf.nn.dropout(h_pool4, tf.constant(0.25))\n",
    "\n",
    "#fc 1\n",
    "h_pool2_flat = tf.reshape(h_drop2, [-1, 7*7*64])\n",
    "\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "#Cluster layers\n",
    "clust1 = tf.matmul(h_fc1, W_softmax1) + b_softmax1\n",
    "clust2 = tf.matmul(h_fc1, W_softmax2) + b_softmax2\n",
    "\n",
    "#Classification layer\n",
    "stackedClusts = tf.stack([clust1,clust2],1)\n",
    "softmaxMat = matrix_softmax(stackedClusts)\n",
    "smStacked = tf.reduce_max(softmaxMat,2)\n",
    "\n",
    "y_conv = smStacked\n",
    "#y_conv=tf.nn.softmax(tf.matmul(softmaxStacked, W_softmaxgroup) + b_softmaxgroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#helper loss funcs\n",
    "def zBar(x):\n",
    "    return tf.maximum(tf.concat(x,1),0)\n",
    "    \n",
    "def bigU(zb):\n",
    "    return tf.matmul(tf.transpose(zb,perm=[0,2,1]),zb)\n",
    "\n",
    "def coactivity(x):\n",
    "    #Select everything not in the diagonal:\n",
    "    selection = np.ones(x.shape[1],dtype='float64') - np.eye(x.shape[2],dtype='float32')\n",
    "    return tf.reduce_sum(tf.multiply(x,selection))\n",
    "\n",
    "def bigV(x):\n",
    "    #return tf.reshape(tf.reduce_sum(x,axis=2),(1,-1))\n",
    "    smallNu=tf.reduce_sum(x,axis=2)\n",
    "    return tf.matmul(tf.transpose(smallNu),smallNu)\n",
    "\n",
    "def specialNormalise(x):\n",
    "    selection = np.ones(x.shape[1],dtype='float32') - np.eye(x.shape[1],dtype='float32')\n",
    "    top = tf.reduce_sum(tf.multiply(x,selection))\n",
    "    bottom = tf.multiply(tf.to_float(x.shape[1]-1),tf.reduce_sum(tf.multiply(x,np.eye(x.shape[1],dtype='float32'))))\n",
    "    return tf.divide(top,bottom)\n",
    "\n",
    "def frobNorm(x):\n",
    "    return tf.sqrt(tf.reduce_sum(tf.square(x)))\n",
    "\n",
    "one = tf.constant(1.0)\n",
    "zero = tf.constant(0.0)\n",
    "\n",
    "tresh = tf.constant(0.01)\n",
    "c1 = tf.constant(1.0)\n",
    "c2 = tf.constant(1.0)\n",
    "c3 = lambda affinity: tf.cond(tf.less(affinity,tresh),lambda: tf.constant(0.0003),lambda: tf.constant(0.0))\n",
    "c4 =tf.constant(0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#calculate losses\n",
    "#affinity\n",
    "bZ = zBar(softmaxMat)\n",
    "bU = bigU(bZ)\n",
    "coact = coactivity(bU)\n",
    "affinity = specialNormalise(bU)\n",
    "\n",
    "#balance\n",
    "bV=bigV(bZ)\n",
    "balance = specialNormalise(bV)\n",
    "\n",
    "#cross entropy\n",
    "cross_entropy = tf.reduce_max(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n",
    "\n",
    "frob = frobNorm(softmaxMat)#tf.square(tf.norm(stackedClusts,ord='fro'),axis=(0,1))\n",
    "\n",
    "loss = cross_entropy + c1*affinity + c2*(1-balance) + c3(affinity)*coact + c4*frob\n",
    "loss = c1*affinity + c2*(1-balance) + c3(affinity)*coact + c4*frob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(1e-5).minimize(loss)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0/7000 \n",
      " Train: accuracy: 0.42, loss: 0.976476 \n",
      " Validation: accuracy: 0.44 loss: 0.977142\n",
      "affinity: 0.00409955, balance: 0.989876, coact: 0.166799\n",
      "step 100/7000 \n",
      " Train: accuracy: 0.44, loss: 0.989706 \n",
      " Validation: accuracy: 0.6 loss: 0.987113\n",
      "affinity: 0.00569435, balance: 0.980017, coact: 0.181679\n",
      "step 200/7000 \n",
      " Train: accuracy: 0.6, loss: 0.978128 \n",
      " Validation: accuracy: 0.52 loss: 0.997485\n",
      "affinity: 0.000448428, balance: 0.986469, coact: 0.317149\n",
      "step 300/7000 \n",
      " Train: accuracy: 0.42, loss: 0.985651 \n",
      " Validation: accuracy: 0.52 loss: 0.968303\n",
      "affinity: 0.00402778, balance: 0.979355, coact: 0.86243\n",
      "step 400/7000 \n",
      " Train: accuracy: 0.4, loss: 0.972731 \n",
      " Validation: accuracy: 0.48 loss: 0.989247\n",
      "affinity: 0.000962384, balance: 0.993747, coact: 0.734215\n",
      "step 500/7000 \n",
      " Train: accuracy: 0.44, loss: 0.972487 \n",
      " Validation: accuracy: 0.4 loss: 0.952536\n",
      "affinity: 0.00254576, balance: 0.990294, coact: 0.462958\n",
      "step 600/7000 \n",
      " Train: accuracy: 0.42, loss: 0.990959 \n",
      " Validation: accuracy: 0.54 loss: 0.998718\n",
      "affinity: 0.00155309, balance: 0.962472, coact: 0.0222151\n",
      "step 700/7000 \n",
      " Train: accuracy: 0.5, loss: 0.989584 \n",
      " Validation: accuracy: 0.52 loss: 0.993264\n",
      "affinity: 0.00172093, balance: 0.986013, coact: 0.568217\n",
      "step 800/7000 \n",
      " Train: accuracy: 0.54, loss: 0.990109 \n",
      " Validation: accuracy: 0.42 loss: 0.978891\n",
      "affinity: 0.00307705, balance: 0.99644, coact: 0.146271\n",
      "step 900/7000 \n",
      " Train: accuracy: 0.58, loss: 0.973728 \n",
      " Validation: accuracy: 0.58 loss: 0.958467\n",
      "affinity: 0.00690533, balance: 0.974397, coact: 0.142341\n",
      "step 1000/7000 \n",
      " Train: accuracy: 0.4, loss: 0.986634 \n",
      " Validation: accuracy: 0.54 loss: 0.983668\n",
      "affinity: 0.00224105, balance: 0.981767, coact: 0.0379581\n",
      "step 1100/7000 \n",
      " Train: accuracy: 0.54, loss: 0.986462 \n",
      " Validation: accuracy: 0.58 loss: 1.00638\n",
      "affinity: 0.00409669, balance: 0.990313, coact: 0.221706\n",
      "step 1200/7000 \n",
      " Train: accuracy: 0.48, loss: 0.982385 \n",
      " Validation: accuracy: 0.4 loss: 0.996016\n",
      "affinity: 0.00132338, balance: 0.975892, coact: 0.599133\n",
      "step 1300/7000 \n",
      " Train: accuracy: 0.48, loss: 0.996483 \n",
      " Validation: accuracy: 0.54 loss: 0.998911\n",
      "affinity: 0.00330856, balance: 0.950412, coact: 0.132419\n",
      "step 1400/7000 \n",
      " Train: accuracy: 0.48, loss: 1.00028 \n",
      " Validation: accuracy: 0.44 loss: 0.977573\n",
      "affinity: 0.00206659, balance: 0.984911, coact: 0.207085\n",
      "step 1500/7000 \n",
      " Train: accuracy: 0.72, loss: 0.999102 \n",
      " Validation: accuracy: 0.44 loss: 0.984455\n",
      "affinity: 0.00281451, balance: 0.994817, coact: 0.0245925\n",
      "step 1600/7000 \n",
      " Train: accuracy: 0.54, loss: 0.983848 \n",
      " Validation: accuracy: 0.54 loss: 0.989209\n",
      "affinity: 0.00261005, balance: 0.998295, coact: 0.155237\n",
      "step 1700/7000 \n",
      " Train: accuracy: 0.6, loss: 0.982637 \n",
      " Validation: accuracy: 0.58 loss: 0.987181\n",
      "affinity: 0.00798686, balance: 0.976427, coact: 0.0136262\n",
      "step 1800/7000 \n",
      " Train: accuracy: 0.4, loss: 0.931991 \n",
      " Validation: accuracy: 0.44 loss: 0.979264\n",
      "affinity: 0.00532488, balance: 0.995589, coact: 0.214933\n",
      "step 1900/7000 \n",
      " Train: accuracy: 0.42, loss: 0.96255 \n",
      " Validation: accuracy: 0.5 loss: 0.982381\n",
      "affinity: 0.00292092, balance: 0.980238, coact: 0.0102415\n",
      "step 2000/7000 \n",
      " Train: accuracy: 0.46, loss: 0.980639 \n",
      " Validation: accuracy: 0.62 loss: 0.98747\n",
      "affinity: 0.00141881, balance: 0.980185, coact: 1.12052\n",
      "step 2100/7000 \n",
      " Train: accuracy: 0.5, loss: 0.993588 \n",
      " Validation: accuracy: 0.5 loss: 0.961757\n",
      "affinity: 0.000250527, balance: 0.983812, coact: 1.70274\n",
      "step 2200/7000 \n",
      " Train: accuracy: 0.5, loss: 0.981286 \n",
      " Validation: accuracy: 0.48 loss: 0.992455\n",
      "affinity: 1.21231e-06, balance: 0.987622, coact: 0.122798\n",
      "step 2300/7000 \n",
      " Train: accuracy: 0.46, loss: 0.968286 \n",
      " Validation: accuracy: 0.64 loss: 0.969253\n",
      "affinity: 0.00108541, balance: 0.979004, coact: 0.00436097\n",
      "step 2400/7000 \n",
      " Train: accuracy: 0.52, loss: 0.998324 \n",
      " Validation: accuracy: 0.4 loss: 0.987336\n",
      "affinity: 0.00054317, balance: 0.983267, coact: 0.00897343\n",
      "step 2500/7000 \n",
      " Train: accuracy: 0.58, loss: 0.985596 \n",
      " Validation: accuracy: 0.4 loss: 0.985644\n",
      "affinity: 0.00171654, balance: 0.990001, coact: 1.52119\n",
      "step 2600/7000 \n",
      " Train: accuracy: 0.54, loss: 0.98533 \n",
      " Validation: accuracy: 0.42 loss: 0.978027\n",
      "affinity: 2.46888e-05, balance: 0.96898, coact: 0.470798\n",
      "step 2700/7000 \n",
      " Train: accuracy: 0.44, loss: 0.986934 \n",
      " Validation: accuracy: 0.52 loss: 0.980196\n",
      "affinity: 0.000955657, balance: 0.984442, coact: 0.498814\n",
      "step 2800/7000 \n",
      " Train: accuracy: 0.62, loss: 0.995437 \n",
      " Validation: accuracy: 0.52 loss: 0.988356\n",
      "affinity: 0.00143294, balance: 0.978048, coact: 0.155752\n",
      "step 2900/7000 \n",
      " Train: accuracy: 0.58, loss: 0.973765 \n",
      " Validation: accuracy: 0.56 loss: 0.97672\n",
      "affinity: 0.00241974, balance: 0.981429, coact: 0.000208965\n",
      "step 3000/7000 \n",
      " Train: accuracy: 0.44, loss: 0.981904 \n",
      " Validation: accuracy: 0.56 loss: 0.980473\n",
      "affinity: 0.00325266, balance: 0.98617, coact: 1.37725\n",
      "step 3100/7000 \n",
      " Train: accuracy: 0.52, loss: 0.976019 \n",
      " Validation: accuracy: 0.52 loss: 0.982402\n",
      "affinity: 0.00482832, balance: 0.97712, coact: 0.670501\n",
      "step 3200/7000 \n",
      " Train: accuracy: 0.5, loss: 0.974953 \n",
      " Validation: accuracy: 0.5 loss: 0.969878\n",
      "affinity: 0.00231406, balance: 0.98322, coact: 0.406785\n",
      "step 3300/7000 \n",
      " Train: accuracy: 0.54, loss: 0.979358 \n",
      " Validation: accuracy: 0.44 loss: 0.984879\n",
      "affinity: 0.0048607, balance: 0.959847, coact: 1.15533\n",
      "step 3400/7000 \n",
      " Train: accuracy: 0.48, loss: 0.972776 \n",
      " Validation: accuracy: 0.52 loss: 0.990254\n",
      "affinity: 0.00399454, balance: 0.975914, coact: 0.0124736\n",
      "step 3500/7000 \n",
      " Train: accuracy: 0.52, loss: 0.970363 \n",
      " Validation: accuracy: 0.64 loss: 0.992869\n",
      "affinity: 0.00284284, balance: 0.941884, coact: 0.399253\n",
      "step 3600/7000 \n",
      " Train: accuracy: 0.58, loss: 0.934681 \n",
      " Validation: accuracy: 0.64 loss: 0.913787\n",
      "affinity: 0.00289987, balance: 0.968116, coact: 1.06913\n",
      "step 3700/7000 \n",
      " Train: accuracy: 0.54, loss: 0.97448 \n",
      " Validation: accuracy: 0.5 loss: 0.975778\n",
      "affinity: 0.0010818, balance: 0.983845, coact: 0.0726214\n",
      "step 3800/7000 \n",
      " Train: accuracy: 0.38, loss: 0.990097 \n",
      " Validation: accuracy: 0.52 loss: 0.965179\n",
      "affinity: 0.00400166, balance: 0.965307, coact: 0.634581\n",
      "step 3900/7000 \n",
      " Train: accuracy: 0.52, loss: 0.986536 \n",
      " Validation: accuracy: 0.5 loss: 0.979372\n",
      "affinity: 0.000895883, balance: 0.975864, coact: 1.51178\n",
      "step 4000/7000 \n",
      " Train: accuracy: 0.46, loss: 0.983177 \n",
      " Validation: accuracy: 0.44 loss: 0.986127\n",
      "affinity: 0.0090938, balance: 0.96558, coact: 1.04971\n",
      "step 4100/7000 \n",
      " Train: accuracy: 0.56, loss: 0.990813 \n",
      " Validation: accuracy: 0.38 loss: 0.972692\n",
      "affinity: 0.0118166, balance: 0.976036, coact: 0.663049\n",
      "step 4200/7000 \n",
      " Train: accuracy: 0.58, loss: 0.979293 \n",
      " Validation: accuracy: 0.56 loss: 0.987214\n",
      "affinity: 0.00271874, balance: 0.983475, coact: 1.63175\n",
      "step 4300/7000 \n",
      " Train: accuracy: 0.54, loss: 0.953709 \n",
      " Validation: accuracy: 0.48 loss: 0.964781\n",
      "affinity: 0.00349699, balance: 0.951034, coact: 1.06442\n",
      "step 4400/7000 \n",
      " Train: accuracy: 0.54, loss: 0.966157 \n",
      " Validation: accuracy: 0.4 loss: 0.983534\n",
      "affinity: 0.00240887, balance: 0.975341, coact: 0.516386\n",
      "step 4500/7000 \n",
      " Train: accuracy: 0.6, loss: 0.96244 \n",
      " Validation: accuracy: 0.5 loss: 0.961901\n",
      "affinity: 0.00362724, balance: 0.97362, coact: 0.747208\n",
      "step 4600/7000 \n",
      " Train: accuracy: 0.52, loss: 0.966024 \n",
      " Validation: accuracy: 0.58 loss: 0.985411\n",
      "affinity: 0.00821382, balance: 0.981143, coact: 0.646558\n",
      "step 4700/7000 \n",
      " Train: accuracy: 0.58, loss: 0.967081 \n",
      " Validation: accuracy: 0.46 loss: 0.976708\n",
      "affinity: 0.00638902, balance: 0.977369, coact: 1.05886\n",
      "step 4800/7000 \n",
      " Train: accuracy: 0.54, loss: 0.986698 \n",
      " Validation: accuracy: 0.44 loss: 0.953934\n",
      "affinity: 0.000713968, balance: 0.997105, coact: 1.5967\n",
      "step 4900/7000 \n",
      " Train: accuracy: 0.38, loss: 0.937653 \n",
      " Validation: accuracy: 0.46 loss: 0.970622\n",
      "affinity: 0.00806905, balance: 0.971738, coact: 0.119718\n",
      "step 5000/7000 \n",
      " Train: accuracy: 0.48, loss: 0.988268 \n",
      " Validation: accuracy: 0.58 loss: 0.939497\n",
      "affinity: 0.00104131, balance: 0.969807, coact: 0.606218\n",
      "step 5100/7000 \n",
      " Train: accuracy: 0.6, loss: 0.976755 \n",
      " Validation: accuracy: 0.52 loss: 0.957728\n",
      "affinity: 0.00755974, balance: 0.975147, coact: 0.656719\n",
      "step 5200/7000 \n",
      " Train: accuracy: 0.52, loss: 0.981523 \n",
      " Validation: accuracy: 0.52 loss: 0.970534\n",
      "affinity: 0.00362483, balance: 0.994212, coact: 0.783879\n",
      "step 5300/7000 \n",
      " Train: accuracy: 0.34, loss: 0.952132 \n",
      " Validation: accuracy: 0.42 loss: 0.970179\n",
      "affinity: 0.00691568, balance: 0.989968, coact: 0.871086\n",
      "step 5400/7000 \n",
      " Train: accuracy: 0.46, loss: 0.974369 \n",
      " Validation: accuracy: 0.44 loss: 0.983293\n",
      "affinity: 0.00845047, balance: 0.975163, coact: 0.686889\n",
      "step 5500/7000 \n",
      " Train: accuracy: 0.36, loss: 0.987249 \n",
      " Validation: accuracy: 0.48 loss: 0.976335\n",
      "affinity: 0.00679581, balance: 0.98752, coact: 1.85371\n",
      "step 5600/7000 \n",
      " Train: accuracy: 0.38, loss: 0.988815 \n",
      " Validation: accuracy: 0.48 loss: 0.982904\n",
      "affinity: 0.00277672, balance: 0.969045, coact: 0.329947\n",
      "step 5700/7000 \n",
      " Train: accuracy: 0.4, loss: 0.990236 \n",
      " Validation: accuracy: 0.52 loss: 0.983552\n",
      "affinity: 0.00157028, balance: 0.980973, coact: 0.643745\n",
      "step 5800/7000 \n",
      " Train: accuracy: 0.56, loss: 0.947622 \n",
      " Validation: accuracy: 0.38 loss: 0.960222\n",
      "affinity: 0.00468645, balance: 0.971958, coact: 1.01381\n",
      "step 5900/7000 \n",
      " Train: accuracy: 0.6, loss: 0.981982 \n",
      " Validation: accuracy: 0.44 loss: 0.976252\n",
      "affinity: 0.00782003, balance: 0.947339, coact: 0.680403\n",
      "step 6000/7000 \n",
      " Train: accuracy: 0.58, loss: 0.974209 \n",
      " Validation: accuracy: 0.5 loss: 0.955651\n",
      "affinity: 0.00139492, balance: 0.942689, coact: 1.14362\n",
      "step 6100/7000 \n",
      " Train: accuracy: 0.5, loss: 0.954111 \n",
      " Validation: accuracy: 0.54 loss: 0.963379\n",
      "affinity: 0.00556066, balance: 0.979028, coact: 1.79678\n",
      "step 6200/7000 \n",
      " Train: accuracy: 0.54, loss: 0.987544 \n",
      " Validation: accuracy: 0.5 loss: 0.982866\n",
      "affinity: 0.00156583, balance: 0.987543, coact: 0.912447\n",
      "step 6300/7000 \n",
      " Train: accuracy: 0.58, loss: 0.997383 \n",
      " Validation: accuracy: 0.34 loss: 0.973742\n",
      "affinity: 0.000196456, balance: 0.998577, coact: 0.873305\n",
      "step 6400/7000 \n",
      " Train: accuracy: 0.66, loss: 0.962928 \n",
      " Validation: accuracy: 0.6 loss: 0.99803\n",
      "affinity: 0.0066629, balance: 0.960357, coact: 0.454028\n",
      "step 6500/7000 \n",
      " Train: accuracy: 0.6, loss: 0.992229 \n",
      " Validation: accuracy: 0.5 loss: 0.977997\n",
      "affinity: 0.00593467, balance: 0.968324, coact: 0.102109\n",
      "step 6600/7000 \n",
      " Train: accuracy: 0.44, loss: 0.978884 \n",
      " Validation: accuracy: 0.38 loss: 0.958181\n",
      "affinity: 0.00266682, balance: 0.989657, coact: 0.697688\n",
      "step 6700/7000 \n",
      " Train: accuracy: 0.6, loss: 0.950379 \n",
      " Validation: accuracy: 0.58 loss: 0.984896\n",
      "affinity: 0.00721422, balance: 0.999233, coact: 1.21602\n",
      "step 6800/7000 \n",
      " Train: accuracy: 0.5, loss: 0.995005 \n",
      " Validation: accuracy: 0.56 loss: 0.984904\n",
      "affinity: 0.00563581, balance: 0.975429, coact: 0.580355\n",
      "step 6900/7000 \n",
      " Train: accuracy: 0.52, loss: 0.973692 \n",
      " Validation: accuracy: 0.48 loss: 0.957758\n",
      "affinity: 0.00093431, balance: 0.990156, coact: 0.23309\n"
     ]
    }
   ],
   "source": [
    "y = {0:[0,1], 1:[1,0]}\n",
    "\n",
    "totalSteps = 7000\n",
    "batchSize = 50\n",
    "for i in range(totalSteps):\n",
    "    trainbatch = mnist.train.next_batch(batchSize)\n",
    "    trainbatch = (trainbatch[0],np.array([y[np.argmax(trainbatch[1][j])>4] for j in range(len(trainbatch[1]))]))\n",
    "    valbatch = mnist.validation.next_batch(batchSize)\n",
    "    valbatch = (valbatch[0],np.array([y[np.argmax(valbatch[1][j])>4] for j in range(len(valbatch[1]))]))\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        train_loss,train_acc = sess.run([loss, accuracy],feed_dict={x: trainbatch[0], y_: trainbatch[1]})\n",
    "        val_loss, val_acc = sess.run([loss, accuracy],feed_dict={x: valbatch[0], y_: valbatch[1]})\n",
    "\n",
    "        print(\"step %d/%d \\n Train: accuracy: %g, loss: %g \\n Validation: accuracy: %g loss: %g\"%(i,totalSteps, train_acc, train_loss, val_acc, val_loss))\n",
    "        af = affinity.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1]})\n",
    "        ba = balance.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1]})\n",
    "        co = coact.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1]})\n",
    "        print(\"affinity: %g, balance: %g, coact: %g\"%(af,(1-ba),co))\n",
    "    feed_dict = {x: trainbatch[0], y_: trainbatch[1]}\n",
    "    _ = sess.run([train_step],feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52400000000000002"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testSize = 1000\n",
    "testbatch = mnist.test.next_batch(testSize)\n",
    "testbatch = (testbatch[0],np.array([y[np.argmax(testbatch[1][j])>4] for j in range(len(testbatch[1]))]))\n",
    "\n",
    "np.sum(np.argmax(y_conv.eval({x: testbatch[0], y_: testbatch[1]}),axis=1)==np.argmax(testbatch[1],axis=1))/float(testSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[  0.00000000e+00,   4.51118439e-23,   2.34795500e-15,\n",
       "           1.95343559e-28,   2.35504170e-15],\n",
       "        [  0.00000000e+00,   5.16833643e-06,   6.27332169e-23,\n",
       "           9.99994874e-01,   4.54980701e-32]]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sess.run(tf.global_variables_initializer())\n",
    "y = {0:[0,1], 1:[1,0]}\n",
    "tb = mnist.test.next_batch(1)\n",
    "testbatch = (tb[0],np.array([y[np.argmax(tb[1][j])>4] for j in range(len(tb[1]))]))\n",
    "print tb[1]\n",
    "smMat, acc = sess.run([softmaxMat,accuracy],feed_dict={x: testbatch[0], y_: testbatch[1]})\n",
    "print acc\n",
    "softmaxMat.eval({x: testbatch[0], y_: testbatch[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul_5:0' shape=(2, 2) dtype=float64>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=np.ones((1,2,5))\n",
    "bigV(zBar(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
