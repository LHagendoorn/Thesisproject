{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#imports and settings:\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "mnist = input_data.read_data_sets('./MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper funcs\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x,W):\n",
    "  return tf.nn.conv2d(x,W,strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Init model weights & biases\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#conv_layer1\n",
    "W_conv1 = weight_variable([5,5,1,32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "#conv_layer2\n",
    "W_conv2 = weight_variable([5,5,32,64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "#fc layer 1\n",
    "W_fc1 = weight_variable([7*7*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "#softmax layer <5\n",
    "W_softmax1 = weight_variable([1024, 5])\n",
    "b_softmax1 = bias_variable([5])\n",
    "\n",
    "#softmax layer >5\n",
    "W_softmax2 = weight_variable([1024, 5])\n",
    "b_softmax2 = bias_variable([5])\n",
    "\n",
    "#(maybe) softmax combined\n",
    "W_softmaxgroup = weight_variable([2,2])\n",
    "b_softmaxgroup = bias_variable([2])\n",
    "\n",
    "#final fc layer\n",
    "W_fc2 = weight_variable([1024, 2])\n",
    "b_fc2 = bias_variable([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define net\n",
    "#conv 1\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "#conv 2\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "#fc 1\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "#h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "#Two FC softmax layers\n",
    "sm1 = tf.nn.softmax(tf.matmul(h_fc1, W_softmax1) + b_softmax1)\n",
    "sm2 = tf.nn.softmax(tf.matmul(h_fc1, W_softmax2) + b_softmax2)\n",
    "\n",
    "#Pool both softmaxes into single \"softmax\" node (?)\n",
    "sm1_pool = tf.reduce_max(sm1,1)\n",
    "sm2_pool = tf.reduce_max(sm2,1)\n",
    "\n",
    "#pool to final classification\n",
    "softmaxStacked = tf.stack([sm1_pool, sm2_pool],1)\n",
    "#y_conv = softmaxStacked\n",
    "y_conv=tf.nn.softmax(tf.matmul(softmaxStacked, W_softmaxgroup) + b_softmaxgroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#helper loss funcs\n",
    "def zBar(x):\n",
    "  return tf.maximum(tf.concat(x,1),0)\n",
    "    \n",
    "def bigU(x):\n",
    "  return tf.matmul(tf.transpose(x),x)\n",
    "\n",
    "def coactivity(x):\n",
    "  #Select everything not in the diagonal:\n",
    "  selection = np.ones(x.shape,dtype='float32') - np.eye(x.shape[1],dtype='float32')\n",
    "  return tf.reduce_sum(tf.matmul(x,selection))\n",
    "\n",
    "def bigV(x):\n",
    "  smallNu=tf.reshape(tf.reduce_sum(x,axis=0),(1,-1))\n",
    "  return tf.matmul(tf.transpose(smallNu),smallNu)\n",
    "\n",
    "def specialNormalise(x):\n",
    "  selection = np.ones(x.shape,dtype='float32') - np.eye(x.shape[0],dtype='float32')\n",
    "  top = tf.reduce_sum(tf.matmul(x,selection))\n",
    "  bottom = tf.multiply(tf.to_float(x.shape[0]-1),tf.reduce_sum(tf.matmul(x,np.eye(x.shape[0],dtype='float32'))))\n",
    "  return tf.divide(top,bottom)\n",
    "\n",
    "one = tf.constant(0.2)\n",
    "zero = tf.constant(0.0)\n",
    "\n",
    "tresh = tf.constant(0.01)\n",
    "c1 = one\n",
    "c2 = one\n",
    "c3 = lambda affinity: tf.cond(tf.less(affinity,tresh),lambda: one,lambda: zero)\n",
    "c4 = one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate losses\n",
    "#affinity\n",
    "bZ = zBar([sm1,sm2])\n",
    "bU = bigU(bZ)\n",
    "coact = coactivity(bU)\n",
    "affinity = specialNormalise(bU)\n",
    "\n",
    "#balance\n",
    "bV=bigV(bZ)\n",
    "balance = specialNormalise(bV)\n",
    "\n",
    "#cross entropy\n",
    "cross_entropy = tf.reduce_max(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n",
    "\n",
    "loss = cross_entropy #+ c1*affinity + c2*(1-balance) + c3*coact + c4*tf.square(tf.norm(tf.concat([sm1,sm2],1),ord='fro',axis=(0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0/20000 \n",
      " Train: accuracy: 0.64, loss: 1.18013 \n",
      " Validation: accuracy: 0.54 loss: 1.11293\n",
      "step 100/20000 \n",
      " Train: accuracy: 0.55, loss: 1.07288e-06 \n",
      " Validation: accuracy: 0.53 loss: 8.34465e-07\n",
      "step 200/20000 \n",
      " Train: accuracy: 0.49, loss: 2.38419e-07 \n",
      " Validation: accuracy: 0.39 loss: 1.19209e-07\n",
      "step 300/20000 \n",
      " Train: accuracy: 0.49, loss: 1.19209e-07 \n",
      " Validation: accuracy: 0.48 loss: 1.19209e-07\n",
      "step 400/20000 \n",
      " Train: accuracy: 0.51, loss: 1.19209e-07 \n",
      " Validation: accuracy: 0.38 loss: -0\n",
      "step 500/20000 \n",
      " Train: accuracy: 0.54, loss: -0 \n",
      " Validation: accuracy: 0.56 loss: -0\n",
      "step 600/20000 \n",
      " Train: accuracy: 0.45, loss: -0 \n",
      " Validation: accuracy: 0.48 loss: -0\n",
      "step 700/20000 \n",
      " Train: accuracy: 0.51, loss: -0 \n",
      " Validation: accuracy: 0.48 loss: -0\n",
      "step 800/20000 \n",
      " Train: accuracy: 0.51, loss: -0 \n",
      " Validation: accuracy: 0.51 loss: -0\n",
      "step 900/20000 \n",
      " Train: accuracy: 0.5, loss: -0 \n",
      " Validation: accuracy: 0.48 loss: -0\n",
      "step 1000/20000 \n",
      " Train: accuracy: 0.41, loss: -0 \n",
      " Validation: accuracy: 0.47 loss: -0\n",
      "step 1100/20000 \n",
      " Train: accuracy: 0.49, loss: -0 \n",
      " Validation: accuracy: 0.52 loss: -0\n",
      "step 1200/20000 \n",
      " Train: accuracy: 0.5, loss: -0 \n",
      " Validation: accuracy: 0.48 loss: -0\n",
      "step 1300/20000 \n",
      " Train: accuracy: 0.48, loss: -0 \n",
      " Validation: accuracy: 0.6 loss: -0\n",
      "step 1400/20000 \n",
      " Train: accuracy: 0.47, loss: -0 \n",
      " Validation: accuracy: 0.53 loss: -0\n",
      "step 1500/20000 \n",
      " Train: accuracy: 0.53, loss: -0 \n",
      " Validation: accuracy: 0.37 loss: -0\n",
      "step 1600/20000 \n",
      " Train: accuracy: 0.44, loss: -0 \n",
      " Validation: accuracy: 0.52 loss: -0\n",
      "step 1700/20000 \n",
      " Train: accuracy: 0.49, loss: -0 \n",
      " Validation: accuracy: 0.49 loss: -0\n",
      "step 1800/20000 \n",
      " Train: accuracy: 0.48, loss: -0 \n",
      " Validation: accuracy: 0.54 loss: -0\n",
      "step 1900/20000 \n",
      " Train: accuracy: 0.51, loss: -0 \n",
      " Validation: accuracy: 0.5 loss: -0\n",
      "step 2000/20000 \n",
      " Train: accuracy: 0.56, loss: -0 \n",
      " Validation: accuracy: 0.41 loss: -0\n",
      "step 2100/20000 \n",
      " Train: accuracy: 0.46, loss: -0 \n",
      " Validation: accuracy: 0.45 loss: -0\n",
      "step 2200/20000 \n",
      " Train: accuracy: 0.59, loss: -0 \n",
      " Validation: accuracy: 0.54 loss: -0\n",
      "step 2300/20000 \n",
      " Train: accuracy: 0.48, loss: -0 \n",
      " Validation: accuracy: 0.5 loss: -0\n",
      "step 2400/20000 \n",
      " Train: accuracy: 0.45, loss: -0 \n",
      " Validation: accuracy: 0.52 loss: -0\n",
      "step 2500/20000 \n",
      " Train: accuracy: 0.55, loss: -0 \n",
      " Validation: accuracy: 0.5 loss: -0\n",
      "step 2600/20000 \n",
      " Train: accuracy: 0.44, loss: -0 \n",
      " Validation: accuracy: 0.47 loss: -0\n",
      "step 2700/20000 \n",
      " Train: accuracy: 0.55, loss: -0 \n",
      " Validation: accuracy: 0.51 loss: -0\n",
      "step 2800/20000 \n",
      " Train: accuracy: 0.52, loss: -0 \n",
      " Validation: accuracy: 0.46 loss: -0\n",
      "step 2900/20000 \n",
      " Train: accuracy: 0.55, loss: -0 \n",
      " Validation: accuracy: 0.56 loss: -0\n",
      "step 3000/20000 \n",
      " Train: accuracy: 0.5, loss: -0 \n",
      " Validation: accuracy: 0.49 loss: -0\n",
      "step 3100/20000 \n",
      " Train: accuracy: 0.47, loss: -0 \n",
      " Validation: accuracy: 0.47 loss: -0\n",
      "step 3200/20000 \n",
      " Train: accuracy: 0.5, loss: -0 \n",
      " Validation: accuracy: 0.4 loss: -0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a898d5b96fc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#print(\"affinity: %g, balance: %g, coact: %g\"%(af,(1-ba),co))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrainbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrainbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y = {0:[0,1], 1:[1,0]}\n",
    "\n",
    "totalSteps = 20000\n",
    "batchSize = 100\n",
    "for i in range(totalSteps):\n",
    "    trainbatch = mnist.train.next_batch(batchSize)\n",
    "    trainbatch = (trainbatch[0],np.array([y[np.argmax(trainbatch[1][j])>4] for j in range(len(trainbatch[1]))]))\n",
    "    valbatch = mnist.validation.next_batch(batchSize)\n",
    "    valbatch = (valbatch[0],np.array([y[np.argmax(valbatch[1][j])>4] for j in range(len(valbatch[1]))]))\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        train_loss,train_acc = sess.run([loss, accuracy],feed_dict={x: trainbatch[0], y_: trainbatch[1], keep_prob: 1})\n",
    "        #train_loss = loss.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1], keep_prob: 1.0})\n",
    "        val_loss, val_acc = sess.run([loss, accuracy],feed_dict={x: valbatch[0], y_: valbatch[1], keep_prob: 1})\n",
    "        #val_loss = loss.eval(feed_dict={x:valbatch[0], y_:valbatch[1], keep_prob: 1.0})\n",
    "        #train_acc = np.sum(np.argmax(train_y,axis=1)==np.argmax(trainbatch[1],axis=1))/float(batchSize)\n",
    "        #val_acc = np.sum(np.argmax(val_y,axis=1)==np.argmax(valbatch[1],axis=1))/float(batchSize)\n",
    "        print(\"step %d/%d \\n Train: accuracy: %g, loss: %g \\n Validation: accuracy: %g loss: %g\"%(i,totalSteps, train_acc, train_loss, val_acc, val_loss))\n",
    "        #af = affinity.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1], keep_prob: 1.0})\n",
    "        #ba = balance.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1], keep_prob: 1.0})\n",
    "        #co = coact.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1], keep_prob: 1.0})\n",
    "        #print(\"affinity: %g, balance: %g, coact: %g\"%(af,(1-ba),co))\n",
    "    feed_dict = {x: trainbatch[0], y_: trainbatch[1], keep_prob: 0.5}\n",
    "    _ = sess.run([train_step],feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testSize = 1000\n",
    "testbatch = mnist.test.next_batch(testSize)\n",
    "testbatch = (testbatch[0],np.array([y[np.argmax(testbatch[1][j])>4] for j in range(len(testbatch[1]))]))\n",
    "\n",
    "np.sum(np.argmax(y_conv.eval({x: testbatch[0], y_: testbatch[1], keep_prob: 0.5}),axis=1)==np.argmax(testbatch[1],axis=1))/float(testSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
