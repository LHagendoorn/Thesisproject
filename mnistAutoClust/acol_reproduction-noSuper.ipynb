{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACOL replication tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#imports and settings:\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "#from jupyterthemes import jtplot\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import threshold\n",
    "#jtplot.style()\n",
    "%matplotlib inline\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "mnist = input_data.read_data_sets('./MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clustCount = 15\n",
    "classCount = 1\n",
    "net = 0\n",
    "trainsteps = 300000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#helper funcs\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "def matrix_softmax(x):\n",
    "    shape = x.get_shape().as_list()\n",
    "    shape[0] = int(-1)\n",
    "    return tf.contrib.layers.flatten(x)\n",
    "    return tf.reshape(tf.nn.softmax(tf.contrib.layers.flatten(x)),shape)\n",
    "\n",
    "def avg_softmax(x):\n",
    "    shape = x.get_shape().as_list()\n",
    "    shape[0] = int(-1)\n",
    "    totalSoft = tf.reshape(tf.nn.softmax(tf.contrib.layers.flatten(x)),shape)\n",
    "    return tf.reduce_sum(totalSoft,2)\n",
    "\n",
    "def max_softmax(x):\n",
    "    shape = x.get_shape().as_list()\n",
    "    shape[0] = int(-1)\n",
    "    totalSoft = tf.reshape(tf.nn.softmax(tf.contrib.layers.flatten(x)),shape)\n",
    "    return tf.reduce_max(totalSoft,2)\n",
    "\n",
    "def initACOL(in_size,clust,clss):\n",
    "    acolLayers = []\n",
    "    for i in range(clss):\n",
    "        acolLayers.append([\n",
    "            weight_variable([in_size, clustCount]),\n",
    "            bias_variable([clustCount])\n",
    "        ])\n",
    "    return acolLayers\n",
    "        \n",
    "def connectACOL(inLayer,acol):\n",
    "    clust = []\n",
    "    for l in range(0,len(acol)):\n",
    "        clust.append(tf.matmul(inLayer, acol[l][0]) + acol[l][1])\n",
    "    return clust\n",
    "        \n",
    "def acol(input,clust_count, class_count):\n",
    "    acolLayers = []\n",
    "    for i in range(class_count):\n",
    "        if isinstance(input, tuple):\n",
    "                input = input[0]\n",
    "\n",
    "        #I don't know what this bit does, but I don't think it'll hurt anything\n",
    "        #Or maybe it does, who knows\n",
    "        input_shape = input.get_shape()\n",
    "        if input_shape.ndims == 4:\n",
    "            dim = 1\n",
    "            for d in input_shape[1:].as_list():\n",
    "                dim *= d\n",
    "        #    feed_in = tf.reshape(tf.transpose(input,[0,3,1,2]), [-1, dim])\n",
    "        else:\n",
    "            feed_in, dim = (input, int(input_shape[-1]))\n",
    "\n",
    "        init_weights = tf.truncated_normal_initializer(0.0, stddev=0.1)#(0.0, stddev=0.01)\n",
    "        init_biases = tf.constant_initializer(1.0)#(0.1)\n",
    "\n",
    "        weights = weight_variable([dim, clust_count])\n",
    "        biases = bias_variable([clust_count])\n",
    "\n",
    "        acoll = tf.nn.xw_plus_b(input,weights,biases)\n",
    "        acolLayers.append(acol)\n",
    "    return acolLayers    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create placeholders (weights&biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if net==0:\n",
    "    #Init model weights & biases\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    x_image = tf.reshape(x, [-1,28,28,1])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, classCount])\n",
    "    y_def = tf.placeholder(tf.float32,shape=[None, 10]) #!!!\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    #conv_layer1\n",
    "    W_conv1 = weight_variable([5,5,1,32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "\n",
    "    #conv_layer2\n",
    "    W_conv2 = weight_variable([5,5,32,64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "\n",
    "    #fc layer 1\n",
    "    W_fc1 = weight_variable([7*7*64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "\n",
    "    acol = initACOL(1024,clustCount,classCount)\n",
    "\n",
    "    #final fc layer\n",
    "    W_fc2 = weight_variable([1024, classCount])\n",
    "    b_fc2 = bias_variable([classCount])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if net==0:\n",
    "    #Define net\n",
    "    #conv 1\n",
    "    l_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    l_pool1 = max_pool_2x2(l_conv1)\n",
    "\n",
    "    #conv 2\n",
    "    l_conv2 = tf.nn.relu(conv2d(l_pool1, W_conv2) + b_conv2)\n",
    "    l_pool2 = max_pool_2x2(l_conv2)\n",
    "\n",
    "    #fc 1\n",
    "    l_pool2_flat = tf.reshape(l_pool2, [-1, 7*7*64])\n",
    "    l_fc1 = tf.nn.relu(tf.matmul(l_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    l_fc1_drop = tf.nn.dropout(l_fc1, keep_prob)\n",
    "\n",
    "    l_acol = connectACOL(l_fc1_drop,acol)\n",
    "    #l_acol = acol(l_fc1_drop,clustCount,classCount)\n",
    "\n",
    "    #Classification layer\n",
    "    stackedClusts = tf.stack(l_acol,1)\n",
    "    softmaxMat = matrix_softmax(stackedClusts)\n",
    "    y_conv = tf.reshape(softmaxMat,[-1,clustCount*classCount])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if net==1:\n",
    "    #Init model weights & biases\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    x_image = tf.reshape(x, [-1,28,28,1])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, classCount])\n",
    "\n",
    "    #keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    #conv_layer1\n",
    "    W_conv1 = weight_variable([3,3,1,32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "\n",
    "    #conv_layer2\n",
    "    W_conv2 = weight_variable([3,3,32,32])\n",
    "    b_conv2 = bias_variable([32])\n",
    "\n",
    "    #conv_layer3\n",
    "    W_conv3 = weight_variable([3,3,32,64])\n",
    "    b_conv3 = bias_variable([64])\n",
    "\n",
    "    #conv_layer4\n",
    "    W_conv4 = weight_variable([3,3,64,64])\n",
    "    b_conv4 = bias_variable([64])\n",
    "\n",
    "    #fc layer 1\n",
    "    W_fc1 = weight_variable([7*7*64, 2048])\n",
    "    b_fc1 = bias_variable([2048])\n",
    "\n",
    "    acol = initACOL(2048,clustCount,classCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if net==1:\n",
    "    #Define net\n",
    "    #conv 1\n",
    "    l_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    #conv 2\n",
    "    l_conv2 = tf.nn.relu(conv2d(l_conv1, W_conv2) + b_conv2)\n",
    "    l_pool2 = max_pool_2x2(l_conv2)\n",
    "\n",
    "    l_drop1 = tf.nn.dropout(l_pool2, tf.constant(0.25))\n",
    "\n",
    "    #conv 3\n",
    "    l_conv3 = tf.nn.relu(conv2d(l_drop1, W_conv3) + b_conv3)\n",
    "    #conv 4\n",
    "    l_conv4 = tf.nn.relu(conv2d(l_conv3, W_conv4) + b_conv4)\n",
    "    l_pool4 = max_pool_2x2(l_conv4)\n",
    "\n",
    "    l_drop2 = tf.nn.dropout(l_pool4, tf.constant(0.25))\n",
    "\n",
    "    #fc 1\n",
    "    l_pool2_flat = tf.reshape(l_drop2, [-1, 7*7*64])\n",
    "\n",
    "    l_fc1 = tf.nn.relu(tf.matmul(l_pool2_flat, W_fc1) + b_fc1)\n",
    "    \n",
    "    l_fc1_drop = tf.nn.dropout(l_fc1, tf.constant(0.5))\n",
    "    \n",
    "    l_acol = connectACOL(l_fc1_drop,acol)\n",
    "    #l_acol = acol(l_fc1_drop,clustCount, classCount)\n",
    "\n",
    "    #Classification layer\n",
    "    stackedClusts = tf.stack(l_acol,1)\n",
    "    softmaxMat = matrix_softmax(stackedClusts)\n",
    "    #smStacked = tf.reduce_max(softmaxMat,2)\n",
    "\n",
    "    y_conv = tf.reshape(softmaxMat,[-1,clustCount*classCount])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helperfunctions for loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#helper loss funcs\n",
    "def zBar(x):\n",
    "    xshape = x.shape.as_list()\n",
    "    #print(xshape)\n",
    "    s=[-1,xshape[1]*xshape[2]]\n",
    "    return tf.maximum(tf.reshape(x,s),0)\n",
    "    \n",
    "def bigU(zb):\n",
    "    return tf.matmul(tf.transpose(zb),zb)\n",
    "\n",
    "def selectNonDiag(x):\n",
    "    selection = np.ones(x.shape.as_list()[0],dtype='float32') - np.eye(x.shape.as_list()[0],dtype='float32')\n",
    "    return tf.reduce_sum(tf.multiply(x,selection))\n",
    "\n",
    "def bigV(x):\n",
    "    smallNu=tf.reshape(tf.reduce_sum(x,axis=0),[1,-1])\n",
    "    return tf.multiply(tf.transpose(smallNu),smallNu)\n",
    "\n",
    "def specialNormalise(x):\n",
    "    top = selectNonDiag(x)\n",
    "    bottom = tf.multiply(tf.to_float(x.shape[1]-1),tf.reduce_sum(tf.multiply(x,np.eye(x.shape[1],dtype='float32'))))\n",
    "    return tf.divide(top,bottom)\n",
    "\n",
    "def frobNorm(x):\n",
    "    return tf.sqrt(tf.reduce_sum(tf.square(x)))\n",
    "\n",
    "#Define a accuracy based on the maximal % of same numbers in a cluster\n",
    "#i.e. per cluster count all labels with the heighest value for that cluster\n",
    "#take the highest and divide by the total numbers in that cluster\n",
    "#def clustAcc(x):\n",
    "    \n",
    "\n",
    "tresh = tf.constant(0.03)#0.03\n",
    "cc0=0.1\n",
    "cc1=1.0\n",
    "cc2=1.0\n",
    "cc3=0.0003 #0.0003\n",
    "cc4=0.000001\n",
    "c0 = tf.constant(cc0)\n",
    "c1 = tf.constant(cc1)\n",
    "c2 = tf.constant(cc2)\n",
    "c3val = tf.constant(cc3)\n",
    "c3 = lambda affinity: tf.cond(tf.less(affinity,tresh),lambda: c3val,lambda: tf.constant(0.0))\n",
    "c4 =tf.constant(cc4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#calculate losses\n",
    "#affinity\n",
    "bZ = zBar(stackedClusts)#softmaxMat)\n",
    "bU = bigU(bZ)\n",
    "coact = selectNonDiag(bU)\n",
    "affinity = specialNormalise(bU)\n",
    "\n",
    "#balance\n",
    "bV=bigV(bZ)\n",
    "balance = specialNormalise(bV)\n",
    "\n",
    "#clustering cross entropy\n",
    "ypred = softmaxMat\n",
    "ypred = tf.reshape(ypred,(-1,clustCount*classCount))\n",
    "ypred = tf.argmax(ypred,1)\n",
    "\n",
    "#onehotY = tf.one_hot(tf.cast(y_,'int32'),10)\n",
    "ylookup = tf.unsorted_segment_sum(y_def,ypred,tf.constant(classCount*clustCount))\n",
    "yconverted = tf.gather(ylookup,ypred)\n",
    "\n",
    "clust_cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_def * tf.log(tf.clip_by_value(yconverted,1e-10,1.0)), reduction_indices=[1]))\n",
    "\n",
    "#cross entropy\n",
    "#cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y_conv,1e-10,1.0)), reduction_indices=[1]))\n",
    "\n",
    "#neutral cross entropy\n",
    "#neut_cross_entropy = tf.reduce_mean(-tf.reduce_max(tf.log(tf.clip_by_value(y_conv,1e-10,1.0)), reduction_indices=[1]))\n",
    "#neut_cross_entropy = tf.constant(0.0)\n",
    "\n",
    "frob = frobNorm(stackedClusts)#softmaxMat)\n",
    "\n",
    "loss = c0*clust_cross_entropy+c1*affinity + c2*tf.subtract(tf.constant(1.0),balance) + c3(affinity)*coact + c4*frob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "#correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#y = {0:[0,1], 1:[1,0]}\n",
    "y = {0:[1,0,0,0,0],\n",
    "     1:[1,0,0,0,0],\n",
    "     2:[0,1,0,0,0],\n",
    "     3:[0,1,0,0,0],\n",
    "     4:[0,0,1,0,0],\n",
    "     5:[0,0,1,0,0],\n",
    "     6:[0,0,0,1,0],\n",
    "     7:[0,0,0,1,0],\n",
    "     8:[0,0,0,0,1],\n",
    "     9:[0,0,0,0,1]}\n",
    "\n",
    "y = {0:[1,0],\n",
    "     1:[1,0],\n",
    "     2:[1,0],\n",
    "     3:[1,0],\n",
    "     4:[1,0],\n",
    "     5:[0,1],\n",
    "     6:[0,1],\n",
    "     7:[0,1],\n",
    "     8:[0,1],\n",
    "     9:[0,1]}\n",
    "\n",
    "y = {0:[1,0,0,0,0,0,0,0,0,0],\n",
    "     1:[0,1,0,0,0,0,0,0,0,0],\n",
    "     2:[0,0,1,0,0,0,0,0,0,0],\n",
    "     3:[0,0,0,1,0,0,0,0,0,0],\n",
    "     4:[0,0,0,0,1,0,0,0,0,0],\n",
    "     5:[0,0,0,0,0,1,0,0,0,0],\n",
    "     6:[0,0,0,0,0,0,1,0,0,0],\n",
    "     7:[0,0,0,0,0,0,0,1,0,0],\n",
    "     8:[0,0,0,0,0,0,0,0,1,0],\n",
    "     9:[0,0,0,0,0,0,0,0,0,1]}\n",
    "\n",
    "\n",
    "totalSteps = trainsteps\n",
    "stepCount=0\n",
    "batchSize = 128#32#64#256#128\n",
    "hist = {\n",
    "    'train_acc':[],\n",
    "    'val_acc':[],\n",
    "    'train_loss':[],\n",
    "    'val_loss':[],\n",
    "    'affinity':[],\n",
    "    'balance':[],\n",
    "    'coactivity':[],\n",
    "    'entr':[]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0/300000 \n",
      " Train: loss: 0.831814 \n",
      " Validation: loss: 0.838361\n",
      "entr: 0, affinity: 0.288081, balance: 0.525242, coact: 64.4872, frob: 0.00035621\n",
      "step 100/300000 \n",
      " Train: loss: 0.281312 \n",
      " Validation: loss: 0.274273\n",
      "entr: 0, affinity: 0.0934918, balance: 0.0884848, coact: 1.59341, frob: 0.000364982\n",
      "step 200/300000 \n",
      " Train: loss: 0.231325 \n",
      " Validation: loss: 0.21073\n",
      "entr: 0, affinity: 0.0834989, balance: 0.169461, coact: 1.98885, frob: 0.000392708\n",
      "step 300/300000 \n",
      " Train: loss: 0.151533 \n",
      " Validation: loss: 0.182184\n",
      "entr: 0, affinity: 0.0914585, balance: 0.0971826, coact: 1.56567, frob: 0.000417192\n",
      "step 400/300000 \n",
      " Train: loss: 0.175859 \n",
      " Validation: loss: 0.198836\n",
      "entr: 0, affinity: 0.0837378, balance: 0.155676, coact: 2.29484, frob: 0.000426947\n",
      "step 500/300000 \n",
      " Train: loss: 0.230757 \n",
      " Validation: loss: 0.23372\n",
      "entr: 0, affinity: 0.0705767, balance: 0.112763, coact: 2.74098, frob: 0.000417397\n",
      "step 600/300000 \n",
      " Train: loss: 0.150894 \n",
      " Validation: loss: 0.196245\n",
      "entr: 0, affinity: 0.0872394, balance: 0.126843, coact: 2.54909, frob: 0.000441073\n",
      "step 700/300000 \n",
      " Train: loss: 0.271785 \n",
      " Validation: loss: 0.188512\n",
      "entr: 0, affinity: 0.0905818, balance: 0.132858, coact: 2.30058, frob: 0.000449367\n",
      "step 800/300000 \n",
      " Train: loss: 0.125805 \n",
      " Validation: loss: 0.169162\n",
      "entr: 0, affinity: 0.0835399, balance: 0.147455, coact: 2.60684, frob: 0.000465303\n",
      "step 900/300000 \n",
      " Train: loss: 0.166894 \n",
      " Validation: loss: 0.18786\n",
      "entr: 0, affinity: 0.0741126, balance: 0.18445, coact: 2.05315, frob: 0.000469587\n",
      "step 1000/300000 \n",
      " Train: loss: 0.205574 \n",
      " Validation: loss: 0.164234\n",
      "entr: 0, affinity: 0.0894325, balance: 0.111943, coact: 2.44508, frob: 0.000470591\n",
      "step 1100/300000 \n",
      " Train: loss: 0.184186 \n",
      " Validation: loss: 0.118179\n",
      "entr: 0, affinity: 0.0861844, balance: 0.112006, coact: 2.4093, frob: 0.000500398\n",
      "step 1200/300000 \n",
      " Train: loss: 0.197694 \n",
      " Validation: loss: 0.192761\n",
      "entr: 0, affinity: 0.0660098, balance: 0.11471, coact: 2.5762, frob: 0.000513364\n",
      "step 1300/300000 \n",
      " Train: loss: 0.167807 \n",
      " Validation: loss: 0.231169\n",
      "entr: 0, affinity: 0.0755917, balance: 0.119167, coact: 3.16355, frob: 0.000505232\n",
      "step 1400/300000 \n",
      " Train: loss: 0.148643 \n",
      " Validation: loss: 0.190271\n",
      "entr: 0, affinity: 0.0673889, balance: 0.152254, coact: 2.65675, frob: 0.000513564\n",
      "step 1500/300000 \n",
      " Train: loss: 0.182851 \n",
      " Validation: loss: 0.224526\n",
      "entr: 0, affinity: 0.0781538, balance: 0.108113, coact: 2.5023, frob: 0.000543165\n",
      "step 1600/300000 \n",
      " Train: loss: 0.129275 \n",
      " Validation: loss: 0.212506\n",
      "entr: 0, affinity: 0.0810374, balance: 0.0705288, coact: 3.91642, frob: 0.000538639\n",
      "step 1700/300000 \n",
      " Train: loss: 0.188289 \n",
      " Validation: loss: 0.182102\n",
      "entr: 0, affinity: 0.0736851, balance: 0.0616562, coact: 3.60372, frob: 0.000532527\n",
      "step 1800/300000 \n",
      " Train: loss: 0.226046 \n",
      " Validation: loss: 0.167823\n",
      "entr: 0, affinity: 0.0811468, balance: 0.08437, coact: 3.09241, frob: 0.000562488\n",
      "step 1900/300000 \n",
      " Train: loss: 0.241431 \n",
      " Validation: loss: 0.241866\n",
      "entr: 0, affinity: 0.0772615, balance: 0.0837317, coact: 3.52583, frob: 0.000557601\n",
      "step 2000/300000 \n",
      " Train: loss: 0.187854 \n",
      " Validation: loss: 0.182022\n",
      "entr: 0, affinity: 0.0706236, balance: 0.10538, coact: 2.72628, frob: 0.000568018\n",
      "step 2100/300000 \n",
      " Train: loss: 0.187146 \n",
      " Validation: loss: 0.198872\n",
      "entr: 0, affinity: 0.0867333, balance: 0.0731214, coact: 4.49358, frob: 0.000559604\n",
      "step 2200/300000 \n",
      " Train: loss: 0.167657 \n",
      " Validation: loss: 0.182277\n",
      "entr: 0, affinity: 0.0685886, balance: 0.107955, coact: 3.30162, frob: 0.000565973\n",
      "step 2300/300000 \n",
      " Train: loss: 0.263675 \n",
      " Validation: loss: 0.154413\n",
      "entr: 0, affinity: 0.0889854, balance: 0.0593096, coact: 3.75752, frob: 0.000563857\n",
      "step 2400/300000 \n",
      " Train: loss: 0.208703 \n",
      " Validation: loss: 0.19918\n",
      "entr: 0, affinity: 0.0712545, balance: 0.0792596, coact: 2.39145, frob: 0.000591931\n",
      "step 2500/300000 \n",
      " Train: loss: 0.189461 \n",
      " Validation: loss: 0.146368\n",
      "entr: 0, affinity: 0.0642051, balance: 0.0748748, coact: 3.6274, frob: 0.000609196\n",
      "step 2600/300000 \n",
      " Train: loss: 0.160324 \n",
      " Validation: loss: 0.221082\n",
      "entr: 0, affinity: 0.0657331, balance: 0.162158, coact: 3.22806, frob: 0.000591527\n",
      "step 2700/300000 \n",
      " Train: loss: 0.128978 \n",
      " Validation: loss: 0.186369\n",
      "entr: 0, affinity: 0.0736274, balance: 0.15368, coact: 3.7507, frob: 0.000616658\n",
      "step 2800/300000 \n",
      " Train: loss: 0.150199 \n",
      " Validation: loss: 0.208915\n",
      "entr: 0, affinity: 0.0737676, balance: 0.0589456, coact: 4.15412, frob: 0.000614699\n",
      "step 2900/300000 \n",
      " Train: loss: 0.208253 \n",
      " Validation: loss: 0.160373\n",
      "entr: 0, affinity: 0.0781374, balance: 0.129449, coact: 5.31628, frob: 0.000622564\n",
      "step 3000/300000 \n",
      " Train: loss: 0.116312 \n",
      " Validation: loss: 0.176797\n",
      "entr: 0, affinity: 0.0718365, balance: 0.0663496, coact: 3.97793, frob: 0.000616216\n",
      "step 3100/300000 \n",
      " Train: loss: 0.154984 \n",
      " Validation: loss: 0.134644\n",
      "entr: 0, affinity: 0.0659763, balance: 0.0927432, coact: 4.83169, frob: 0.000629313\n",
      "step 3200/300000 \n",
      " Train: loss: 0.180323 \n",
      " Validation: loss: 0.167296\n",
      "entr: 0, affinity: 0.0660239, balance: 0.13553, coact: 4.65842, frob: 0.000642784\n",
      "step 3300/300000 \n",
      " Train: loss: 0.174998 \n",
      " Validation: loss: 0.169352\n",
      "entr: 0, affinity: 0.071898, balance: 0.0386223, coact: 3.37384, frob: 0.000659786\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-73b9446c7fa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"entr: %g, affinity: %g, balance: %g, coact: %g, frob: %g\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcc1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'affinity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcc2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'balance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcc3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coactivity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcc4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfrb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrainbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_def\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrainbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(totalSteps):\n",
    "    if i>0 and i%100000:\n",
    "        lr=lr/10\n",
    "    trainbatch = mnist.train.next_batch(batchSize)\n",
    "    trainbatch = (trainbatch[0],np.array([y[np.argmax(trainbatch[1][j])] for j in range(len(trainbatch[1]))]))\n",
    "    valbatch = mnist.validation.next_batch(batchSize)\n",
    "    valbatch = (valbatch[0],np.array([y[np.argmax(valbatch[1][j])] for j in range(len(valbatch[1]))]))\n",
    "    if i%100 == 0:\n",
    "        train_loss = sess.run([loss],feed_dict={x: trainbatch[0], y_def: trainbatch[1], keep_prob: 0.5})[0]\n",
    "        val_loss = sess.run([loss],feed_dict={x: valbatch[0], y_def: trainbatch[1], keep_prob: 0.5})[0]\n",
    "        #hist['train_acc'].append(train_acc)\n",
    "        #hist['val_acc'].append(val_acc)\n",
    "        hist['train_loss'].append(train_loss)\n",
    "        hist['val_loss'].append(val_loss)\n",
    "        print(\"step %d/%d \\n Train: loss: %g \\n Validation: loss: %g\"%(i,totalSteps, train_loss, val_loss))\n",
    "        hist['affinity'].append(affinity.eval(feed_dict={x:trainbatch[0], y_def: trainbatch[1], keep_prob: 0.5}))\n",
    "        hist['balance'].append(1-balance.eval(feed_dict={x:trainbatch[0], y_def: trainbatch[1], keep_prob: 0.5}))\n",
    "        hist['coactivity'].append(coact.eval(feed_dict={x:trainbatch[0], y_def: trainbatch[1], keep_prob: 0.5}))\n",
    "        hist['entr'].append(clust_cross_entropy.eval(feed_dict={x:trainbatch[0], y_def: trainbatch[1], keep_prob: 0.5}))\n",
    "        frb = frob.eval(feed_dict={x:trainbatch[0], y_def: trainbatch[1], keep_prob: 0.5})\n",
    "        #print bV.eval(feed_dict={x:trainbatch[0], y_: trainbatch[1]})\n",
    "\n",
    "        print(\"entr: %g, affinity: %g, balance: %g, coact: %g, frob: %g\"%(hist['entr'][-1],cc1*hist['affinity'][-1],cc2*(hist['balance'][-1]),cc3*hist['coactivity'][-1],cc4*frb))\n",
    "    feed_dict = {x: trainbatch[0], y_def: trainbatch[1], keep_prob: 0.5}\n",
    "    _ = sess.run([train_step],feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing & visualising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testSize = 1000\n",
    "testbatch = mnist.test.next_batch(testSize)\n",
    "testbatch = (testbatch[0],np.array([y[np.argmax(testbatch[1][j])] for j in range(len(testbatch[1]))]))\n",
    "\n",
    "test_loss = sess.run([loss],{x: testbatch[0],y_def: testbatch[1], keep_prob: 1.0})[0]\n",
    "print('Test: loss: %g'%(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot accuracy and loss\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(211)\n",
    "#plt.plot(hist['train_acc'],'-b',label='train_acc')\n",
    "#plt.plot(hist['val_acc'],'-r',label='val_acc')\n",
    "plt.legend()\n",
    "plt.subplot(212)\n",
    "plt.plot(hist['train_loss'],'-b',label='train_loss')\n",
    "plt.plot(hist['val_loss'],'-r',label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f=plt.figure(figsize=(15,7))\n",
    "#plt.plot(hist['train_loss'],'-b',label='train_loss')\n",
    "plt.plot(hist['affinity'],'-r',label='affinity')\n",
    "plt.plot(np.subtract(1,hist['balance']),'-y',label='balance')\n",
    "plt.plot(hist['coactivity'],'-g',label='coactivity')\n",
    "plt.plot(hist['entr'],'-b',label='entropy')\n",
    "plt.ylim([0,10])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "digitTrace = np.zeros((classCount*clustCount,784))\n",
    "digitTraceCount = np.zeros((classCount*clustCount))\n",
    "digitCount = np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#sess.run(tf.global_variables_initializer())\n",
    "for i in range(1000):\n",
    "    tb = mnist.test.next_batch(1)\n",
    "    testbatch = tb\n",
    "    digitCount[np.argmax(tb[1])]+=1\n",
    "    testbatch = (tb[0],np.array([y[np.argmax(tb[1][j])] for j in range(len(tb[1]))]))\n",
    "    smMat = sess.run([softmaxMat],feed_dict={x: testbatch[0],y_def: testbatch[1], keep_prob: 1.0})[0]\n",
    "    ypred = softmaxMat.eval({x: testbatch[0],y_def: testbatch[1], keep_prob: 1.0})\n",
    "    digitTrace[np.argmax(ypred),:] += tb[0].ravel()\n",
    "    digitTraceCount[np.argmax(ypred)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(digitCount)\n",
    "print(digitTraceCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "digitTrace = digitTrace/np.max(digitTrace)\n",
    "f,ax=plt.subplots(nrows=classCount, ncols=clustCount, figsize=(1.5*clustCount,1.5*classCount))\n",
    "gs = gridspec.GridSpec(classCount,clustCount)\n",
    "gs.update(wspace=0.025, hspace=0.025)\n",
    "f.subplots_adjust(wspace=0,hspace=0)\n",
    "for i in range(digitTrace.shape[0]):\n",
    "    sp = plt.subplot(gs[i])\n",
    "    sp.set_xticklabels([])\n",
    "    sp.set_yticklabels([])\n",
    "    sp.set_aspect('equal')\n",
    "    sp.grid = False\n",
    "    plt.imshow(np.reshape(digitTrace[i,:],(28,28)),cmap='bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stepCount = len(hist['train_acc'])*100\n",
    "with open('./trainlog.txt','ab') as f:\n",
    "    f.write('lr: %g, batchsize: %i, steps: %i, thresh: %g, c1: %g, c2: %g, c3: %g, c4: %g, test_loss: %g\\n'%\n",
    "            (lr,batchSize,stepCount,tresh.eval(), cc1, cc2, cc3, cc4, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tb = mnist.test.next_batch(10000)\n",
    "#tb[0][:,:] = threshold(tb[0][:,:],threshmin=0.5,newval=0)\n",
    "#tb[0][:,:] = threshold(tb[0][:,:],threshmax=0.49,newval=1)\n",
    "testbatch = (tb[0],np.array([y[np.argmax(tb[1][j])>4] for j in range(len(tb[1]))]))\n",
    "testFeed = {x: testbatch[0],y_def: testbatch[1]}\n",
    "ypred = softmaxMat.eval(testFeed)\n",
    "ypred = ypred.reshape((tb[0].shape[0],clustCount*classCount))\n",
    "ypred = tf.argmax(ypred,1).eval()\n",
    "ylookup = [np.argmax(np.sum(tb[1][ypred==i],0)).astype('int32') for i in range(clustCount*classCount)]\n",
    "yconverted = [ylookup[i] for i in ypred]\n",
    "correct_prediction = tf.equal(yconverted, np.argmax(tb[1],1).astype('int32'))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)).eval()\n",
    "print 'Clustering Accuracy: %g'%(accuracy)\n",
    "print ylookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare it to k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tb0 = [tb[0][np.argmax(tb[1],1)<5],tb[1][np.argmax(tb[1],1)<5]]\n",
    "tb1 = [tb[0][np.argmax(tb[1],1)>4],tb[1][np.argmax(tb[1],1)>4]]\n",
    "#<5\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "km0_ypred = kmeans.fit_transform(tb0[0])\n",
    "km0_ypred = np.argmax(km0_ypred,1)\n",
    "km_ylookup = [np.argmax(np.sum(tb0[1][km0_ypred==i],0)).astype('int32') for i in range(clustCount)]\n",
    "km_yconverted = [km_ylookup[i] for i in km0_ypred]\n",
    "km_correct_prediction = tf.equal(km_yconverted, np.argmax(tb0[1],1).astype('int32'))\n",
    "km0_accuracy = tf.reduce_mean(tf.cast(km_correct_prediction, tf.float32)).eval()\n",
    "#>4\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "km1_ypred = kmeans.fit_transform(tb1[0])\n",
    "km1_ypred = np.argmax(km1_ypred,1)\n",
    "km_ylookup = [np.argmax(np.sum(tb1[1][km1_ypred==i],0)).astype('int32') for i in range(clustCount)]\n",
    "km_yconverted = [km_ylookup[i] for i in km1_ypred]\n",
    "km_correct_prediction = tf.equal(km_yconverted, np.argmax(tb1[1],1).astype('int32'))\n",
    "km1_accuracy = tf.reduce_mean(tf.cast(km_correct_prediction, tf.float32)).eval()\n",
    "print 'ACOL Accuracy: %g'%(accuracy)\n",
    "print 'KMeans Accuracy: %g'%((km0_accuracy+km1_accuracy)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#visualise kmeans\n",
    "digitTrace = np.concatenate([[np.sum(tb0[0][km0_ypred==i,:],axis=0) for i in range(clustCount)],\n",
    "                       [np.sum(tb1[0][km1_ypred==i,:],axis=0) for i in range(clustCount)]])\n",
    "digitTrace = digitTrace/np.max(digitTrace)\n",
    "f,ax=plt.subplots(nrows=classCount, ncols=clustCount, figsize=(1.5*clustCount,1.5*classCount))\n",
    "gs = gridspec.GridSpec(classCount,clustCount)\n",
    "gs.update(wspace=0.025, hspace=0.025)\n",
    "f.subplots_adjust(wspace=0,hspace=0)\n",
    "for i in range(digitTrace.shape[0]):\n",
    "    sp = plt.subplot(gs[i])\n",
    "    sp.set_xticklabels([])\n",
    "    sp.set_yticklabels([])\n",
    "    sp.set_aspect('equal')\n",
    "    sp.grid = False\n",
    "    plt.imshow(np.reshape(digitTrace[i,:],(28,28)),cmap='bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neut_cross_entropy.eval(feed_dict={y_conv:[[0,1,0,0,0.2,0,1,0,0,0.2,0,0.2,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
